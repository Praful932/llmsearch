{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Check how the model pads input\n",
    "Check how different enecoding & decoding params affect the encoding & decoding\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPTQConfig, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from llmsearch.model_downloader import download_model_from_hf\n",
    "from llmsearch.utils.model_utils import batcher, decoder_parser\n",
    "\n",
    "import awq\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0+cu121', '0.2.3')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')\n",
    "\n",
    "torch.__version__, awq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "def perform_single_example_inference(example, model, tokenizer,gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output, prompt_tokens, completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders\n",
    "\n",
    "class MultiTokenEOSCriteria(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\n",
    "\n",
    "    This code is not thread safe. The same object cannot be used simultaneously in multiple threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_ids : List[int],\n",
    "    ) -> None:\n",
    "        self.sequence_ids = torch.tensor(sequence_ids, dtype = torch.int32, device = \"cuda:0\")\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = self.sequence_ids.shape[0] + 2\n",
    "        self.state_initialized = False\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "    def set_state(self, batch_size, input_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_length = input_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.state_initialized = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_size = None\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "\n",
    "        ret_val = False\n",
    "\n",
    "        if not self.state_initialized:\n",
    "            # 1st call to __call__ for this batch\n",
    "            self.set_state(input_ids.shape[0], input_ids.shape[1])\n",
    "\n",
    "        # IDs of all the tokens except the prompt\n",
    "        lookback_ids_batch = input_ids[:, self.input_length :]\n",
    "        # look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        # no elements yet to look back\n",
    "        if lookback_ids_batch.nelement() == 0:\n",
    "            return False\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                # look back only as far as the last token of the stop sequence\n",
    "                self.done_tracker[i] = self.sequence_ids == lookback_ids_batch[i][-(self.sequence_ids.shape[0]):]\n",
    "        ret_val = False not in self.done_tracker\n",
    "        if ret_val:\n",
    "            # print(f\"finish, \", self.sequence_ids, lookback_ids_batch)\n",
    "            self.reset()\n",
    "        return ret_val\n",
    "\n",
    "\n",
    "def load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_kwargs,temp_model_dir, model_branch = \"main\"):\n",
    "    output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = model_branch)\n",
    "\n",
    "    model_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "    tokenizer_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "\n",
    "    model_name_or_path = model_loader_kwargs.pop('pretrained_model_name_or_path')\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=model_name_or_path,\n",
    "        **model_loader_kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # pad token is null in config -https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ/blob/eb64c310c44905321d012962db9ac0d47c3a64fa/tokenizer_config.json#L53\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_loader_backend_map = {\n",
    "    # \"exllama_2_hf\": load_model_with_exllama_2_hf_backend,\n",
    "    # \"hf\": load_model_with_hf_backend,\n",
    "    # 'auto_gptq' : load_model_with_autogptq_backend,\n",
    "    'awq' : load_model_with_awq_backend,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to /workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.0  /51.0   243kiB/s\n",
      "100%|██████████| 17.9k /17.9k  25.7MiB/s\n",
      "100%|██████████| 115   /115    756kiB/s\n",
      "100%|██████████| 911   /911    3.30MiB/s\n",
      "100%|██████████| 126   /126    285kiB/s\n",
      "100%|██████████| 420   /420    1.61MiB/s\n",
      "100%|██████████| 1.60k /1.60k  3.90MiB/s\n",
      "\n",
      "100%|██████████| 493k  /493k   5.98MiB/s\n",
      "100%|██████████| 1.80M /1.80M  3.17MiB/s\n",
      "100%|██████████| 4.15G /4.15G  56.7MiB/s\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  6.28it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:02<00:00, 10.85it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "\n",
    "temp_model_dir = Path(f\"/workspace/temp_model_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "model_loader_kwargs = {\n",
    "    'device_map' : {'' : 0},\n",
    "    'fuse_layers' : True,\n",
    "}\n",
    "\n",
    "tokenizer_loader_kwargs = {\n",
    "    'use_fast' : False,\n",
    "    'legacy' : False,\n",
    "    'padding_side' : 'left',\n",
    "}\n",
    "\n",
    "model, tokenizer = load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_loader_kwargs,temp_model_dir, model_branch = \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04bdc2266184fe9a0d7ebda10e48c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "# Add prompt template\n",
    "processed_dataset = preprocess_dataset(gsm8k_dataset['train'], tokenizer,pt = pt, pt_cols = pt_cols, system_prompt = system_prompt, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "bm_sample_size = 100\n",
    "bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bos_token': '<s>',\n",
       "  'eos_token': '<|im_end|>',\n",
       "  'unk_token': '<unk>',\n",
       "  'pad_token': '<|im_end|>'},\n",
       " False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map, tokenizer.clean_up_tokenization_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset:\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed Dataset:\\n\")\n",
    "for i in range(5):\n",
    "    print(processed_dataset[i]['X'])\n",
    "    print('\\n')\n",
    "    print('---' * 10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       "  \"Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"],\n",
       " 'answer': ['Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       "  'He has 6 - 2 = <<6-2=4>>4 cats.\\nHe has 4 - 1 = <<4-1=3>>3 parrots.\\nHe has 4 + 6 = <<4+6=10>>10 snakes.\\nHe has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\\n#### 19'],\n",
       " 'X': ['<|im_start|>system\\nSolve the following math problems, end with The answer is<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  \"<|im_start|>system\\nSolve the following math problems, end with The answer is<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\\n<|im_start|>assistant\\n\"],\n",
       " 'actual input': ['Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       "  \"Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.model_utils import decoder_parser\n",
    "\n",
    "def perform_single_example_inference(example, model, tokenizer, gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    out = decoder_parser(outputs = [decoded_output], formatted_prompts = [example], prepoc = lambda x : x.strip())\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       "  \"Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"],\n",
       " 'answer': ['Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       "  'He has 6 - 2 = <<6-2=4>>4 cats.\\nHe has 4 - 1 = <<4-1=3>>3 parrots.\\nHe has 4 + 6 = <<4+6=10>>10 snakes.\\nHe has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\\n#### 19'],\n",
       " 'X': ['<|im_start|>system\\nSolve the following math problems, end with The answer is<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  \"<|im_start|>system\\nSolve the following math problems, end with The answer is<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\\n<|im_start|>assistant\\n\"],\n",
       " 'actual input': ['Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       "  \"Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry', 'date', 'elderberry']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "my_list = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n",
    "\n",
    "pprint(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b776489e7d42ce930ec8d5ca6dcb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 245])\n",
      "{'input_ids': tensor([[    1, 28705, 32001,  6574,    13, 28735,  5303,   272,  2296, 11049,\n",
      "          4418, 28725,   948,   395,   415,  4372,   349, 32000,    13, 32001,\n",
      "          1838,    13, 28824, 28747,  1387,   460, 28705, 28740, 28782,  7099,\n",
      "           297,   272,  5977,   333, 28723,  8697,   333,  7433,   622,  5100,\n",
      "          7099,   297,   272,  5977,   333,  3154, 28723,  2530,   590,   460,\n",
      "          2203, 28725,   736,   622,   347, 28705, 28750, 28740,  7099, 28723,\n",
      "          1602,  1287,  7099,   863,   272,  5977,   333,  7433,  5100,  3154,\n",
      "         28804,    13, 28741, 28747,  1387,   460, 28705, 28740, 28782,  7099,\n",
      "         10806, 28723,  2479,   736,   654, 28705, 28750, 28740,  7099,  1024,\n",
      "           741,   680,   654, 24571, 28723,  1537,   736,  1580,   506,   750,\n",
      "         28705, 28750, 28740,   387, 28705, 28740, 28782,   327, 28705, 28784,\n",
      "         28723,   415,  4372,   349, 28705, 28784, 28723,    13,    13, 28824,\n",
      "         28747,  1047,   736,   460, 28705, 28770,  8300,   297,   272, 12128,\n",
      "          2055,   304, 28705, 28750,   680,  8300, 12688, 28725,   910,  1287,\n",
      "          8300,   460,   297,   272, 12128,  2055, 28804,    13, 28741, 28747,\n",
      "          1387,   460, 10806, 28705, 28770,  8300, 28723, 28705, 28750,   680,\n",
      "          8300, 12688, 28723, 28705, 28770,   648, 28705, 28750,   327, 28705,\n",
      "         28782, 28723,   415,  4372,   349, 28705, 28782, 28723,    13,    13,\n",
      "         28824, 28747,   351, 13840,  7715,   582, 28705, 28750, 13627, 28443,\n",
      "         13436, 28713,   356,   272, 10305, 28723, 28705, 25363,  1419,  8660,\n",
      "           390,  1287, 10043, 28713,   390,   351, 13840,   304,  1658,   706,\n",
      "           297,   516, 10585, 28723,  1337,   956, 13300,   624, 28733, 16507,\n",
      "           302,   272, 10043, 28713,   369, 25363,  1419, 28723, 28705,  1602,\n",
      "          1287, 28443, 13436, 28713,   863,  1337,   956,   506, 28804, 32000,\n",
      "            13, 32001,   489, 11143,    13],\n",
      "        [32000, 32000, 32000,     1, 28705, 32001,  6574,    13, 28735,  5303,\n",
      "           272,  2296, 11049,  4418, 28725,   948,   395,   415,  4372,   349,\n",
      "         32000,    13, 32001,  1838,    13, 28824, 28747,  1387,   460, 28705,\n",
      "         28740, 28782,  7099,   297,   272,  5977,   333, 28723,  8697,   333,\n",
      "          7433,   622,  5100,  7099,   297,   272,  5977,   333,  3154, 28723,\n",
      "          2530,   590,   460,  2203, 28725,   736,   622,   347, 28705, 28750,\n",
      "         28740,  7099, 28723,  1602,  1287,  7099,   863,   272,  5977,   333,\n",
      "          7433,  5100,  3154, 28804,    13, 28741, 28747,  1387,   460, 28705,\n",
      "         28740, 28782,  7099, 10806, 28723,  2479,   736,   654, 28705, 28750,\n",
      "         28740,  7099,  1024,   741,   680,   654, 24571, 28723,  1537,   736,\n",
      "          1580,   506,   750, 28705, 28750, 28740,   387, 28705, 28740, 28782,\n",
      "           327, 28705, 28784, 28723,   415,  4372,   349, 28705, 28784, 28723,\n",
      "            13,    13, 28824, 28747,  1047,   736,   460, 28705, 28770,  8300,\n",
      "           297,   272, 12128,  2055,   304, 28705, 28750,   680,  8300, 12688,\n",
      "         28725,   910,  1287,  8300,   460,   297,   272, 12128,  2055, 28804,\n",
      "            13, 28741, 28747,  1387,   460, 10806, 28705, 28770,  8300, 28723,\n",
      "         28705, 28750,   680,  8300, 12688, 28723, 28705, 28770,   648, 28705,\n",
      "         28750,   327, 28705, 28782, 28723,   415,  4372,   349, 28705, 28782,\n",
      "         28723,    13,    13, 28824, 28747,  4790,   412, 28742, 28713,  4386,\n",
      "          1346,   713,   506,  1287, 24642, 28723,   650,   659,  3522,   680,\n",
      "          2502,  1593,   821,   400,   659, 18097, 28723,   650,   659,   624,\n",
      "          2108,   940,  5490,   821, 18097, 28723, 13934,   302,   516, 24642,\n",
      "           506,  2308,  7969, 28723,   650,   659, 28705, 28750,  9540, 28723,\n",
      "          1602,  1287, 24642,  1235,   400,   506,   297,  3102, 28804, 32000,\n",
      "            13, 32001,   489, 11143,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "0\n",
      "['<s>  <|im_start|> system\\n'\n",
      " 'Solve the following math problems, end with The answer is<|im_end|>\\n'\n",
      " ' <|im_start|> user\\n'\n",
      " 'Q: There are 15 trees in the grove. Grove workers will plant trees in the '\n",
      " 'grove today. After they are done, there will be 21 trees. How many trees did '\n",
      " 'the grove workers plant today?\\n'\n",
      " 'A: There are 15 trees originally. Then there were 21 trees after some more '\n",
      " 'were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n'\n",
      " '\\n'\n",
      " 'Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many '\n",
      " 'cars are in the parking lot?\\n'\n",
      " 'A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is '\n",
      " '5.\\n'\n",
      " '\\n'\n",
      " 'Q: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many '\n",
      " 'shells as Mimi and put them in his pocket. Leigh grabbed one-third of the '\n",
      " 'shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\\n'\n",
      " ' <|im_start|> assistant\\n',\n",
      " '<|im_end|><|im_end|><|im_end|><s>  <|im_start|> system\\n'\n",
      " 'Solve the following math problems, end with The answer is<|im_end|>\\n'\n",
      " ' <|im_start|> user\\n'\n",
      " 'Q: There are 15 trees in the grove. Grove workers will plant trees in the '\n",
      " 'grove today. After they are done, there will be 21 trees. How many trees did '\n",
      " 'the grove workers plant today?\\n'\n",
      " 'A: There are 15 trees originally. Then there were 21 trees after some more '\n",
      " 'were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n'\n",
      " '\\n'\n",
      " 'Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many '\n",
      " 'cars are in the parking lot?\\n'\n",
      " 'A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is '\n",
      " '5.\\n'\n",
      " '\\n'\n",
      " \"Q: Frankie's parents let him have many pets. He has six more snakes than he \"\n",
      " 'has cats. He has one less parrot than cats. Six of his pets have four legs. '\n",
      " 'He has 2 dogs. How many pets does he have in total?<|im_end|>\\n'\n",
      " ' <|im_start|> assistant\\n']\n",
      "\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmsearch.utils.model_utils import batcher\n",
    "\n",
    "def batch_inputs(inputs, tokenizer,batch_size, tokenizer_encoding_kwargs):\n",
    "    batched_inputs = []\n",
    "\n",
    "\n",
    "    for batch in tqdm(batcher(inputs, batch_size)):\n",
    "        model_input = [item['X'] for item in batch]\n",
    "        encoded_input = tokenizer(text = model_input, **tokenizer_encoding_kwargs, return_tensors = \"pt\")\n",
    "\n",
    "        # TODO : When a batch is encoded an item in the list could be a batch, use torch.chunk to split and extend the list\n",
    "        # final objective is to get a list of items how it would be encoded if batch size was some value\n",
    "        print(encoded_input['input_ids'].shape)\n",
    "        batched_inputs.append(encoded_input)\n",
    "\n",
    "        # print(batched_inputs)\n",
    "\n",
    "    return batched_inputs\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "sample_size = 2\n",
    "tokenizer_encoding_kwargs = {\n",
    "    # pad to longest seq in batch\n",
    "    'padding' : 'longest',\n",
    "    # adds <s> to the start (only adds to the longest sequence in the batch for some reason)\n",
    "    'add_special_tokens' : True\n",
    "}\n",
    "# convert to a list of dicts\n",
    "bm_sample_dicts = [{k: v[i] for k, v in bm_samples[:sample_size].items()} for i in range(sample_size)]\n",
    "\n",
    "batched_input = batch_inputs(bm_sample_dicts, tokenizer, batch_size,tokenizer_encoding_kwargs)\n",
    "\n",
    "tokenizer_decoding_kwargs = {\n",
    "    # 'skip_special_tokens' : True,\n",
    "    # 'clean_up_tokenization_spaces' : True,\n",
    "}\n",
    "\n",
    "\n",
    "for idx, encoded_batch in enumerate(batched_input):\n",
    "    print(encoded_batch)\n",
    "    decoded_input = tokenizer.batch_decode(encoded_batch['input_ids'], **tokenizer_decoding_kwargs)\n",
    "    print(idx)\n",
    "    pprint(decoded_input)\n",
    "    print('\\n\\n', '---' * 10, '\\n\\n')\n",
    "\n",
    "# for idx, item in enumerate(tokenizer.batch_decode(batched_input['input_ids'])):\n",
    "#     print(idx)\n",
    "#     print(item)\n",
    "#     print('\\n\\n', '---' * 10, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 288\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is\n",
      " <|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?\n",
      " <|im_start|>assistant\n",
      "A: Let's break down the information given:\n",
      "\n",
      "1. Olaf initially had 150 toy cars.\n",
      "2. Grandpa gave Olaf twice as many as Uncle, so Grandpa gave 2 * Uncle's cars.\n",
      "3. Dad gave Olaf 10 cars, 5 less than Mum, so Mum gave Olaf 10 + 5 = 15 cars.\n",
      "4. Auntie gave Olaf 6 cars, 1 more than Uncle, so Uncle gave Olaf 6 - 1 = 5 cars.\n",
      "\n",
      "Now let's calculate how many cars each family member gave:\n",
      "- Grandpa: 2 * Uncle's cars\n",
      "- Dad: 10 cars\n",
      "- Mum: 15 cars\n",
      "- Auntie: 6 cars\n",
      "\n",
      "Uncle's cars = 5 cars (as mentioned in the question)\n",
      "Grandpa gave Olaf 2 * Uncle's cars = 2 * 5 = 10 cars\n",
      "\n",
      "Now let's add up all the cars given by the family members:\n",
      "Total cars = Uncle's cars + Dad's cars + Mum's cars + Auntie's cars\n",
      "Total cars = 5 + 10 + 15 + 6 = 36 cars\n",
      "\n",
      "So, Olaf received a total of 36 toy cars from his family. Adding this to his initial collection, Olaf has a total of 150 + 36 = 186 toy cars. The answer is 186.<|im_end|>\n",
      "Completion Tokens - 335\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([MultiTokenEOSCriteria(sequence_ids = [32000])])\n",
    "gen_kwargs = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria\n",
    "}\n",
    "\n",
    "# Output changes based on skip_special_tokens value\n",
    "# padding tokens influencing output\n",
    "\n",
    "out = perform_single_example_inference(tokenizer.decode(batched_input['input_ids'][2], skip_special_tokens=True), model, tokenizer, gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       " 'He has 6 - 2 = <<6-2=4>>4 cats.\\nHe has 4 - 1 = <<4-1=3>>3 parrots.\\nHe has 4 + 6 = <<4+6=10>>10 snakes.\\nHe has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\\n#### 19',\n",
       " \"Dad gave Olaf 10 toy cars,\\nMom has given Olaf 5 more toy cars than Dad, so 10 + 5 = <<10+5=15>>15 toy cars\\nAuntie gave Olaf 6 toy cars,\\nUncle has given 1 less toy than Auntie, so 6 - 1 = <<6-1=5>>5 toy cars\\nGrandpa gave Olaf 2 * 5 = <<2*5=10>>10 toy cars.\\nAll the family together gave Olaf 10 +15 + 6 + 5 + 10 = <<10+15+6+5+10=46>>46.\\nAdding the cars Olaf already had, Olaf's collection has 150 + 46 = <<150+46=196>>196 cars.\\n#### 196\",\n",
       " 'She spend $56 because 7 x 8 = <<7*8=56>>56\\nShe has $44 left in the bank because 100 - 56 = <<100-56=44>>44\\nShe can get 8 five dollar bills because 44 / 5 = <<44/5=8.8>>8.8\\nThis is equal to $40 because 8 x 5 = <<8*5=40>>40\\nShe has $4 left in the account because 44 - 40 = <<44-40=4>>4\\n#### 4']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_samples['answer'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(batched_input['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
