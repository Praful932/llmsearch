{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b390a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "import llmsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25b6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device - mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "\n",
    "print(f\"Device - {device}\")\n",
    "\n",
    "def beep(duration = 1, frequency=440, rhythm=1):\n",
    "    sample_rate = 44100  # Standard audio sample rate\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    audio_data = np.sin(2*np.pi*frequency*t)  # Generate a sine wave\n",
    "    audio_data *= np.where(np.arange(len(audio_data)) % rhythm == 0, 1, 0)  # Apply rhythm\n",
    "    display(Audio(audio_data, rate=sample_rate, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2fe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/praful932/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8e4eacf17049d2a22479a39932f794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca4a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"train\"][:sample_size])\n",
    "samples_to_tune_on = samples_to_tune_on.rename_columns(column_mapping = {'dialogue' : 'X', 'summary' : \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40ddb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = False)\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ba55bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation: Amanda: I baked  cookies. Do you want some?\r\n",
      "Jerry: Sure!\r\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "Summary:\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "X = samples_to_tune_on[0]['X']\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(\"Conversation: {X}\\nSummary:\")\n",
    "\n",
    "print(pt.format_prompt(X = X).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cddba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    \n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True, use_aggregator=False)\n",
    "    return np.mean(result['rouge2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deca2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import get_total_available_ram, get_gpu_information\n",
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "seed = 42\n",
    "\n",
    "set_verbosity_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87baa229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner_ob = Tuner(model = model,tokenizer = tokenizer,dataset = samples_to_tune_on,\n",
    "                 device = device, batch_size = 512,\n",
    "                 tokenizer_encoding_kwargs={'padding': True, 'truncation': True, 'max_length': 512},\n",
    "                 tokenizer_decoding_kwargs = {'skip_special_tokens' : True,  'spaces_between_special_tokens' : False}, \n",
    "                 scorer = get_rouge_score, prompt_template = pt, is_encoder_decoder = True, seed = seed, column_mapping = {\"text_column_name\": \"X\", \"label_column_name\": \"y\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740358a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 15:41:55.558 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'max_new_tokens': 120}\n",
      "2023-09-23 15:41:55.559 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-23 15:41:55.559 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6cc324570142ca8f0df6e51a350827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:2389: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0:\n",
      "2023-09-23 15:42:23.838 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 28.278377 secs\n"
     ]
    }
   ],
   "source": [
    "# Earlier\n",
    "from llmsearch.utils.model_utils import seed_everything\n",
    "\"\"\"\n",
    "parameters and how they affect do_sample == False\n",
    "1. temperature - output does not change - greedy decoding\n",
    "2. top_k - output does not change - greedy decoding\n",
    "3. repetition_penalty - output changes\n",
    "4. no_repeat_ngram_size - output changes\n",
    "\"\"\"\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "initial_generation_params1 = {\n",
    "    'max_new_tokens' : 120,\n",
    "    'num_beams': 3,\n",
    "#     'temperature' : 0.7,\n",
    "#     'do_sample' : True,\n",
    "#     'generation_seed' : 42,\n",
    "#     'tfs' : 0.95,\n",
    "#     'top_a' : 0.3,\n",
    "    \n",
    "#     \"epsilon_cutoff\": 1.49,\n",
    "#     \"eta_cutoff\": 10.42,\n",
    "#     \"repetition_penalty\": 1.17,\n",
    "#     \"temperature\": 1.31,\n",
    "#     \"top_a\": 0.52,\n",
    "#     \"top_k\": 49,\n",
    "#     \"top_p\": 0.14,\n",
    "#     \"do_sample\": True,\n",
    "#     \"generation_seed\": 42,\n",
    "}\n",
    "score, outputs1 = tuner_ob.get_score(initial_generation_params1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02791f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18734483930420814\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f98bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 16:04:56.710 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'no_repeat_ngram_size': 5, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-23 16:04:56.711 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-23 16:04:56.711 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77ccaf834dd40f48c48daf80606e861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:719: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n"
     ]
    }
   ],
   "source": [
    "# Earlier\n",
    "from llmsearch.utils.model_utils import seed_everything\n",
    "\"\"\"\n",
    "parameters and how they affect do_sample == False\n",
    "1. temperature - output does not change - greedy decoding\n",
    "2. top_k - output does not change - greedy decoding\n",
    "3. repetition_penalty - output changes\n",
    "4. no_repeat_ngram_size - output changes\n",
    "\"\"\"\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "gen_params = {\n",
    "    'do_sample': False, \n",
    "    'generation_seed': 42, \n",
    "    'max_new_tokens': 120, \n",
    "    'no_repeat_ngram_size': 5, \n",
    "    'num_beam_groups': 1, \n",
    "    'num_beams': 3,\n",
    "}\n",
    "score, outputs1 = tuner_ob.get_score(gen_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb962ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epsilon_cutoff': [1.49],\n",
       "  'eta_cutoff': [10.42],\n",
       "  'repetition_penalty': [1.17],\n",
       "  'temperature': [1.31],\n",
       "  'top_a': [0.52],\n",
       "  'top_k': [49],\n",
       "  'top_p': [0.14],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.01],\n",
       "  'temperature': [0.87],\n",
       "  'tfs': [0.68],\n",
       "  'top_k': [85],\n",
       "  'top_p': [0.99],\n",
       "  'typical_p': [0.68],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.15],\n",
       "  'temperature': [0.7],\n",
       "  'top_k': [20],\n",
       "  'top_p': [0.9],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.09],\n",
       "  'temperature': [1.31],\n",
       "  'top_k': [72],\n",
       "  'top_p': [0.29],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'temperature': [0.2],\n",
       "  'top_k': [50],\n",
       "  'top_p': [0.95],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'encoder_repetition_penalty': [1.07],\n",
       "  'eta_cutoff': [10.78],\n",
       "  'repetition_penalty': [1.21],\n",
       "  'temperature': [1.01],\n",
       "  'top_a': [0.75],\n",
       "  'top_k': [91],\n",
       "  'top_p': [0.21],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.15],\n",
       "  'temperature': [0.7],\n",
       "  'tfs': [0.95],\n",
       "  'top_a': [0.2],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'do_sample': [False],\n",
       "  'penalty_alpha': [0.3],\n",
       "  'top_k': [4],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.02],\n",
       "  'temperature': [1.68],\n",
       "  'tfs': [0.97],\n",
       "  'top_a': [0.42],\n",
       "  'top_k': [77],\n",
       "  'top_p': [0.17],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'mirostat_mode': [2],\n",
       "  'mirostat_eta': [0.1],\n",
       "  'mirostat_tau': [5],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]},\n",
       " {'repetition_penalty': [1.05],\n",
       "  'temperature': [1.07],\n",
       "  'top_k': [100],\n",
       "  'do_sample': [True],\n",
       "  'generation_seed': [42],\n",
       "  'max_new_tokens': [120]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmsearch.utils.gen_utils import get_sample_hyp_space\n",
    "\n",
    "sample_hyp_spaces = get_sample_hyp_space(seed = 42, max_new_tokens = 120)\n",
    "sample_hyp_spaces[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bec4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_param_grid = sample_hyp_spaces[0]\n",
    "\n",
    "scorer = make_scorer(score_func=get_rouge_score, greater_is_better=True)\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator = tuner_ob.estimator,\n",
    "    param_grid=hyp_param_grid,\n",
    "    scoring = scorer,\n",
    "    cv = 5,\n",
    "    n_jobs = None,\n",
    ")\n",
    "\n",
    "# clf = RandomizedSearchCV(\n",
    "#     estimator=tuner_ob.estimator,\n",
    "#     param_distributions=hyp_param_grid,\n",
    "#     n_iter = 2,\n",
    "#     scoring=scorer,\n",
    "#     cv=5,\n",
    "#     random_state = 42,\n",
    "#     n_jobs=None,\n",
    "# )\n",
    "\n",
    "# \"\"\"\n",
    "# 5 fold means, whole sample set of 100 examples will be split into 80:20 ratio\n",
    "# for each hyper_parameter set we have a model f(hyper_params)\n",
    "#     - we will evaluate this model and get the cross val score (test on each 20 samples 5 times, while training on the rest 80 each time)\n",
    "#     - we get the score on the quality of hyperparams by evaluating the model with the hyperparams on the unseen 1 fold\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f5f6ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuner_ob.get_score({'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'repetition_penalty': 1.01, 'temperature': 0.87, 'tfs': 0.68, 'top_k': 85, 'top_p': 0.99, 'typical_p': 0.68})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967d672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:20.918 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 4}\n",
      "2023-09-19 19:25:20.918 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:20.919 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e54d1b5f69f45919d27ccae6dc71a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:719: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "2023-09-19 19:25:31.350 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 10.430458 secs\n",
      "2023-09-19 19:25:31.382 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 4}\n",
      "2023-09-19 19:25:31.383 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:31.383 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f48fd878b0a475298e92c143a7af838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:35.401 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.018452 secs\n",
      "2023-09-19 19:25:35.438 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 4}\n",
      "2023-09-19 19:25:35.438 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:35.439 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49494da19a2f4f80892035288bddc88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:38.145 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.705929 secs\n",
      "2023-09-19 19:25:38.179 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 4}\n",
      "2023-09-19 19:25:38.180 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:38.180 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91f5b1df7f741b0bf1548a13047ce52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:41.387 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.206380 secs\n",
      "2023-09-19 19:25:41.424 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 4}\n",
      "2023-09-19 19:25:41.425 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:41.425 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70047ad858854d1bbce4c913991b71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:46.370 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.945392 secs\n",
      "2023-09-19 19:25:46.400 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 5}\n",
      "2023-09-19 19:25:46.401 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:46.401 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2c7812a20e4a2685d72a03563dbe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:49.531 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.130157 secs\n",
      "2023-09-19 19:25:49.563 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 5}\n",
      "2023-09-19 19:25:49.564 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:49.564 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e59475e2e948df9ad2d00eee5d8f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:54.512 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.948178 secs\n",
      "2023-09-19 19:25:54.549 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 5}\n",
      "2023-09-19 19:25:54.550 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:54.551 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5871b769cf24ae89b1f31764bfd8879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:25:57.274 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.723170 secs\n",
      "2023-09-19 19:25:57.309 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 5}\n",
      "2023-09-19 19:25:57.309 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:25:57.310 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a464a736e534ed4bf185f37de356897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:00.075 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.765363 secs\n",
      "2023-09-19 19:26:00.108 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 5}\n",
      "2023-09-19 19:26:00.110 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:00.112 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0b3c42596d41c09255c9314df66154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:03.254 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.141758 secs\n",
      "2023-09-19 19:26:03.285 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 6}\n",
      "2023-09-19 19:26:03.285 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:03.285 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a6b52f1eb243929cdfc449691d8d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:06.098 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.812130 secs\n",
      "2023-09-19 19:26:06.130 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 6}\n",
      "2023-09-19 19:26:06.131 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:06.131 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984e43115b7f449a893e7d75f65c2cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:09.285 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.154441 secs\n",
      "2023-09-19 19:26:09.321 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 6}\n",
      "2023-09-19 19:26:09.321 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:09.321 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118923b5095d4ba1ba192f6c4ec361ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:11.909 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.587485 secs\n",
      "2023-09-19 19:26:11.943 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 6}\n",
      "2023-09-19 19:26:11.944 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:11.944 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc185683d64642a849204feb85d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:14.721 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.776461 secs\n",
      "2023-09-19 19:26:14.754 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 6}\n",
      "2023-09-19 19:26:14.755 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:14.755 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7def5317fe2b4220925bab79e40d3dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:17.855 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.100252 secs\n",
      "2023-09-19 19:26:17.885 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 7}\n",
      "2023-09-19 19:26:17.885 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:17.886 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e064ec67c9224fc1b291d2e74f0786cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:21.409 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.523154 secs\n",
      "2023-09-19 19:26:21.441 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 7}\n",
      "2023-09-19 19:26:21.441 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:21.441 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8b4b22b2a34a5dbd0320223fa9148b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:28.342 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.900421 secs\n",
      "2023-09-19 19:26:28.379 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 7}\n",
      "2023-09-19 19:26:28.380 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:28.380 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c6d68bdf0044ef820249427f98a8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:31.097 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.716602 secs\n",
      "2023-09-19 19:26:31.130 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 7}\n",
      "2023-09-19 19:26:31.131 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:31.132 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb6a46a1a3c4940b4eb27c1df81690f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:33.785 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.653827 secs\n",
      "2023-09-19 19:26:33.819 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 7}\n",
      "2023-09-19 19:26:33.819 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:33.820 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8553ae4e3d26439c851de4d9f739727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:36.828 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.007939 secs\n",
      "2023-09-19 19:26:36.858 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 8}\n",
      "2023-09-19 19:26:36.858 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:36.859 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48e5d681ff2430687c52d53366aed28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:40.323 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.464586 secs\n",
      "2023-09-19 19:26:40.355 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 8}\n",
      "2023-09-19 19:26:40.356 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:40.357 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e4e308702f410ea4405fe08e5139d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:43.588 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.231133 secs\n",
      "2023-09-19 19:26:43.630 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 8}\n",
      "2023-09-19 19:26:43.631 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:43.631 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05508d92ff7443e3b14095271baa1f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:46.771 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.139913 secs\n",
      "2023-09-19 19:26:46.803 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 8}\n",
      "2023-09-19 19:26:46.805 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:46.808 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e89334878964a2ca14b2f694ebd610b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:50.060 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.252029 secs\n",
      "2023-09-19 19:26:50.111 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 8}\n",
      "2023-09-19 19:26:50.111 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:50.112 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8156c2da0420e9870fa4c811f7031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:53.267 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.155038 secs\n",
      "2023-09-19 19:26:53.298 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 9}\n",
      "2023-09-19 19:26:53.300 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:53.301 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6a5973622b4c6e9db991856f52f1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:26:56.963 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.662499 secs\n",
      "2023-09-19 19:26:56.996 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 9}\n",
      "2023-09-19 19:26:56.996 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:26:56.996 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7334c20b2b8410fb561a0329f7d4e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:00.172 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.176077 secs\n",
      "2023-09-19 19:27:00.208 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 9}\n",
      "2023-09-19 19:27:00.210 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:00.213 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb83a8b1022640c798639d6768f88aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:03.456 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.243316 secs\n",
      "2023-09-19 19:27:03.493 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 9}\n",
      "2023-09-19 19:27:03.494 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:03.495 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e40539ee8a428c972c0da128d68a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:06.868 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.373849 secs\n",
      "2023-09-19 19:27:06.905 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 9}\n",
      "2023-09-19 19:27:06.906 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:06.908 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddfb21a42b244c192342cf1f9cd30b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:09.206 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.298170 secs\n",
      "2023-09-19 19:27:09.236 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 10}\n",
      "2023-09-19 19:27:09.236 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:09.237 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff1a0273c2649fdbdd74ccdd24358ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:13.356 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.118974 secs\n",
      "2023-09-19 19:27:13.389 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 10}\n",
      "2023-09-19 19:27:13.390 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:13.391 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925fa0f1fc974dccb72249c19261c2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:17.145 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.754742 secs\n",
      "2023-09-19 19:27:17.180 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 10}\n",
      "2023-09-19 19:27:17.180 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:17.180 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb964b1e63c344e984c6536cee0c2120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:20.343 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.162481 secs\n",
      "2023-09-19 19:27:20.378 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 10}\n",
      "2023-09-19 19:27:20.379 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:20.379 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2ed5009ba24afe9b6825d6f2cfb4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:23.374 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.995151 secs\n",
      "2023-09-19 19:27:23.407 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 10}\n",
      "2023-09-19 19:27:23.407 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:23.408 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01e2f0aa38f45fab61bf9749c37ea86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:25.810 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.402361 secs\n",
      "2023-09-19 19:27:25.839 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 11}\n",
      "2023-09-19 19:27:25.840 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:25.840 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c01723edcd438ca360e338b9427190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:30.136 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.296153 secs\n",
      "2023-09-19 19:27:30.166 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 11}\n",
      "2023-09-19 19:27:30.166 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:30.167 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39601b3ce5d24edd96e8879a827cd2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:33.698 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.531086 secs\n",
      "2023-09-19 19:27:33.731 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 11}\n",
      "2023-09-19 19:27:33.731 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:33.732 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1eec2b31524472817646bf668e71c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:36.834 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.101652 secs\n",
      "2023-09-19 19:27:36.865 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 11}\n",
      "2023-09-19 19:27:36.867 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:36.875 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbef57d089854442b9c09cd409e68d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:40.222 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.347709 secs\n",
      "2023-09-19 19:27:40.289 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 11}\n",
      "2023-09-19 19:27:40.289 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:40.290 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ce4d3faec64d41a73dc336ee84f68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:43.573 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.282990 secs\n",
      "2023-09-19 19:27:43.604 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 12}\n",
      "2023-09-19 19:27:43.604 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:43.604 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c74308c111b4ce6a3a61817d9851b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:47.097 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.492785 secs\n",
      "2023-09-19 19:27:47.129 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 12}\n",
      "2023-09-19 19:27:47.130 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:47.130 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74cfb90ecfa468f9697eb5291f32582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:52.575 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.444783 secs\n",
      "2023-09-19 19:27:52.615 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 12}\n",
      "2023-09-19 19:27:52.616 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:52.616 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bac2457e3974e02b296686f7ffa6d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:55.947 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.331463 secs\n",
      "2023-09-19 19:27:55.983 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 12}\n",
      "2023-09-19 19:27:55.984 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:55.984 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15a70e6f0d490889c9ab64dd1a6a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:27:59.440 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.456090 secs\n",
      "2023-09-19 19:27:59.475 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 12}\n",
      "2023-09-19 19:27:59.476 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:27:59.476 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee0483e96b94ea1a2d2aed1112fce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:02.636 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.159370 secs\n",
      "2023-09-19 19:28:02.668 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 13}\n",
      "2023-09-19 19:28:02.669 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:02.669 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b23398c918549719b5b79339031fc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:06.021 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.351299 secs\n",
      "2023-09-19 19:28:06.053 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 13}\n",
      "2023-09-19 19:28:06.054 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:06.055 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c41044badc4600b8899ce1270c10c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:10.855 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.800725 secs\n",
      "2023-09-19 19:28:10.896 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 13}\n",
      "2023-09-19 19:28:10.897 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:10.898 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee6f261a295476da6ac62f2e4a2e368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:14.474 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.576570 secs\n",
      "2023-09-19 19:28:14.508 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 13}\n",
      "2023-09-19 19:28:14.509 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:14.509 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b404abc8c5d544d5ac249bd4cf383f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:18.004 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.495572 secs\n",
      "2023-09-19 19:28:18.039 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 13}\n",
      "2023-09-19 19:28:18.040 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:18.040 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1daf334ac5e460c916b5b266c033996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:21.348 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.307876 secs\n",
      "2023-09-19 19:28:21.378 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 14}\n",
      "2023-09-19 19:28:21.379 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:21.380 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67a92d107ff4292b7f8e466293e30c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:24.780 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.400737 secs\n",
      "2023-09-19 19:28:24.813 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 14}\n",
      "2023-09-19 19:28:24.813 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:24.814 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef8c3faa4a40b096a834806a1ff44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:28.946 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.132695 secs\n",
      "2023-09-19 19:28:28.982 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 14}\n",
      "2023-09-19 19:28:28.985 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:28.986 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a50fc0fdc694b7ab14e436eb7d1cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:32.230 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.244009 secs\n",
      "2023-09-19 19:28:32.264 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 14}\n",
      "2023-09-19 19:28:32.264 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:32.265 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2aa42698cd4cff973913d3260ce7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:35.544 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.279482 secs\n",
      "2023-09-19 19:28:35.578 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 14}\n",
      "2023-09-19 19:28:35.579 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:35.579 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512d8e31288040a6b5e919872fa276d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:38.872 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.293275 secs\n",
      "2023-09-19 19:28:38.903 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 15}\n",
      "2023-09-19 19:28:38.904 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:38.904 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461c3f967a964816a4b8f7a31c9e9c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:42.626 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.721749 secs\n",
      "2023-09-19 19:28:42.658 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 15}\n",
      "2023-09-19 19:28:42.659 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:42.659 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0b834249d74969ae327a1eb1b6a93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:46.511 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.851733 secs\n",
      "2023-09-19 19:28:46.545 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 15}\n",
      "2023-09-19 19:28:46.546 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:46.546 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df081572133d4b19beaff59b2ef7901d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:50.401 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.854470 secs\n",
      "2023-09-19 19:28:50.441 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 15}\n",
      "2023-09-19 19:28:50.441 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:50.442 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3562d0c3ae704fdda43ee841aa35b3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:53.551 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.108577 secs\n",
      "2023-09-19 19:28:53.595 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 15}\n",
      "2023-09-19 19:28:53.595 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:53.596 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27b84485b1e40fdaeabacb7632ac3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:56.791 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.194633 secs\n",
      "2023-09-19 19:28:56.826 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 16}\n",
      "2023-09-19 19:28:56.826 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:56.827 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb588b06ba4c69bfe1dfc1c3ccd0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:28:59.937 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.110356 secs\n",
      "2023-09-19 19:28:59.972 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 16}\n",
      "2023-09-19 19:28:59.973 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:28:59.973 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb01db138a46ddbb6ae4a918ca1010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:07.984 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.010610 secs\n",
      "2023-09-19 19:29:08.018 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 16}\n",
      "2023-09-19 19:29:08.019 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:08.019 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb4dd103d774dd0b88185491ddb4415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:11.448 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.429441 secs\n",
      "2023-09-19 19:29:11.482 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 16}\n",
      "2023-09-19 19:29:11.482 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:11.483 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c816efd3f7e54674aab1ae939d01e104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:14.141 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.657449 secs\n",
      "2023-09-19 19:29:14.174 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 16}\n",
      "2023-09-19 19:29:14.177 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:14.179 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6603fabc43094d64b3adf9371df5d11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:17.384 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.205200 secs\n",
      "2023-09-19 19:29:17.414 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 17}\n",
      "2023-09-19 19:29:17.417 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:17.418 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e6edf2fe4c4be2bcaa17307f7a84e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:20.583 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.165044 secs\n",
      "2023-09-19 19:29:20.612 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 17}\n",
      "2023-09-19 19:29:20.612 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:20.613 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6985d3d217834531a4087a0cfe838aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:25.443 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.829978 secs\n",
      "2023-09-19 19:29:25.481 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 17}\n",
      "2023-09-19 19:29:25.482 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:25.482 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d656f05a34404f8917242af92ac8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:28.716 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.233482 secs\n",
      "2023-09-19 19:29:28.749 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 17}\n",
      "2023-09-19 19:29:28.750 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:28.750 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adc0f4e49294a61b9b28f2f13bc35a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:31.440 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.689339 secs\n",
      "2023-09-19 19:29:31.477 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 17}\n",
      "2023-09-19 19:29:31.477 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:31.478 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45f1f05df4a41d5819004a0c2750496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:34.680 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.202426 secs\n",
      "2023-09-19 19:29:34.710 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 18}\n",
      "2023-09-19 19:29:34.711 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:34.711 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b769b755246c4395aacafac25b5f5f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:37.916 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.204506 secs\n",
      "2023-09-19 19:29:37.948 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 18}\n",
      "2023-09-19 19:29:37.948 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:37.949 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87c7d6a5ce34c358f05c36510b05bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:42.766 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.817198 secs\n",
      "2023-09-19 19:29:42.801 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 18}\n",
      "2023-09-19 19:29:42.801 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:42.802 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccfaff10c9e46fabe589148e682a5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:46.119 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.316664 secs\n",
      "2023-09-19 19:29:46.150 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 18}\n",
      "2023-09-19 19:29:46.150 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:46.151 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78553a325ab04a41866a3b92ba9ab8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:48.940 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.789103 secs\n",
      "2023-09-19 19:29:48.971 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 18}\n",
      "2023-09-19 19:29:48.972 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:48.973 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce57cfdc3534d91b42a7d57e6e1b8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:52.557 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.584940 secs\n",
      "2023-09-19 19:29:52.586 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 19}\n",
      "2023-09-19 19:29:52.587 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:52.588 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b273cf360e4f5fb73c8e642a61d5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:29:56.415 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.827035 secs\n",
      "2023-09-19 19:29:56.446 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 19}\n",
      "2023-09-19 19:29:56.447 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:29:56.447 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2719149e134b2db78e166108d280c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:00.004 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.556416 secs\n",
      "2023-09-19 19:30:00.040 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 19}\n",
      "2023-09-19 19:30:00.041 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:00.041 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67dc578f1af4684b050a9d8a494ebea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:03.532 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.490732 secs\n",
      "2023-09-19 19:30:03.566 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 19}\n",
      "2023-09-19 19:30:03.566 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:03.566 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472076b768ef40d99e56c01bf3dc8c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:06.293 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.726653 secs\n",
      "2023-09-19 19:30:06.327 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.3, 'top_k': 19}\n",
      "2023-09-19 19:30:06.328 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:06.328 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e7881119c2401cbb43b882f6565c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:09.688 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.359672 secs\n",
      "2023-09-19 19:30:09.719 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 4}\n",
      "2023-09-19 19:30:09.719 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:09.720 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278c0459956340dcb167d89d335ee592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:12.517 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.797387 secs\n",
      "2023-09-19 19:30:12.547 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 4}\n",
      "2023-09-19 19:30:12.548 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:12.548 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6bf285440448feaa37cb4071c2b2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:15.569 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.021074 secs\n",
      "2023-09-19 19:30:15.604 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 4}\n",
      "2023-09-19 19:30:15.605 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:15.605 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387d86d797348f5a4c67feb776d087e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:17.859 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.254032 secs\n",
      "2023-09-19 19:30:17.892 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 4}\n",
      "2023-09-19 19:30:17.892 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:17.892 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d60dc58ad0463c89a6715e6a081f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:20.736 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.843966 secs\n",
      "2023-09-19 19:30:20.768 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 4}\n",
      "2023-09-19 19:30:20.768 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:20.769 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309926f54e2a4009874c5d09860cf740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:24.278 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.509303 secs\n",
      "2023-09-19 19:30:24.308 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 5}\n",
      "2023-09-19 19:30:24.309 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:24.309 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e1a5a927ba46dfa2d720f943f5ad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:27.662 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.352852 secs\n",
      "2023-09-19 19:30:27.692 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 5}\n",
      "2023-09-19 19:30:27.693 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:27.693 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b18e99b2bc4a2bb90f28f52a2a3fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:31.699 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.005900 secs\n",
      "2023-09-19 19:30:31.732 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 5}\n",
      "2023-09-19 19:30:31.732 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:31.733 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439b56c645604171918f990caa3b7739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:34.637 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.903904 secs\n",
      "2023-09-19 19:30:34.671 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 5}\n",
      "2023-09-19 19:30:34.673 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:34.677 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67be9ff4bde64e12855d6f4d4d737735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:37.608 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.931643 secs\n",
      "2023-09-19 19:30:37.644 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 5}\n",
      "2023-09-19 19:30:37.644 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:37.645 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6627cc49ea341fdb5ee1e90bc349155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:40.813 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.167817 secs\n",
      "2023-09-19 19:30:40.853 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 6}\n",
      "2023-09-19 19:30:40.854 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:40.854 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05969fb0a4944cf2ae636a1cdd8fabd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:43.706 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.852205 secs\n",
      "2023-09-19 19:30:43.738 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 6}\n",
      "2023-09-19 19:30:43.738 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:43.739 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d242ca1717e24dfda929237526280d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:47.144 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.405161 secs\n",
      "2023-09-19 19:30:47.179 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 6}\n",
      "2023-09-19 19:30:47.180 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:47.180 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff865e8a72a45229834fa1776fa2cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:49.738 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.557127 secs\n",
      "2023-09-19 19:30:49.771 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 6}\n",
      "2023-09-19 19:30:49.772 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:49.772 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f832385b054dcd8ac8e2b2e4d57b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:52.504 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.732395 secs\n",
      "2023-09-19 19:30:52.536 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 6}\n",
      "2023-09-19 19:30:52.536 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:52.536 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07adff7b7c434db89c28aec722ad11df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:55.650 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.113936 secs\n",
      "2023-09-19 19:30:55.679 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 7}\n",
      "2023-09-19 19:30:55.680 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:55.682 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daebfe04d1144f89779b84f9b69bcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:30:59.246 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.564132 secs\n",
      "2023-09-19 19:30:59.277 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 7}\n",
      "2023-09-19 19:30:59.277 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:30:59.278 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b917b008a2bf48778db105f17efd4c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:04.273 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.995092 secs\n",
      "2023-09-19 19:31:04.317 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 7}\n",
      "2023-09-19 19:31:04.319 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:04.323 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a13325044d4d23bb26278abe52c5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:07.456 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.133595 secs\n",
      "2023-09-19 19:31:07.490 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 7}\n",
      "2023-09-19 19:31:07.491 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:07.491 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92695a0d84d14df1af5c2c3dc25e639d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:10.369 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.878096 secs\n",
      "2023-09-19 19:31:10.402 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 7}\n",
      "2023-09-19 19:31:10.402 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:10.403 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26e9d3a7e2343f298a464384f6fe074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:14.114 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.711649 secs\n",
      "2023-09-19 19:31:14.144 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 8}\n",
      "2023-09-19 19:31:14.144 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:14.145 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd92803c1f8a45229953cf458d672d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:18.169 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.023675 secs\n",
      "2023-09-19 19:31:18.202 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 8}\n",
      "2023-09-19 19:31:18.203 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:18.203 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a849cbf6e64a488f849318083e35356b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:22.203 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.999532 secs\n",
      "2023-09-19 19:31:22.237 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 8}\n",
      "2023-09-19 19:31:22.238 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:22.238 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2250451d004e80bdb318218d09ac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:25.701 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.462498 secs\n",
      "2023-09-19 19:31:25.734 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 8}\n",
      "2023-09-19 19:31:25.734 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:25.734 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845fdb5b7f1142f88897d9a723066663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:29.212 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.477990 secs\n",
      "2023-09-19 19:31:29.281 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 8}\n",
      "2023-09-19 19:31:29.283 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:29.283 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5528c8aafc746fe9a2951bf8fbd90c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:33.249 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.965878 secs\n",
      "2023-09-19 19:31:33.281 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 9}\n",
      "2023-09-19 19:31:33.282 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:33.283 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f5f30fc8ea4af2a6448e79dca68474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:37.322 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.039740 secs\n",
      "2023-09-19 19:31:37.363 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 9}\n",
      "2023-09-19 19:31:37.363 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:37.364 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df43f9359fb74e64b9a74619e782cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:41.197 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.833321 secs\n",
      "2023-09-19 19:31:41.235 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 9}\n",
      "2023-09-19 19:31:41.236 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:41.238 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d6a0b3285940a5978c953c27972f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:44.661 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.423943 secs\n",
      "2023-09-19 19:31:44.696 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 9}\n",
      "2023-09-19 19:31:44.697 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:44.697 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68108e7c91484ff2b4ea3bd8b08a771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:48.113 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.415246 secs\n",
      "2023-09-19 19:31:48.145 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 9}\n",
      "2023-09-19 19:31:48.146 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:48.146 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd95e99747643d0b75b8e961cca6243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:50.486 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.340086 secs\n",
      "2023-09-19 19:31:50.516 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 10}\n",
      "2023-09-19 19:31:50.517 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:50.518 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff8e3a2e3546dc9e4885365e4d6e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:54.901 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.382499 secs\n",
      "2023-09-19 19:31:54.933 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 10}\n",
      "2023-09-19 19:31:54.934 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:54.934 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd7295d387443385e67abd0d9eff36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:31:59.564 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.630210 secs\n",
      "2023-09-19 19:31:59.600 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 10}\n",
      "2023-09-19 19:31:59.600 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:31:59.601 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d263518783e3448aa5c8134804a9817d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:02.705 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.103549 secs\n",
      "2023-09-19 19:32:02.738 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 10}\n",
      "2023-09-19 19:32:02.738 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:02.738 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edcb63df2fa479cb130546bdf50ab5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:06.267 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.528714 secs\n",
      "2023-09-19 19:32:06.326 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 10}\n",
      "2023-09-19 19:32:06.327 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:06.328 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6e28791d8143788085b0c6d44f19ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:09.414 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.086609 secs\n",
      "2023-09-19 19:32:09.447 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 11}\n",
      "2023-09-19 19:32:09.448 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:09.448 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5241dd96d034a5da00ac924a5f42770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:14.568 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.119785 secs\n",
      "2023-09-19 19:32:14.599 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 11}\n",
      "2023-09-19 19:32:14.599 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:14.600 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df62efc0dd94eb1aac4b013875da2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:19.141 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.541467 secs\n",
      "2023-09-19 19:32:19.174 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 11}\n",
      "2023-09-19 19:32:19.175 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:19.175 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67b479df0af4522b7b3941ab384369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:22.744 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.569186 secs\n",
      "2023-09-19 19:32:22.780 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 11}\n",
      "2023-09-19 19:32:22.781 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:22.781 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187c7ba2775240a7bdcd5f1804ac529d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:26.556 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.774362 secs\n",
      "2023-09-19 19:32:26.594 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 11}\n",
      "2023-09-19 19:32:26.595 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:26.596 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800687825ce7453f8cedcb89fb4b4038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:30.187 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.591092 secs\n",
      "2023-09-19 19:32:30.221 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 12}\n",
      "2023-09-19 19:32:30.221 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:30.221 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45606fabfdaf4992a939b747c245377d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:33.471 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.249048 secs\n",
      "2023-09-19 19:32:33.501 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 12}\n",
      "2023-09-19 19:32:33.502 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:33.502 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a33945021b455384d27759fe26ba96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:38.283 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.781533 secs\n",
      "2023-09-19 19:32:38.325 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 12}\n",
      "2023-09-19 19:32:38.326 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:38.326 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114c7aa5e7604c03877aad6f44f9606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:41.836 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.509947 secs\n",
      "2023-09-19 19:32:41.873 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 12}\n",
      "2023-09-19 19:32:41.875 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:41.876 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb1333b99d849458df3e68a989153e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:45.621 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.744676 secs\n",
      "2023-09-19 19:32:45.656 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 12}\n",
      "2023-09-19 19:32:45.657 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:45.657 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b173b1fe05184d17ac4918db1d02c9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:48.819 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.161464 secs\n",
      "2023-09-19 19:32:48.853 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 13}\n",
      "2023-09-19 19:32:48.854 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:48.854 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7975654ca41a4d13a6d84c5dad04d061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:52.229 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.374361 secs\n",
      "2023-09-19 19:32:52.261 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 13}\n",
      "2023-09-19 19:32:52.262 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:52.263 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efb68fd03b7480aa18e1bf3d67e365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:32:56.832 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.568758 secs\n",
      "2023-09-19 19:32:56.870 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 13}\n",
      "2023-09-19 19:32:56.871 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:32:56.871 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06cb8ce7314438eaecbbaac9bafc3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:00.386 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.514888 secs\n",
      "2023-09-19 19:33:00.420 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 13}\n",
      "2023-09-19 19:33:00.420 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:00.421 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b6f8fb500f4349b67e8f94ad4b4cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:04.224 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.802763 secs\n",
      "2023-09-19 19:33:04.258 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 13}\n",
      "2023-09-19 19:33:04.259 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:04.259 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4392f313ab4a475a88129c6e7b6a57a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:07.795 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.536197 secs\n",
      "2023-09-19 19:33:07.831 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 14}\n",
      "2023-09-19 19:33:07.832 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:07.832 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23751f3713b94521bc980da762b51f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:11.250 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.418527 secs\n",
      "2023-09-19 19:33:11.284 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 14}\n",
      "2023-09-19 19:33:11.285 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:11.285 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02270f3c1be646fdadc4840e88a41116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:15.924 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.639378 secs\n",
      "2023-09-19 19:33:15.961 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 14}\n",
      "2023-09-19 19:33:15.962 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:15.962 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b3523d19b6432d986b8118def70d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:19.598 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.635691 secs\n",
      "2023-09-19 19:33:19.636 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 14}\n",
      "2023-09-19 19:33:19.636 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:19.636 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766d3cd71d9847af9c5ccf4a09d3348f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:23.243 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.606946 secs\n",
      "2023-09-19 19:33:23.278 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 14}\n",
      "2023-09-19 19:33:23.279 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:23.280 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4534acb4d6b94625b249c709b0909d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:26.732 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.452297 secs\n",
      "2023-09-19 19:33:26.769 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 15}\n",
      "2023-09-19 19:33:26.769 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:26.769 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5a0a72c4164222a99e99b3d22b9d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:30.930 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.160329 secs\n",
      "2023-09-19 19:33:30.960 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 15}\n",
      "2023-09-19 19:33:30.960 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:30.960 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e58b7d65fb54ec6a3478c8d1ae700ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:35.507 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.546284 secs\n",
      "2023-09-19 19:33:35.544 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 15}\n",
      "2023-09-19 19:33:35.545 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:35.546 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc1bf83bee141fea2fd0c7663371826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:40.350 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.804135 secs\n",
      "2023-09-19 19:33:40.385 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 15}\n",
      "2023-09-19 19:33:40.385 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:40.386 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b4f722519b44c3a130a06575c52a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:44.227 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.841512 secs\n",
      "2023-09-19 19:33:44.266 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 15}\n",
      "2023-09-19 19:33:44.267 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:44.267 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c32f27a7464169ae57ff0b98515331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:47.730 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.462820 secs\n",
      "2023-09-19 19:33:47.760 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 16}\n",
      "2023-09-19 19:33:47.761 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:47.761 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a041fff5c94f0cbcbeeeb4233e674d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:51.174 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.412888 secs\n",
      "2023-09-19 19:33:51.206 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 16}\n",
      "2023-09-19 19:33:51.207 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:51.207 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed76d7f9d6aa4b93b42d9551db790f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:33:56.826 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.619213 secs\n",
      "2023-09-19 19:33:56.863 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 16}\n",
      "2023-09-19 19:33:56.863 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:33:56.863 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c0e9deb8f249e5b736da0936f71b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:00.684 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.821018 secs\n",
      "2023-09-19 19:34:00.721 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 16}\n",
      "2023-09-19 19:34:00.722 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:00.722 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a9cd51729a4619b3602e7e0db2f7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:03.720 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.998304 secs\n",
      "2023-09-19 19:34:03.755 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 16}\n",
      "2023-09-19 19:34:03.755 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:03.756 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594813e60da844e9bc85a8232e0e1120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:07.259 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.502841 secs\n",
      "2023-09-19 19:34:07.290 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 17}\n",
      "2023-09-19 19:34:07.291 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:07.292 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b46167876414a1b878638aea58f91f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:10.993 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.701516 secs\n",
      "2023-09-19 19:34:11.024 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 17}\n",
      "2023-09-19 19:34:11.024 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:11.025 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c48d67362aa433aa8104bee83697a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:16.848 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.823202 secs\n",
      "2023-09-19 19:34:16.893 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 17}\n",
      "2023-09-19 19:34:16.893 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:16.894 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9924d80916cd44e081aac773ed0de702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:21.098 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.203929 secs\n",
      "2023-09-19 19:34:21.155 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 17}\n",
      "2023-09-19 19:34:21.157 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:21.157 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b7dd87abae4fadaf4e732703823e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:24.586 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.429339 secs\n",
      "2023-09-19 19:34:24.620 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 17}\n",
      "2023-09-19 19:34:24.621 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:24.621 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d91c837e21434cb1c430eb590e80c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:28.638 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.016167 secs\n",
      "2023-09-19 19:34:28.667 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 18}\n",
      "2023-09-19 19:34:28.671 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:28.673 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722d278fdaba4b5e8d5f288d5e173c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:32.391 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.718829 secs\n",
      "2023-09-19 19:34:32.427 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 18}\n",
      "2023-09-19 19:34:32.433 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:32.437 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bbf0be308540ad97994dad8a9ba933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:38.231 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.794804 secs\n",
      "2023-09-19 19:34:38.277 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 18}\n",
      "2023-09-19 19:34:38.278 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:38.279 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f265bdacae4ca7aed6cd2eb3a47de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:42.236 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.957880 secs\n",
      "2023-09-19 19:34:42.271 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 18}\n",
      "2023-09-19 19:34:42.272 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:42.273 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c478169cb6644e7b285bc8f5d4042ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:45.446 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.173523 secs\n",
      "2023-09-19 19:34:45.477 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 18}\n",
      "2023-09-19 19:34:45.478 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:45.478 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88da839a01f4b66a5787d5a8c84d711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:48.952 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.474503 secs\n",
      "2023-09-19 19:34:48.986 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 19}\n",
      "2023-09-19 19:34:48.987 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:48.988 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b8ecf203b94a508139dba0d043d229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:52.926 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.938732 secs\n",
      "2023-09-19 19:34:52.958 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 19}\n",
      "2023-09-19 19:34:52.959 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:52.959 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab9261bb9414abf858e14fbe92c894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:34:56.572 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.613194 secs\n",
      "2023-09-19 19:34:56.610 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 19}\n",
      "2023-09-19 19:34:56.610 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:34:56.611 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f3ed97050f419db2b3c8fa47a25535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:00.181 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.570185 secs\n",
      "2023-09-19 19:35:00.215 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 19}\n",
      "2023-09-19 19:35:00.216 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:00.216 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410635c7331e4867a55aad4b980d914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:03.473 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.257090 secs\n",
      "2023-09-19 19:35:03.515 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.4, 'top_k': 19}\n",
      "2023-09-19 19:35:03.515 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:03.516 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026ffb55fcd4b498e9c4c12ea165ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:07.296 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.779804 secs\n",
      "2023-09-19 19:35:07.334 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 4}\n",
      "2023-09-19 19:35:07.342 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:07.343 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d1371f3b4e44a1a41279eb3ef77604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:10.600 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.256558 secs\n",
      "2023-09-19 19:35:10.631 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 4}\n",
      "2023-09-19 19:35:10.632 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:10.632 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf6733d3ff34d9f85c0c53384c88c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:14.396 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.764043 secs\n",
      "2023-09-19 19:35:14.470 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 4}\n",
      "2023-09-19 19:35:14.471 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:14.473 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10c625ac2ff45bb8bacbee716b360b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:17.359 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.886137 secs\n",
      "2023-09-19 19:35:17.404 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 4}\n",
      "2023-09-19 19:35:17.405 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:17.405 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75043539db24cedbb9eded9748123bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:20.541 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.135682 secs\n",
      "2023-09-19 19:35:20.576 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 4}\n",
      "2023-09-19 19:35:20.577 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:20.578 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eaa4d0e8c7c43f184ceffe73f10ce6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:24.801 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.223544 secs\n",
      "2023-09-19 19:35:24.842 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 5}\n",
      "2023-09-19 19:35:24.843 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:24.846 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5bb23ae41e4d01aa6a22854650fe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:28.408 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.562840 secs\n",
      "2023-09-19 19:35:28.440 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 5}\n",
      "2023-09-19 19:35:28.440 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:28.441 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1c233b60434928b67c4445ae577d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:32.652 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.211657 secs\n",
      "2023-09-19 19:35:32.687 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 5}\n",
      "2023-09-19 19:35:32.688 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:32.688 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f9b952a725407fb8220987af6a9831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:35.639 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.950203 secs\n",
      "2023-09-19 19:35:35.671 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 5}\n",
      "2023-09-19 19:35:35.677 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:35.681 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639fc96dc6c148b29c20c8a48fc9e2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:38.650 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.969235 secs\n",
      "2023-09-19 19:35:38.681 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 5}\n",
      "2023-09-19 19:35:38.681 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:38.682 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fe562d85654ad9bec15bc4f96dede5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:41.892 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.210345 secs\n",
      "2023-09-19 19:35:41.918 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 6}\n",
      "2023-09-19 19:35:41.918 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:41.919 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff1b9594d4e438ab237d9813382fb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:44.950 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.031560 secs\n",
      "2023-09-19 19:35:44.991 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 6}\n",
      "2023-09-19 19:35:44.991 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:44.991 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02439ed50a514c99b52656a4958a5562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:48.651 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.659843 secs\n",
      "2023-09-19 19:35:48.685 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 6}\n",
      "2023-09-19 19:35:48.686 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:48.686 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540587482b264f4e8843d8280c2f87ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:51.707 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.021350 secs\n",
      "2023-09-19 19:35:51.748 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 6}\n",
      "2023-09-19 19:35:51.749 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:51.751 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f27e1970d4b85b8261e03e8d6bd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:54.929 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.177764 secs\n",
      "2023-09-19 19:35:54.964 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 6}\n",
      "2023-09-19 19:35:54.965 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:54.965 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d461898dee65442a98a67fd860309342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:35:58.278 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.313293 secs\n",
      "2023-09-19 19:35:58.306 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 7}\n",
      "2023-09-19 19:35:58.307 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:35:58.310 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d8fe8aa02f4b72a24f795bce73dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:02.174 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.864293 secs\n",
      "2023-09-19 19:36:02.207 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 7}\n",
      "2023-09-19 19:36:02.208 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:02.209 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfd533ee3ce48cb93187d86ec2879f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:07.495 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.286059 secs\n",
      "2023-09-19 19:36:07.533 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 7}\n",
      "2023-09-19 19:36:07.533 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:07.534 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28e2a7dc84a45f7b051462eb2baaa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:10.744 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.210332 secs\n",
      "2023-09-19 19:36:10.816 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 7}\n",
      "2023-09-19 19:36:10.816 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:10.816 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fe2582840f45d9a28d0e2388d3042c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:13.839 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.022226 secs\n",
      "2023-09-19 19:36:13.873 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 7}\n",
      "2023-09-19 19:36:13.873 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:13.874 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c042ef691f444894e3f4ba7663aa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:17.194 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.320326 secs\n",
      "2023-09-19 19:36:17.243 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 8}\n",
      "2023-09-19 19:36:17.244 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:17.244 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb48b28c94744b2cb1c5e30355d9e643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:21.354 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.109566 secs\n",
      "2023-09-19 19:36:21.385 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 8}\n",
      "2023-09-19 19:36:21.385 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:21.386 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fee22ccb74ba4bc2fa9e2f630e994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:25.345 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.959099 secs\n",
      "2023-09-19 19:36:25.381 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 8}\n",
      "2023-09-19 19:36:25.382 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:25.383 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd62a252b1f485796dd1d86f5fcd10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:29.241 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.858534 secs\n",
      "2023-09-19 19:36:29.286 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 8}\n",
      "2023-09-19 19:36:29.287 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:29.287 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f88a648da04a5b9ecbcf3ce81bade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:32.792 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.505272 secs\n",
      "2023-09-19 19:36:32.828 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 8}\n",
      "2023-09-19 19:36:32.829 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:32.829 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581dfd376a004442b1610159715a5de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:36.492 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.663008 secs\n",
      "2023-09-19 19:36:36.528 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 9}\n",
      "2023-09-19 19:36:36.528 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:36.529 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf28b955a814d11b58ff41343d1f1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:40.753 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.224054 secs\n",
      "2023-09-19 19:36:40.786 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 9}\n",
      "2023-09-19 19:36:40.787 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:40.787 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473f951ba74441f28725d48a8d2bb431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:44.470 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.682732 secs\n",
      "2023-09-19 19:36:44.504 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 9}\n",
      "2023-09-19 19:36:44.504 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:44.505 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801e8e35a61f42668334695b216ee84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:47.873 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.368025 secs\n",
      "2023-09-19 19:36:47.908 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 9}\n",
      "2023-09-19 19:36:47.908 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:47.909 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc8dafe94e4dae9d4ef2cb4fb58068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:51.895 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.986284 secs\n",
      "2023-09-19 19:36:51.932 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 9}\n",
      "2023-09-19 19:36:51.933 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:51.933 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aff25c8267f4488b4cb3ac47a21603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:54.782 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.848487 secs\n",
      "2023-09-19 19:36:54.818 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 10}\n",
      "2023-09-19 19:36:54.820 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:54.820 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495d2abdf8b84f618f6488380d48baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:36:59.950 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.130044 secs\n",
      "2023-09-19 19:36:59.986 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 10}\n",
      "2023-09-19 19:36:59.987 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:36:59.987 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4313b4bcd96a4614b72c46d62c7f471c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:05.178 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.190362 secs\n",
      "2023-09-19 19:37:05.215 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 10}\n",
      "2023-09-19 19:37:05.215 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:05.216 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a41504be4974abd8c9717cb483e22b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:08.364 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.147823 secs\n",
      "2023-09-19 19:37:08.398 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 10}\n",
      "2023-09-19 19:37:08.398 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:08.398 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeee401879d46619e00acccc051adbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:11.765 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.366088 secs\n",
      "2023-09-19 19:37:11.822 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 10}\n",
      "2023-09-19 19:37:11.823 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:11.823 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47b3bc87555437caa3c3d54a7ec277c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:14.855 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.031537 secs\n",
      "2023-09-19 19:37:14.890 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 11}\n",
      "2023-09-19 19:37:14.891 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:14.892 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9af95e33e8b4096a0ac80de40e805c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:19.689 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.797239 secs\n",
      "2023-09-19 19:37:19.723 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 11}\n",
      "2023-09-19 19:37:19.724 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:19.724 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f910624924564d3e9a904e422467ffdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:23.947 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.222933 secs\n",
      "2023-09-19 19:37:23.982 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 11}\n",
      "2023-09-19 19:37:23.982 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:23.983 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581cf6c95f3b4905bcdfc0ac09c8114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:27.334 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.350963 secs\n",
      "2023-09-19 19:37:27.379 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 11}\n",
      "2023-09-19 19:37:27.379 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:27.380 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a53390349041358cd38f60178c2b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:31.076 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.696882 secs\n",
      "2023-09-19 19:37:31.109 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 11}\n",
      "2023-09-19 19:37:31.110 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:31.111 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e023aa81f32466d8bc1fa32323725c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:34.438 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.326962 secs\n",
      "2023-09-19 19:37:34.470 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 12}\n",
      "2023-09-19 19:37:34.471 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:34.471 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fd8fe51a804d53acc85de5477932fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:38.192 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.721058 secs\n",
      "2023-09-19 19:37:38.225 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 12}\n",
      "2023-09-19 19:37:38.225 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:38.225 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a9227e24b041ef86ac3ad1d5a3d6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:43.448 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.222321 secs\n",
      "2023-09-19 19:37:43.501 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 12}\n",
      "2023-09-19 19:37:43.503 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:43.505 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abf9c069e1b4f89863ef740963d2cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:46.893 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.388880 secs\n",
      "2023-09-19 19:37:46.927 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 12}\n",
      "2023-09-19 19:37:46.929 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:46.929 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10755c91bc384ed89802bab88cd02f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:50.351 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.421733 secs\n",
      "2023-09-19 19:37:50.383 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 12}\n",
      "2023-09-19 19:37:50.384 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:50.384 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0a881ba8ad4680b7f16e88c0e201b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:53.673 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.288677 secs\n",
      "2023-09-19 19:37:53.702 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 13}\n",
      "2023-09-19 19:37:53.703 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:53.704 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e67f26f5334d718a6a681d1b96bf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:37:57.925 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.220886 secs\n",
      "2023-09-19 19:37:57.968 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 13}\n",
      "2023-09-19 19:37:57.969 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:37:57.970 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb47f4eb98249c483a46ac86f42670c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:02.846 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.876586 secs\n",
      "2023-09-19 19:38:02.885 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 13}\n",
      "2023-09-19 19:38:02.885 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:02.886 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2615a02a8aa4095a300ca3da0ddfa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:06.477 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.591532 secs\n",
      "2023-09-19 19:38:06.511 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 13}\n",
      "2023-09-19 19:38:06.512 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:06.515 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c097847f978b4962a19e9a157fcedf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:10.061 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.546358 secs\n",
      "2023-09-19 19:38:10.093 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 13}\n",
      "2023-09-19 19:38:10.093 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:10.094 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e5647007b14984982cc98e5d817394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:13.734 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.639940 secs\n",
      "2023-09-19 19:38:13.768 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 14}\n",
      "2023-09-19 19:38:13.769 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:13.770 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7744a46031644ab8215336d002aa98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:17.335 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.565478 secs\n",
      "2023-09-19 19:38:17.371 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 14}\n",
      "2023-09-19 19:38:17.371 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:17.372 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95e2e83eedc4085b7bd293f31ddab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:21.700 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.328185 secs\n",
      "2023-09-19 19:38:21.739 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 14}\n",
      "2023-09-19 19:38:21.739 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:21.740 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab05c97417f48e5ae311a1656e5682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:25.250 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.509721 secs\n",
      "2023-09-19 19:38:25.284 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 14}\n",
      "2023-09-19 19:38:25.285 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:25.285 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f776bef8f44c3f8f6857be0b046ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:28.598 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.313621 secs\n",
      "2023-09-19 19:38:28.631 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 14}\n",
      "2023-09-19 19:38:28.631 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:28.632 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e5ee8a7744f2c9d2b2691241a826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:32.012 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.380377 secs\n",
      "2023-09-19 19:38:32.048 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 15}\n",
      "2023-09-19 19:38:32.048 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:32.049 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75aa24c19df846e5ba931065b7ac95d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:36.417 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.368758 secs\n",
      "2023-09-19 19:38:36.452 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 15}\n",
      "2023-09-19 19:38:36.452 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:36.453 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6342a3030d9541f1bee2f1504bf80755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:41.101 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.647869 secs\n",
      "2023-09-19 19:38:41.138 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 15}\n",
      "2023-09-19 19:38:41.139 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:41.139 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a9c9efab4d4b7bb07e5fe682af39f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:45.734 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.594688 secs\n",
      "2023-09-19 19:38:45.775 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 15}\n",
      "2023-09-19 19:38:45.776 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:45.776 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5346c7fb2cb04d98a7afc3e5c90dcded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:49.196 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.419542 secs\n",
      "2023-09-19 19:38:49.229 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 15}\n",
      "2023-09-19 19:38:49.230 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:49.230 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c87225341a413bb78d3c6632e532b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:52.505 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.274653 secs\n",
      "2023-09-19 19:38:52.536 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 16}\n",
      "2023-09-19 19:38:52.536 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:52.537 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504520f930c049079225d8a54d25349c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:38:55.635 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.098433 secs\n",
      "2023-09-19 19:38:55.667 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 16}\n",
      "2023-09-19 19:38:55.667 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:38:55.668 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2554b0fa433746928e1821db8d648734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:00.609 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.941406 secs\n",
      "2023-09-19 19:39:00.644 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 16}\n",
      "2023-09-19 19:39:00.645 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:00.645 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aefadb01d094006a763c1a46d59dda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:04.231 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.585957 secs\n",
      "2023-09-19 19:39:04.265 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 16}\n",
      "2023-09-19 19:39:04.266 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:04.266 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3288a7e29f48cebd3b39c6aec1589f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:07.034 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.767642 secs\n",
      "2023-09-19 19:39:07.066 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 16}\n",
      "2023-09-19 19:39:07.067 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:07.067 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cc127338624cc18ae2994a814a4387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:10.276 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.208487 secs\n",
      "2023-09-19 19:39:10.329 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 17}\n",
      "2023-09-19 19:39:10.340 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:10.346 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df52404c920b4ff696a71b8c42955e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:13.465 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.118977 secs\n",
      "2023-09-19 19:39:13.497 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 17}\n",
      "2023-09-19 19:39:13.498 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:13.498 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a7efef0b0f4d5fa5c0d637b8355a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:18.457 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.959348 secs\n",
      "2023-09-19 19:39:18.495 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 17}\n",
      "2023-09-19 19:39:18.496 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:18.496 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76be1a9200c342a8b8ae5d7743a957ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:21.760 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.263761 secs\n",
      "2023-09-19 19:39:21.792 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 17}\n",
      "2023-09-19 19:39:21.793 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:21.793 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc52191dbadc4af29f686f0f62dbb15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:24.504 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.711293 secs\n",
      "2023-09-19 19:39:24.538 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 17}\n",
      "2023-09-19 19:39:24.538 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:24.538 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f00bdcf3f4bd59f9ccadfa2e397f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:27.771 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.232698 secs\n",
      "2023-09-19 19:39:27.803 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 18}\n",
      "2023-09-19 19:39:27.803 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:27.804 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb126b728e44339ad1276030cdcc45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:30.942 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.138400 secs\n",
      "2023-09-19 19:39:30.975 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 18}\n",
      "2023-09-19 19:39:30.975 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:30.976 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2ec288816441ae8ec6f2113ac50b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:35.887 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.911414 secs\n",
      "2023-09-19 19:39:35.925 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 18}\n",
      "2023-09-19 19:39:35.926 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:35.926 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15da975675554357a450f64a0d8bf985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:39.151 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.225118 secs\n",
      "2023-09-19 19:39:39.185 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 18}\n",
      "2023-09-19 19:39:39.185 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:39.186 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a192acdc98f47cea92fe1965542fcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:41.964 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.778494 secs\n",
      "2023-09-19 19:39:42.004 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 18}\n",
      "2023-09-19 19:39:42.005 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:42.005 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d74aef165642a894638070db518bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:45.258 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.252547 secs\n",
      "2023-09-19 19:39:45.287 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 19}\n",
      "2023-09-19 19:39:45.287 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:45.288 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c993db83af4a07a937546ca8f6cc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:48.858 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.569930 secs\n",
      "2023-09-19 19:39:48.890 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 19}\n",
      "2023-09-19 19:39:48.890 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:48.891 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7b2787d6fc4426be245afe09c5188a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:51.985 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.093963 secs\n",
      "2023-09-19 19:39:52.021 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 19}\n",
      "2023-09-19 19:39:52.021 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:52.022 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97013217e9fe4c04a6027a0442b7b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:55.170 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.148662 secs\n",
      "2023-09-19 19:39:55.205 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 19}\n",
      "2023-09-19 19:39:55.205 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:55.206 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892e50ffbf8646fbad8c5cb3de649652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:39:57.940 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.734190 secs\n",
      "2023-09-19 19:39:57.973 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.5, 'top_k': 19}\n",
      "2023-09-19 19:39:57.973 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:39:57.974 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449acc808e26452d943b05d4db79a400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:01.219 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.245239 secs\n",
      "2023-09-19 19:40:01.250 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 4}\n",
      "2023-09-19 19:40:01.250 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:01.251 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b37234be0284db2bcd4afc446a6ad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:04.095 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.843732 secs\n",
      "2023-09-19 19:40:04.126 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 4}\n",
      "2023-09-19 19:40:04.127 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:04.127 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc644e3f800b4f239868381c4a200e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:07.076 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.949267 secs\n",
      "2023-09-19 19:40:07.110 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 4}\n",
      "2023-09-19 19:40:07.111 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:07.111 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c469f8ad8e4d49fea1f8472b183b9666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:09.314 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.202402 secs\n",
      "2023-09-19 19:40:09.347 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 4}\n",
      "2023-09-19 19:40:09.348 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:09.348 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df44b60f4e99484f9ae067af2f9ef23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:12.135 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.786600 secs\n",
      "2023-09-19 19:40:12.167 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 4}\n",
      "2023-09-19 19:40:12.168 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:12.168 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d648e4c58145d483262bb833402326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:15.515 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.346593 secs\n",
      "2023-09-19 19:40:15.545 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 5}\n",
      "2023-09-19 19:40:15.546 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:15.546 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f004158a1c4f43f3864eaf18844fef99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:18.861 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.314791 secs\n",
      "2023-09-19 19:40:18.891 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 5}\n",
      "2023-09-19 19:40:18.892 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:18.892 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691a448455ef4ad88ffb37119634a336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:22.776 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.883591 secs\n",
      "2023-09-19 19:40:22.829 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 5}\n",
      "2023-09-19 19:40:22.830 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:22.831 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f810ec099746d494bc354aa74cdcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:25.732 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.901038 secs\n",
      "2023-09-19 19:40:25.766 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 5}\n",
      "2023-09-19 19:40:25.766 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:25.767 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bef82eb038e48bfa3cd818225bb48b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:28.540 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.772715 secs\n",
      "2023-09-19 19:40:28.572 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 5}\n",
      "2023-09-19 19:40:28.572 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:28.573 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a5a51ccbb46319ada033124b48498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:31.637 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.064463 secs\n",
      "2023-09-19 19:40:31.666 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 6}\n",
      "2023-09-19 19:40:31.667 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:31.667 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1a88dd8fb44b8cbee19fecc8071f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:34.537 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.869902 secs\n",
      "2023-09-19 19:40:34.567 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 6}\n",
      "2023-09-19 19:40:34.568 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:34.568 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c220a80c3bd648b491e290535b061217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:37.877 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.308538 secs\n",
      "2023-09-19 19:40:37.911 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 6}\n",
      "2023-09-19 19:40:37.911 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:37.912 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefa703f7da74a3683e2ecfdd22cfd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:40.525 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.612944 secs\n",
      "2023-09-19 19:40:40.559 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 6}\n",
      "2023-09-19 19:40:40.559 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:40.560 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c7bf392d7d43dc9b77e47935f563b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:43.323 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.763426 secs\n",
      "2023-09-19 19:40:43.356 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 6}\n",
      "2023-09-19 19:40:43.357 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:43.357 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cb682ad3a14a4988b6877289e8324b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:46.442 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.084865 secs\n",
      "2023-09-19 19:40:46.470 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 7}\n",
      "2023-09-19 19:40:46.471 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:46.471 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65867fc0a628402ba445394f6f87708f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:50.050 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.578816 secs\n",
      "2023-09-19 19:40:50.081 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 7}\n",
      "2023-09-19 19:40:50.081 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:50.082 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01065d1a5e174f4ca56d4f2d338ad817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:54.952 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.870679 secs\n",
      "2023-09-19 19:40:54.990 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 7}\n",
      "2023-09-19 19:40:54.990 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:54.991 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adf2a1d065746669cba46bb2d55f822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:40:58.387 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.395849 secs\n",
      "2023-09-19 19:40:58.429 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 7}\n",
      "2023-09-19 19:40:58.429 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:40:58.430 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3967e023816f404b8fb503de8288e0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:01.340 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.910434 secs\n",
      "2023-09-19 19:41:01.372 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 7}\n",
      "2023-09-19 19:41:01.373 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:01.373 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba48276f4a14564a77c01bd8dad2047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:04.976 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.603127 secs\n",
      "2023-09-19 19:41:05.006 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 8}\n",
      "2023-09-19 19:41:05.006 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:05.007 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78afb51459d4c8db865d92e9a93bcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:08.900 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.893116 secs\n",
      "2023-09-19 19:41:08.933 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 8}\n",
      "2023-09-19 19:41:08.934 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:08.934 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4908ffa83dd643ccad8177f0d2b32626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:12.574 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.639777 secs\n",
      "2023-09-19 19:41:12.609 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 8}\n",
      "2023-09-19 19:41:12.610 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:12.610 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f075ee9734562bb1ecbd464f61c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:15.862 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.251871 secs\n",
      "2023-09-19 19:41:15.894 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 8}\n",
      "2023-09-19 19:41:15.895 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:15.895 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c25c38780c423f9fb15a991e254e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:19.408 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.513068 secs\n",
      "2023-09-19 19:41:19.442 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 8}\n",
      "2023-09-19 19:41:19.442 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:19.443 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6b0fb65ecf474a981541b10afa5abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:22.626 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.183178 secs\n",
      "2023-09-19 19:41:22.654 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 9}\n",
      "2023-09-19 19:41:22.654 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:22.654 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b972e81b388f40ea89177c1e8c34aacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:26.234 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.579857 secs\n",
      "2023-09-19 19:41:26.266 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 9}\n",
      "2023-09-19 19:41:26.267 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:26.268 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95223df743f4e04b86a58d58cdd274c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:29.529 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.260706 secs\n",
      "2023-09-19 19:41:29.564 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 9}\n",
      "2023-09-19 19:41:29.564 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:29.565 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d50e6d31f7462e95dc3197b2d8356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:32.777 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.212092 secs\n",
      "2023-09-19 19:41:32.812 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 9}\n",
      "2023-09-19 19:41:32.813 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:32.813 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6890feb83e44fa3a7ddc7f6b16dd6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:36.234 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.420960 secs\n",
      "2023-09-19 19:41:36.266 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 9}\n",
      "2023-09-19 19:41:36.268 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:36.271 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eef8dc097034b4b81ebfd3e1478f247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:38.673 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.401750 secs\n",
      "2023-09-19 19:41:38.702 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 10}\n",
      "2023-09-19 19:41:38.703 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:38.704 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20009dedca6403ea4660b6b64cb0bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:43.117 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.413634 secs\n",
      "2023-09-19 19:41:43.150 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 10}\n",
      "2023-09-19 19:41:43.151 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:43.151 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64dc5f7075b4efc90ea444e32e64e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:47.279 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.127496 secs\n",
      "2023-09-19 19:41:47.316 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 10}\n",
      "2023-09-19 19:41:47.317 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:47.317 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa34d51f8f3c418b8d718caeef822c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:50.087 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.770162 secs\n",
      "2023-09-19 19:41:50.122 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 10}\n",
      "2023-09-19 19:41:50.122 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:50.122 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8d4c517ae246b8b6d377b7dfd4eb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:53.083 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.960617 secs\n",
      "2023-09-19 19:41:53.116 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 10}\n",
      "2023-09-19 19:41:53.116 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:53.117 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4182716cde9f43b3bc7e2280209ef80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:41:55.630 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.513340 secs\n",
      "2023-09-19 19:41:55.663 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 11}\n",
      "2023-09-19 19:41:55.663 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:41:55.664 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db67d6662d7243aeb033a23b1a0ee63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:00.018 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.354398 secs\n",
      "2023-09-19 19:42:00.054 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 11}\n",
      "2023-09-19 19:42:00.055 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:00.055 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03204c27e13f407b80a197b4fa7906af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:04.557 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.501942 secs\n",
      "2023-09-19 19:42:04.593 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 11}\n",
      "2023-09-19 19:42:04.594 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:04.594 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180e2c88e92f47f4a4177ca2e46f229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:08.042 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.447578 secs\n",
      "2023-09-19 19:42:08.075 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 11}\n",
      "2023-09-19 19:42:08.076 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:08.076 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7715fbf9664a4f93f5a8df50053720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:11.480 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.403556 secs\n",
      "2023-09-19 19:42:11.511 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 11}\n",
      "2023-09-19 19:42:11.512 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:11.512 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e85677d7bd4153ad067fe6aa1e237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:14.751 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.239172 secs\n",
      "2023-09-19 19:42:14.782 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 12}\n",
      "2023-09-19 19:42:14.783 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:14.783 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cd0cab0afc49a5b64305ea7dbc9858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:18.263 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.480551 secs\n",
      "2023-09-19 19:42:18.294 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 12}\n",
      "2023-09-19 19:42:18.295 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:18.295 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86af9f0811c84a04b8cb5f4404d5cb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:23.397 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.101811 secs\n",
      "2023-09-19 19:42:23.435 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 12}\n",
      "2023-09-19 19:42:23.436 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:23.436 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519fd36a29d349fe809eaf4784a908d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:26.769 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.332482 secs\n",
      "2023-09-19 19:42:26.804 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 12}\n",
      "2023-09-19 19:42:26.804 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:26.805 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3c895a05fe48da827cd1c7d2ba994a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:30.267 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.462238 secs\n",
      "2023-09-19 19:42:30.301 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 12}\n",
      "2023-09-19 19:42:30.301 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:30.301 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706bbc4b15946ff92ffa7bd41eaf917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:33.566 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.264520 secs\n",
      "2023-09-19 19:42:33.596 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 13}\n",
      "2023-09-19 19:42:33.597 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:33.597 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266bb5ea5fe44faf8387623cad908605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:37.055 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.457604 secs\n",
      "2023-09-19 19:42:37.088 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 13}\n",
      "2023-09-19 19:42:37.088 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:37.089 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1097e90caccb4b998969e83bfd73ca69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:42.558 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.469334 secs\n",
      "2023-09-19 19:42:42.599 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 13}\n",
      "2023-09-19 19:42:42.600 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:42.600 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eed08e4869d42ea9049eb02521e5669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:46.491 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.890593 secs\n",
      "2023-09-19 19:42:46.525 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 13}\n",
      "2023-09-19 19:42:46.526 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:46.526 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23469bd24bb0467a99d754744ac846fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:50.226 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.699594 secs\n",
      "2023-09-19 19:42:50.275 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 13}\n",
      "2023-09-19 19:42:50.276 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:50.276 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17748c1edf8346c8b5a831c704a0fd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:53.709 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.432389 secs\n",
      "2023-09-19 19:42:53.739 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 14}\n",
      "2023-09-19 19:42:53.740 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:53.742 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c234d0a115e24157a70cb15af2c80615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:42:57.266 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.523669 secs\n",
      "2023-09-19 19:42:57.297 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 14}\n",
      "2023-09-19 19:42:57.298 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:42:57.298 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0149e330f924f06a26f7f256bd6fc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:01.844 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.546068 secs\n",
      "2023-09-19 19:43:01.881 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 14}\n",
      "2023-09-19 19:43:01.881 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:01.882 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b434600775346ccb4387a1695bd7760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:05.835 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.953803 secs\n",
      "2023-09-19 19:43:05.869 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 14}\n",
      "2023-09-19 19:43:05.870 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:05.870 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c225713cc6e14ee3a57f515c3537d1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:09.320 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.449945 secs\n",
      "2023-09-19 19:43:09.353 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 14}\n",
      "2023-09-19 19:43:09.353 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:09.354 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda2ce35253b4fc9860f0dc379b371b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:12.839 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.485266 secs\n",
      "2023-09-19 19:43:12.868 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 15}\n",
      "2023-09-19 19:43:12.868 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:12.869 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8468be3770634267813a66df97e9149b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:17.235 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.366016 secs\n",
      "2023-09-19 19:43:17.268 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 15}\n",
      "2023-09-19 19:43:17.268 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:17.269 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c858f0c31e46c2b8b778ef2446b19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:22.220 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.951836 secs\n",
      "2023-09-19 19:43:22.256 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 15}\n",
      "2023-09-19 19:43:22.257 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:22.257 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b6a705fda4433ea7a889767b03da6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:26.661 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.403665 secs\n",
      "2023-09-19 19:43:26.697 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 15}\n",
      "2023-09-19 19:43:26.697 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:26.697 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49928db1c4734ffbbe064b6080ff6f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:29.930 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.232311 secs\n",
      "2023-09-19 19:43:29.962 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 15}\n",
      "2023-09-19 19:43:29.962 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:29.962 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0024a5af30094972ba827f691f86adb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:33.284 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.321204 secs\n",
      "2023-09-19 19:43:33.312 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 16}\n",
      "2023-09-19 19:43:33.312 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:33.313 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150bbe4715c478290afca26fb9c8d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:36.821 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.508227 secs\n",
      "2023-09-19 19:43:36.851 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 16}\n",
      "2023-09-19 19:43:36.852 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:36.852 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8de44c69cd1466787a75200b210d2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:42.225 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.373067 secs\n",
      "2023-09-19 19:43:42.264 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 16}\n",
      "2023-09-19 19:43:42.264 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:42.265 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a75f2c769545f49ab6eb733bd7297b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:46.084 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.819068 secs\n",
      "2023-09-19 19:43:46.118 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 16}\n",
      "2023-09-19 19:43:46.118 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:46.119 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe97efc43ec46ad8c197048e2fda572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:49.050 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.931083 secs\n",
      "2023-09-19 19:43:49.084 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 16}\n",
      "2023-09-19 19:43:49.085 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:49.085 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7f8173ebd747579abcee7696127960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:52.489 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.404168 secs\n",
      "2023-09-19 19:43:52.519 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 17}\n",
      "2023-09-19 19:43:52.519 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:52.520 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c218806341394a81a1b7929a0c6e8b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:43:55.695 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.175778 secs\n",
      "2023-09-19 19:43:55.726 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 17}\n",
      "2023-09-19 19:43:55.727 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:43:55.727 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334115715dda4756a4dd8f1bda9a170d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:00.783 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.055978 secs\n",
      "2023-09-19 19:44:00.821 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 17}\n",
      "2023-09-19 19:44:00.821 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:00.821 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0b136689cd432fa8cf736476d9391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:04.774 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.952954 secs\n",
      "2023-09-19 19:44:04.810 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 17}\n",
      "2023-09-19 19:44:04.811 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:04.811 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d55f877d4a44772a3434683f7c9c548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:07.703 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.891953 secs\n",
      "2023-09-19 19:44:07.736 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 17}\n",
      "2023-09-19 19:44:07.737 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:07.737 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4375bfddfb7643e0857494c61895cdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:11.062 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.324145 secs\n",
      "2023-09-19 19:44:11.091 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 18}\n",
      "2023-09-19 19:44:11.092 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:11.092 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067ab66d5d854d09837c09d2c4da7670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:14.418 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.325956 secs\n",
      "2023-09-19 19:44:14.456 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 18}\n",
      "2023-09-19 19:44:14.457 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:14.457 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38797cb953347af865c51a368435062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:19.810 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.352748 secs\n",
      "2023-09-19 19:44:19.845 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 18}\n",
      "2023-09-19 19:44:19.845 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:19.845 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c303c0529071462099ef2928693d99f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:23.628 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.782850 secs\n",
      "2023-09-19 19:44:23.663 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 18}\n",
      "2023-09-19 19:44:23.664 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:23.664 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99934ce9c6414e6c8ca4e59d5ce16c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:26.687 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.023360 secs\n",
      "2023-09-19 19:44:26.720 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 18}\n",
      "2023-09-19 19:44:26.720 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:26.721 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f9860f63154331b8a0a1052374ef63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:30.100 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.379246 secs\n",
      "2023-09-19 19:44:30.129 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 19}\n",
      "2023-09-19 19:44:30.130 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:30.131 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3ed604919f4fc9a72faa8caed907b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:33.893 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.762326 secs\n",
      "2023-09-19 19:44:33.926 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 19}\n",
      "2023-09-19 19:44:33.926 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:33.927 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12b0aba264c49119338b0b9cc4a9f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:37.180 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.253058 secs\n",
      "2023-09-19 19:44:37.223 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 19}\n",
      "2023-09-19 19:44:37.223 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:37.224 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8f6d02a9cf4db8b2f4f2bddc64305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:40.813 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.589697 secs\n",
      "2023-09-19 19:44:40.846 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 19}\n",
      "2023-09-19 19:44:40.846 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:40.846 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f916c4206e4911978d0c6f949e102e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:43.706 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.859878 secs\n",
      "2023-09-19 19:44:43.740 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.6, 'top_k': 19}\n",
      "2023-09-19 19:44:43.740 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:43.740 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f89a9ff74464d028f56e7c17174e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:47.022 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.281404 secs\n",
      "2023-09-19 19:44:47.052 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 4}\n",
      "2023-09-19 19:44:47.053 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:47.053 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a31270c7a3a44419d6a80f70097e682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:49.916 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.862920 secs\n",
      "2023-09-19 19:44:49.947 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 4}\n",
      "2023-09-19 19:44:49.947 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:49.948 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7958719d9941889e2defee25b59979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:53.074 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.126826 secs\n",
      "2023-09-19 19:44:53.110 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 4}\n",
      "2023-09-19 19:44:53.110 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:53.111 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797274e53fd447ecad74736254e7e3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:55.384 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.273337 secs\n",
      "2023-09-19 19:44:55.417 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 4}\n",
      "2023-09-19 19:44:55.418 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:55.418 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21fbd1937d5494689f383c09333a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:44:58.304 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.885846 secs\n",
      "2023-09-19 19:44:58.337 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 4}\n",
      "2023-09-19 19:44:58.339 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:44:58.342 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751a942c877146558ce8e22fcaad4a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:02.261 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.919555 secs\n",
      "2023-09-19 19:45:02.292 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 5}\n",
      "2023-09-19 19:45:02.292 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:02.293 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dabf1f551fc47f2919eab5f79d8d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:05.803 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.510724 secs\n",
      "2023-09-19 19:45:05.834 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 5}\n",
      "2023-09-19 19:45:05.835 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:05.835 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9691263c5cc34dfaa25dfbabf36a28d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:10.143 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.307108 secs\n",
      "2023-09-19 19:45:10.177 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 5}\n",
      "2023-09-19 19:45:10.177 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:10.178 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233666debb214f42bfd560e92ddecf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:13.203 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.025266 secs\n",
      "2023-09-19 19:45:13.237 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 5}\n",
      "2023-09-19 19:45:13.237 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:13.238 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efc2ed4c6ee4d38a58a47e226cd08f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:16.190 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.952108 secs\n",
      "2023-09-19 19:45:16.241 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 5}\n",
      "2023-09-19 19:45:16.241 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:16.242 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db79d28e77cb4f28bbbfa0f0832ac88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:19.550 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.307928 secs\n",
      "2023-09-19 19:45:19.578 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 6}\n",
      "2023-09-19 19:45:19.579 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:19.579 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8cbb5c71e54231a9447fd73d56e4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:22.727 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.147862 secs\n",
      "2023-09-19 19:45:22.758 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 6}\n",
      "2023-09-19 19:45:22.759 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:22.759 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f056409af1145399cb9b14be51cf802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:26.242 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.482403 secs\n",
      "2023-09-19 19:45:26.274 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 6}\n",
      "2023-09-19 19:45:26.274 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:26.275 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a830f6d03af4d448be4d8f2aef7d3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:28.908 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.632528 secs\n",
      "2023-09-19 19:45:28.946 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 6}\n",
      "2023-09-19 19:45:28.947 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:28.947 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869682b1185548cc88ed153f51ba9595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:31.781 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.833174 secs\n",
      "2023-09-19 19:45:31.813 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 6}\n",
      "2023-09-19 19:45:31.814 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:31.814 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dced1baaed54e5e8e302b31a9138a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:35.036 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.221520 secs\n",
      "2023-09-19 19:45:35.065 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 7}\n",
      "2023-09-19 19:45:35.066 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:35.066 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650ee4761944ae49f0d0207d2ba0e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:38.914 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.847604 secs\n",
      "2023-09-19 19:45:38.945 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 7}\n",
      "2023-09-19 19:45:38.946 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:38.947 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7b8a66f7f44c784821b9d2f12b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:43.998 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.051050 secs\n",
      "2023-09-19 19:45:44.034 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 7}\n",
      "2023-09-19 19:45:44.035 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:44.035 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9d6fc6e0414647b7c471892a768eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:47.031 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.995769 secs\n",
      "2023-09-19 19:45:47.063 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 7}\n",
      "2023-09-19 19:45:47.064 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:47.064 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b901ed8b50413f9a40fffc02b0e90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:49.890 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.825846 secs\n",
      "2023-09-19 19:45:49.924 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 7}\n",
      "2023-09-19 19:45:49.924 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:49.925 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f031f73b4904fe28fa4dd8957d73a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:53.198 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.273409 secs\n",
      "2023-09-19 19:45:53.227 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 8}\n",
      "2023-09-19 19:45:53.227 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:53.227 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d645319b517b4b66a2b8dfbc21c403b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:45:56.909 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.681322 secs\n",
      "2023-09-19 19:45:56.940 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 8}\n",
      "2023-09-19 19:45:56.941 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:45:56.941 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f367021a27d49cdad6289469dd09552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:00.320 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.378557 secs\n",
      "2023-09-19 19:46:00.353 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 8}\n",
      "2023-09-19 19:46:00.354 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:00.355 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2c5ac0a0354db5bb160cb325a81bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:03.684 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.329691 secs\n",
      "2023-09-19 19:46:03.718 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 8}\n",
      "2023-09-19 19:46:03.719 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:03.722 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4d347fd39541e99c8110c7d864b659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:07.138 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.415748 secs\n",
      "2023-09-19 19:46:07.174 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 8}\n",
      "2023-09-19 19:46:07.175 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:07.175 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3060195193cb45e8bd719bfc116276d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:10.535 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.360040 secs\n",
      "2023-09-19 19:46:10.565 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 9}\n",
      "2023-09-19 19:46:10.566 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:10.566 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b80f996c7b4aeead5236d8bbab5ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:14.403 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.836667 secs\n",
      "2023-09-19 19:46:14.436 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 9}\n",
      "2023-09-19 19:46:14.436 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:14.437 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9cabe2a834fa2b0ef9da3fcf1749a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:18.037 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.600683 secs\n",
      "2023-09-19 19:46:18.074 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 9}\n",
      "2023-09-19 19:46:18.075 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:18.075 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0702be3f010a41ec991a08afdca6d7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:21.382 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.306666 secs\n",
      "2023-09-19 19:46:21.415 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 9}\n",
      "2023-09-19 19:46:21.415 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:21.416 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249725d6cdfd4a2bac6184dbfb0d668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:24.809 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.393078 secs\n",
      "2023-09-19 19:46:24.842 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 9}\n",
      "2023-09-19 19:46:24.843 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:24.844 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad95f45179047b0a037ae77ea505bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:27.234 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.390209 secs\n",
      "2023-09-19 19:46:27.263 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 10}\n",
      "2023-09-19 19:46:27.264 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:27.264 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a62b8011f44317996bb88d40388b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:31.569 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.305187 secs\n",
      "2023-09-19 19:46:31.601 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 10}\n",
      "2023-09-19 19:46:31.602 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:31.602 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e98103496b46208009de3f6e33fcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:35.743 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.141330 secs\n",
      "2023-09-19 19:46:35.779 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 10}\n",
      "2023-09-19 19:46:35.779 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:35.779 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d94a87291b548f0b91ef79b6fcfb369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:38.727 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.947984 secs\n",
      "2023-09-19 19:46:38.761 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 10}\n",
      "2023-09-19 19:46:38.761 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:38.765 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798acbd5b4064ba68ff34e14a3a42b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:41.722 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.957773 secs\n",
      "2023-09-19 19:46:41.755 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 10}\n",
      "2023-09-19 19:46:41.756 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:41.756 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc96d5d51694fbca7c63d935d5ceb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:44.235 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.478400 secs\n",
      "2023-09-19 19:46:44.263 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 11}\n",
      "2023-09-19 19:46:44.264 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:44.264 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d62b1caa154ebd844b9c036e38c246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:48.555 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.290691 secs\n",
      "2023-09-19 19:46:48.588 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 11}\n",
      "2023-09-19 19:46:48.589 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:48.589 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13a7395ec244af183acea99a194eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:52.712 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.122584 secs\n",
      "2023-09-19 19:46:52.748 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 11}\n",
      "2023-09-19 19:46:52.748 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:52.749 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1241d323cb6f425da058d1203848b8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:55.919 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.170389 secs\n",
      "2023-09-19 19:46:55.952 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 11}\n",
      "2023-09-19 19:46:55.952 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:55.953 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74693efa4e9448dab40de680750d4684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:46:59.449 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.496299 secs\n",
      "2023-09-19 19:46:59.482 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 11}\n",
      "2023-09-19 19:46:59.483 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:46:59.483 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9880e9ade4c8ebca665116a8bd2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:02.848 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.364588 secs\n",
      "2023-09-19 19:47:02.878 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 12}\n",
      "2023-09-19 19:47:02.879 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:02.879 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf473d81cd39408497817dee0faba82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:06.353 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.474440 secs\n",
      "2023-09-19 19:47:06.385 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 12}\n",
      "2023-09-19 19:47:06.386 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:06.386 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8607a47bdf5945b5aa3f9fd4d1b8e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:11.563 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.176940 secs\n",
      "2023-09-19 19:47:11.601 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 12}\n",
      "2023-09-19 19:47:11.601 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:11.602 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919ec17055954be384806ab2bb3722e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:15.067 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.464873 secs\n",
      "2023-09-19 19:47:15.101 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 12}\n",
      "2023-09-19 19:47:15.102 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:15.102 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03103275239849a797d3fc140f0c90b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:18.752 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.650144 secs\n",
      "2023-09-19 19:47:18.786 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 12}\n",
      "2023-09-19 19:47:18.787 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:18.787 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6c2819cc7c486eb12c01b7119e934d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:22.075 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.287815 secs\n",
      "2023-09-19 19:47:22.103 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 13}\n",
      "2023-09-19 19:47:22.104 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:22.105 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72af79d4168b49f5aa5e9da972e28e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:25.523 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.417863 secs\n",
      "2023-09-19 19:47:25.555 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 13}\n",
      "2023-09-19 19:47:25.555 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:25.556 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd4deae1e004d7690d7b909f3345b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:30.039 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.482931 secs\n",
      "2023-09-19 19:47:30.076 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 13}\n",
      "2023-09-19 19:47:30.077 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:30.077 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddfaf99bcd94509bc7a85a4a1ff2e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:33.518 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.441282 secs\n",
      "2023-09-19 19:47:33.552 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 13}\n",
      "2023-09-19 19:47:33.553 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:33.553 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b349e9f7ac564e849feb3da608ef1ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:37.063 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.510116 secs\n",
      "2023-09-19 19:47:37.098 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 13}\n",
      "2023-09-19 19:47:37.099 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:37.099 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46167269bb354e04b0a2b720ef9cb5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:40.518 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.418535 secs\n",
      "2023-09-19 19:47:40.547 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 14}\n",
      "2023-09-19 19:47:40.548 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:40.548 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40ad45a27804bf0990052765f8287c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:43.807 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.258842 secs\n",
      "2023-09-19 19:47:43.841 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 14}\n",
      "2023-09-19 19:47:43.841 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:43.842 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72197de750274164827b9e6c08f32a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:47.867 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.025465 secs\n",
      "2023-09-19 19:47:47.904 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 14}\n",
      "2023-09-19 19:47:47.904 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:47.905 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f19096fcb342ea81486d3c0501be79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:51.198 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.293805 secs\n",
      "2023-09-19 19:47:51.232 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 14}\n",
      "2023-09-19 19:47:51.232 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:51.233 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9aa87cebfa461199d9b229713f79db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:54.521 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.288175 secs\n",
      "2023-09-19 19:47:54.557 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 14}\n",
      "2023-09-19 19:47:54.557 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:54.557 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e21ae862c341d29a3f7bc3c0e92136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:47:57.893 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.335147 secs\n",
      "2023-09-19 19:47:57.928 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 15}\n",
      "2023-09-19 19:47:57.928 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:47:57.929 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4677d47bb8fd416592417dafc4c2a74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:01.966 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.037680 secs\n",
      "2023-09-19 19:48:01.996 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 15}\n",
      "2023-09-19 19:48:01.997 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:01.997 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1479ddc5bafa48af9000c709e5d08be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:06.358 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.360870 secs\n",
      "2023-09-19 19:48:06.390 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 15}\n",
      "2023-09-19 19:48:06.390 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:06.391 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0f02a4f07244d8b22ed99e8f204893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:10.531 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.139934 secs\n",
      "2023-09-19 19:48:10.564 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 15}\n",
      "2023-09-19 19:48:10.564 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:10.565 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8f343527b6408899652c0474f8ad1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:13.693 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.128877 secs\n",
      "2023-09-19 19:48:13.726 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 15}\n",
      "2023-09-19 19:48:13.727 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:13.727 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e09525ef6740ee85df7fd9372f0e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:17.051 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.324298 secs\n",
      "2023-09-19 19:48:17.083 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 16}\n",
      "2023-09-19 19:48:17.084 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:17.084 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4a54bf6e0d4953b038255a785f0c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:20.411 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.326764 secs\n",
      "2023-09-19 19:48:20.442 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 16}\n",
      "2023-09-19 19:48:20.443 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:20.443 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa4d730789e4fc488e8ca8db844cdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:25.836 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.392341 secs\n",
      "2023-09-19 19:48:25.872 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 16}\n",
      "2023-09-19 19:48:25.873 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:25.874 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23966e03a64129bb445995915247f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:29.584 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.710483 secs\n",
      "2023-09-19 19:48:29.619 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 16}\n",
      "2023-09-19 19:48:29.619 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:29.620 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16280a9580f4941a85941b2ed5fe669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:32.464 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.843742 secs\n",
      "2023-09-19 19:48:32.497 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 16}\n",
      "2023-09-19 19:48:32.498 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:32.499 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2394a690af084a74a31e22619b043a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:35.851 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.352287 secs\n",
      "2023-09-19 19:48:35.877 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 17}\n",
      "2023-09-19 19:48:35.878 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:35.878 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014b7c129c4f44bc891e1a2587a5dc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:39.170 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.291689 secs\n",
      "2023-09-19 19:48:39.207 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 17}\n",
      "2023-09-19 19:48:39.208 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:39.208 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d464010f6d4010af2c17749beb24ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:44.523 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.315076 secs\n",
      "2023-09-19 19:48:44.558 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 17}\n",
      "2023-09-19 19:48:44.558 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:44.559 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1547aa57248c2ac67611ab569a04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:48.210 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.651372 secs\n",
      "2023-09-19 19:48:48.244 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 17}\n",
      "2023-09-19 19:48:48.244 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:48.245 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997826b75e3e4b72a9e5f12991f98db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:51.084 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.839223 secs\n",
      "2023-09-19 19:48:51.118 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 17}\n",
      "2023-09-19 19:48:51.119 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:51.119 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcacefa0fc0b4a748e356778a5e5f786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:54.362 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.242786 secs\n",
      "2023-09-19 19:48:54.392 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 18}\n",
      "2023-09-19 19:48:54.393 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:54.393 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1763200a03c14b66b012f7b8212dfd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:48:57.517 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.123656 secs\n",
      "2023-09-19 19:48:57.547 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 18}\n",
      "2023-09-19 19:48:57.547 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:48:57.547 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a3f5545994888865c5ef6489a05a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:02.847 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.299705 secs\n",
      "2023-09-19 19:49:02.884 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 18}\n",
      "2023-09-19 19:49:02.885 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:02.885 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7897bfb0a4d64a52b98a38d517115d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:06.606 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.721191 secs\n",
      "2023-09-19 19:49:06.638 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 18}\n",
      "2023-09-19 19:49:06.638 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:06.639 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb2bde88b1240f9ab47b7c695ef6494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:09.647 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.007583 secs\n",
      "2023-09-19 19:49:09.679 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 18}\n",
      "2023-09-19 19:49:09.679 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:09.680 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397bdb7800894f2d91cdc51fc2661442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:12.994 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.313594 secs\n",
      "2023-09-19 19:49:13.023 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 19}\n",
      "2023-09-19 19:49:13.024 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:13.024 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eea9d8a0e5b4ffb814da5dea73283cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:16.720 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.696431 secs\n",
      "2023-09-19 19:49:16.749 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 19}\n",
      "2023-09-19 19:49:16.749 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:16.750 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ab75044276442ba16da0eee2ddeeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:20.199 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.449676 secs\n",
      "2023-09-19 19:49:20.242 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 19}\n",
      "2023-09-19 19:49:20.243 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:20.243 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5ba5d8166b434b8be5478a0fec7263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:23.578 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.334430 secs\n",
      "2023-09-19 19:49:23.611 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 19}\n",
      "2023-09-19 19:49:23.611 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:23.612 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b17cb08add411e9d135cbd075733f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:26.355 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.743502 secs\n",
      "2023-09-19 19:49:26.389 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.7, 'top_k': 19}\n",
      "2023-09-19 19:49:26.391 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:26.391 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec078a7361647f2aa262341b8c012d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:29.623 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.231950 secs\n",
      "2023-09-19 19:49:29.660 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 4}\n",
      "2023-09-19 19:49:29.661 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:29.661 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a6480faeff434d9353884c8b8ee489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:32.432 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.771250 secs\n",
      "2023-09-19 19:49:32.464 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 4}\n",
      "2023-09-19 19:49:32.465 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:32.465 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7adc4d9ce7452aa0a53ea593e45d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:35.498 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.032883 secs\n",
      "2023-09-19 19:49:35.533 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 4}\n",
      "2023-09-19 19:49:35.534 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:35.534 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2abb3e319948f1b88fc1d712e5b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:37.885 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.350495 secs\n",
      "2023-09-19 19:49:37.926 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 4}\n",
      "2023-09-19 19:49:37.927 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:37.927 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8236164bf70949a28bafa3f157e13a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:40.756 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.829373 secs\n",
      "2023-09-19 19:49:40.786 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 4}\n",
      "2023-09-19 19:49:40.791 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:40.806 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883368d5024146a89112bda8e1d5bc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:44.373 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.567310 secs\n",
      "2023-09-19 19:49:44.411 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 5}\n",
      "2023-09-19 19:49:44.411 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:44.412 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405b1ecfcf74427183e0b052cc34dbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:47.651 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.239006 secs\n",
      "2023-09-19 19:49:47.680 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 5}\n",
      "2023-09-19 19:49:47.680 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:47.681 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681ae7abcfa140e9bc1bb97f00e25c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:51.691 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.009890 secs\n",
      "2023-09-19 19:49:51.726 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 5}\n",
      "2023-09-19 19:49:51.727 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:51.727 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b21031daa534092b3faf8f7c9f77029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:54.618 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.891214 secs\n",
      "2023-09-19 19:49:54.657 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 5}\n",
      "2023-09-19 19:49:54.658 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:54.658 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59f34be3d36471b85edb133ef7cbaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:49:57.490 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.832110 secs\n",
      "2023-09-19 19:49:57.523 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 5}\n",
      "2023-09-19 19:49:57.524 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:49:57.525 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8c1d591afb429db3fb9b03de718fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:00.627 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.102401 secs\n",
      "2023-09-19 19:50:00.656 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 6}\n",
      "2023-09-19 19:50:00.657 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:00.657 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdad4f272494394a5ce92395afa0e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:03.787 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.130230 secs\n",
      "2023-09-19 19:50:03.820 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 6}\n",
      "2023-09-19 19:50:03.821 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:03.822 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7ec46a061f496d9878fec49fd500dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:07.358 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.536609 secs\n",
      "2023-09-19 19:50:07.394 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 6}\n",
      "2023-09-19 19:50:07.394 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:07.395 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc7823d360f4a2697a9be29b3c048a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:09.998 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.603597 secs\n",
      "2023-09-19 19:50:10.032 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 6}\n",
      "2023-09-19 19:50:10.033 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:10.033 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f0987e616840f8bf4ac2120fb201c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:12.844 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.810511 secs\n",
      "2023-09-19 19:50:12.876 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 6}\n",
      "2023-09-19 19:50:12.877 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:12.877 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d16c78faed24439a086f6374d5cf044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:16.029 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.152382 secs\n",
      "2023-09-19 19:50:16.059 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 7}\n",
      "2023-09-19 19:50:16.059 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:16.060 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2022fe703a4d33898655f341891461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:19.644 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.584099 secs\n",
      "2023-09-19 19:50:19.674 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 7}\n",
      "2023-09-19 19:50:19.675 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:19.676 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df0dac31860464c8f2a949bf4fe6173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:24.442 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.766922 secs\n",
      "2023-09-19 19:50:24.478 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 7}\n",
      "2023-09-19 19:50:24.479 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:24.480 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a64d274f081459c81054fbd171645b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:27.535 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.055484 secs\n",
      "2023-09-19 19:50:27.568 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 7}\n",
      "2023-09-19 19:50:27.569 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:27.569 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e50a797c3046debcae644d5964767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:30.442 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.872648 secs\n",
      "2023-09-19 19:50:30.474 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 7}\n",
      "2023-09-19 19:50:30.475 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:30.479 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e628994d574f8ab79e1e8f161238ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:33.783 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.304027 secs\n",
      "2023-09-19 19:50:33.808 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 8}\n",
      "2023-09-19 19:50:33.808 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:33.809 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ba267fd7fa4a5b94112408c04ccfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:37.868 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.059269 secs\n",
      "2023-09-19 19:50:37.897 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 8}\n",
      "2023-09-19 19:50:37.898 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:37.898 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecc92d566de40b285de02a6147a0d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:41.682 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.784175 secs\n",
      "2023-09-19 19:50:41.717 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 8}\n",
      "2023-09-19 19:50:41.718 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:41.718 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da80303be9334d71a15237a9310b4660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:45.169 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.450877 secs\n",
      "2023-09-19 19:50:45.199 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 8}\n",
      "2023-09-19 19:50:45.204 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:45.214 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc27ea83d544ff7bddff8efd1b82d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:48.741 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.526311 secs\n",
      "2023-09-19 19:50:48.774 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 8}\n",
      "2023-09-19 19:50:48.774 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:48.775 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bc19af085c4e589cb553f69c8ad5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:52.240 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.465344 secs\n",
      "2023-09-19 19:50:52.275 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 9}\n",
      "2023-09-19 19:50:52.275 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:52.276 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb3e39420e54de8ba94dfb65861b0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:55.947 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.671497 secs\n",
      "2023-09-19 19:50:55.980 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 9}\n",
      "2023-09-19 19:50:55.982 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:55.988 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e91651090584596b161b47cb0160f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:50:59.625 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.641653 secs\n",
      "2023-09-19 19:50:59.660 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 9}\n",
      "2023-09-19 19:50:59.662 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:50:59.663 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b837ba12279e4756bb45586d52bb1c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:03.218 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.554631 secs\n",
      "2023-09-19 19:51:03.251 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 9}\n",
      "2023-09-19 19:51:03.251 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:03.252 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938befc78ee54ad98abd86454d6e6505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:06.934 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.682946 secs\n",
      "2023-09-19 19:51:06.968 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 9}\n",
      "2023-09-19 19:51:06.968 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:06.969 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dbf42e50984e55bc385598eb8b062b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:09.416 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.447286 secs\n",
      "2023-09-19 19:51:09.442 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 10}\n",
      "2023-09-19 19:51:09.443 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:09.443 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593e329d69074aee8f05a8b1d7984750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:13.936 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.493051 secs\n",
      "2023-09-19 19:51:13.970 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 10}\n",
      "2023-09-19 19:51:13.970 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:13.971 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e1267fe1945729aad145d0ac1066b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:18.113 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.141958 secs\n",
      "2023-09-19 19:51:18.149 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 10}\n",
      "2023-09-19 19:51:18.149 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:18.150 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfff99be5784899a4eae51c8eb3d88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:21.006 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.856383 secs\n",
      "2023-09-19 19:51:21.039 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 10}\n",
      "2023-09-19 19:51:21.040 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:21.040 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c7cf0796ad4f4cb2139b22f0d05717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:23.940 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.900206 secs\n",
      "2023-09-19 19:51:23.975 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 10}\n",
      "2023-09-19 19:51:23.975 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:23.976 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98d272cbfbd4d2990728abafcfe6836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:26.444 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.467916 secs\n",
      "2023-09-19 19:51:26.477 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 11}\n",
      "2023-09-19 19:51:26.478 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:26.478 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23400b0f6224fd9a5f53e99b9f85da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:30.911 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.432440 secs\n",
      "2023-09-19 19:51:30.941 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 11}\n",
      "2023-09-19 19:51:30.941 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:30.942 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553bc425a6184f23b671e43719cbe7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:34.719 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.777707 secs\n",
      "2023-09-19 19:51:34.771 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 11}\n",
      "2023-09-19 19:51:34.772 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:34.772 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9151a7f670b4b7b9fadec0d603a7caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:37.922 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.149654 secs\n",
      "2023-09-19 19:51:37.954 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 11}\n",
      "2023-09-19 19:51:37.955 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:37.955 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ef837f1ad9440a997eafafbaa5f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:41.402 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.446686 secs\n",
      "2023-09-19 19:51:41.436 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 11}\n",
      "2023-09-19 19:51:41.437 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:41.437 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb63da3ce564922a8890b096f86e666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:44.538 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.100578 secs\n",
      "2023-09-19 19:51:44.568 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 12}\n",
      "2023-09-19 19:51:44.569 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:44.569 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae18f08df6f547e1ad3b7d869607c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:47.856 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.287384 secs\n",
      "2023-09-19 19:51:47.890 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 12}\n",
      "2023-09-19 19:51:47.891 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:47.892 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee94003b5081484ea1b93ba93c1226d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:52.838 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.945996 secs\n",
      "2023-09-19 19:51:52.874 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 12}\n",
      "2023-09-19 19:51:52.876 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:52.876 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52636581106841768a9174fb28553927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:56.163 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.286927 secs\n",
      "2023-09-19 19:51:56.197 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 12}\n",
      "2023-09-19 19:51:56.198 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:56.198 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa5cd1924f64f43a93bd3f9b7e21b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:51:59.602 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.404296 secs\n",
      "2023-09-19 19:51:59.636 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 12}\n",
      "2023-09-19 19:51:59.637 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:51:59.637 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66d49f318d64ef0afd70e47010e6d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:03.000 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.362548 secs\n",
      "2023-09-19 19:52:03.031 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 13}\n",
      "2023-09-19 19:52:03.032 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:03.032 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f75a243b4e4a708f177e500a4336e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:06.501 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.469229 secs\n",
      "2023-09-19 19:52:06.531 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 13}\n",
      "2023-09-19 19:52:06.532 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:06.532 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46e80c90a144090bb924934d1d97f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:11.450 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.917622 secs\n",
      "2023-09-19 19:52:11.488 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 13}\n",
      "2023-09-19 19:52:11.489 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:11.489 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e8c2952cb4e6f85a6ad865060995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:15.202 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.712478 secs\n",
      "2023-09-19 19:52:15.236 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 13}\n",
      "2023-09-19 19:52:15.236 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:15.237 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784db4eb5d1a4fbbb41e95698760605b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:19.011 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.774159 secs\n",
      "2023-09-19 19:52:19.046 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 13}\n",
      "2023-09-19 19:52:19.046 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:19.046 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802bdfe711e54d08819419a8a1db7a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:22.507 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.460460 secs\n",
      "2023-09-19 19:52:22.537 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 14}\n",
      "2023-09-19 19:52:22.538 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:22.538 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093c5513b84a4fb19b3c028f03a34f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:25.859 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.320315 secs\n",
      "2023-09-19 19:52:25.891 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 14}\n",
      "2023-09-19 19:52:25.891 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:25.892 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea8384f4af9457881d67ad3277cc008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:30.253 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.361320 secs\n",
      "2023-09-19 19:52:30.295 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 14}\n",
      "2023-09-19 19:52:30.296 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:30.297 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64903295ee04ddd89733a2c0a66cdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:33.900 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.602488 secs\n",
      "2023-09-19 19:52:33.935 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 14}\n",
      "2023-09-19 19:52:33.936 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:33.936 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec43d336a8e5476d83e789bd6015c003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:37.509 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.572954 secs\n",
      "2023-09-19 19:52:37.543 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 14}\n",
      "2023-09-19 19:52:37.543 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:37.544 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a47570f03ce4fca93b53d369d36bc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:40.958 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.413826 secs\n",
      "2023-09-19 19:52:40.990 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 15}\n",
      "2023-09-19 19:52:40.990 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:40.991 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb9b999f4804f51b719131cf8b426f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:45.009 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.018154 secs\n",
      "2023-09-19 19:52:45.041 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 15}\n",
      "2023-09-19 19:52:45.041 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:45.042 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438d64cf00294a6fb4d7d12f02f04fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:49.936 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.894238 secs\n",
      "2023-09-19 19:52:49.971 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 15}\n",
      "2023-09-19 19:52:49.972 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:49.972 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3edc4036aa641f2ab4c3e71f0f3e141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:54.742 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.769804 secs\n",
      "2023-09-19 19:52:54.773 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 15}\n",
      "2023-09-19 19:52:54.774 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:54.774 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845b37a51d294b24b7323f43b7acd0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:52:58.506 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.732388 secs\n",
      "2023-09-19 19:52:58.541 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 15}\n",
      "2023-09-19 19:52:58.543 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:52:58.547 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de65abeaa4f4ff09b7261d50bfc405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:02.341 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.794564 secs\n",
      "2023-09-19 19:53:02.374 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 16}\n",
      "2023-09-19 19:53:02.375 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:02.375 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b46e0f03b642bb9e8cb02c2dac724a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:06.209 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.833162 secs\n",
      "2023-09-19 19:53:06.253 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 16}\n",
      "2023-09-19 19:53:06.255 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:06.255 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5f5f2efc274a319ba3c71a9121f3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:12.484 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.228506 secs\n",
      "2023-09-19 19:53:12.529 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 16}\n",
      "2023-09-19 19:53:12.530 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:12.531 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0153f54e156240fcb7cab54c55e77ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:16.854 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.322492 secs\n",
      "2023-09-19 19:53:16.889 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 16}\n",
      "2023-09-19 19:53:16.890 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:16.890 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2166b84568e4413f8ca43a989446abad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:19.777 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.886988 secs\n",
      "2023-09-19 19:53:19.819 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 16}\n",
      "2023-09-19 19:53:19.819 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:19.820 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4ac7f707a44fe9ade22eb98e896ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:23.224 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.404055 secs\n",
      "2023-09-19 19:53:23.255 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 17}\n",
      "2023-09-19 19:53:23.256 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:23.257 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6f02359c274441a66ff166766c0e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:26.577 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.319922 secs\n",
      "2023-09-19 19:53:26.609 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 17}\n",
      "2023-09-19 19:53:26.610 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:26.610 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804654e18cb04325acb57bcdda6c3366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:32.034 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.423523 secs\n",
      "2023-09-19 19:53:32.072 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 17}\n",
      "2023-09-19 19:53:32.073 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:32.073 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab541d542ecd45168113e727b7b94a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:35.745 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.671671 secs\n",
      "2023-09-19 19:53:35.777 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 17}\n",
      "2023-09-19 19:53:35.778 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:35.778 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b032eb559f4e25b4efdbbff2670d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:38.866 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.088254 secs\n",
      "2023-09-19 19:53:38.901 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 17}\n",
      "2023-09-19 19:53:38.901 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:38.902 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee654383fb24e7d9f424fa58eb695ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:42.513 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.610906 secs\n",
      "2023-09-19 19:53:42.543 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 18}\n",
      "2023-09-19 19:53:42.544 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:42.544 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1003827af62441f3a66053383f9d1b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:45.942 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.398217 secs\n",
      "2023-09-19 19:53:45.983 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 18}\n",
      "2023-09-19 19:53:45.983 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:45.984 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d9ad1ef0e4872be12554d5c8db747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:51.419 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.434863 secs\n",
      "2023-09-19 19:53:51.465 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 18}\n",
      "2023-09-19 19:53:51.466 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:51.466 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316b9bb8c2a6488eae8e0874f4e5f400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:55.019 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.553191 secs\n",
      "2023-09-19 19:53:55.051 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 18}\n",
      "2023-09-19 19:53:55.051 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:55.051 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c856fbf2e6454f708c333fc416ead7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:53:57.933 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.881889 secs\n",
      "2023-09-19 19:53:57.966 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 18}\n",
      "2023-09-19 19:53:57.967 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:53:57.967 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b3591fcb25457c99ef5312a83d5bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:01.359 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.391540 secs\n",
      "2023-09-19 19:54:01.389 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 19}\n",
      "2023-09-19 19:54:01.389 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:01.390 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373e574eba204814beaf661d7cb1e5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:05.253 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.862845 secs\n",
      "2023-09-19 19:54:05.285 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 19}\n",
      "2023-09-19 19:54:05.285 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:05.286 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6be11b2d664d56ad6b178b03230a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:08.748 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.462525 secs\n",
      "2023-09-19 19:54:08.784 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 19}\n",
      "2023-09-19 19:54:08.784 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:08.785 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da36ddbe92b487c85aafb271af86ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:12.125 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.340310 secs\n",
      "2023-09-19 19:54:12.159 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 19}\n",
      "2023-09-19 19:54:12.160 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:12.160 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686610480a1e4ae6a3ada6aa1d201f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:15.187 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.027362 secs\n",
      "2023-09-19 19:54:15.221 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.8, 'top_k': 19}\n",
      "2023-09-19 19:54:15.222 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:15.222 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69afa50480a4ed2a04c19fe56495504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:18.564 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.342365 secs\n",
      "2023-09-19 19:54:18.595 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 4}\n",
      "2023-09-19 19:54:18.595 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:18.595 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aee22a30b1a4ec59f76eaafe18a433a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:21.492 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.896769 secs\n",
      "2023-09-19 19:54:21.522 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 4}\n",
      "2023-09-19 19:54:21.522 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:21.523 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75899a8b52e14c7fb8235aff753cf84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:24.529 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.006665 secs\n",
      "2023-09-19 19:54:24.564 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 4}\n",
      "2023-09-19 19:54:24.564 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:24.565 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d5d32f47d243f38e71b5521a392fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:26.877 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.312314 secs\n",
      "2023-09-19 19:54:26.910 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 4}\n",
      "2023-09-19 19:54:26.910 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:26.911 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad08cb6bd6f8454ba65930d5dded8019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:29.767 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.856909 secs\n",
      "2023-09-19 19:54:29.800 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 4}\n",
      "2023-09-19 19:54:29.800 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:29.800 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccdf43767ad48fa99d16008f0377f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:33.358 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.557983 secs\n",
      "2023-09-19 19:54:33.389 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 5}\n",
      "2023-09-19 19:54:33.390 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:33.390 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727732dd5a5b480abda709bbf0ec6a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:37.439 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.049090 secs\n",
      "2023-09-19 19:54:37.470 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 5}\n",
      "2023-09-19 19:54:37.471 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:37.471 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749532df97274ac7b59dba97a22cec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:42.629 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.157627 secs\n",
      "2023-09-19 19:54:42.674 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 5}\n",
      "2023-09-19 19:54:42.675 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:42.675 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2eb8d28d7a4fee920a8843ed08d464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:46.449 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.774104 secs\n",
      "2023-09-19 19:54:46.486 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 5}\n",
      "2023-09-19 19:54:46.488 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:46.489 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9493736f54402f9991646dec08a1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:49.760 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.270888 secs\n",
      "2023-09-19 19:54:49.796 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 5}\n",
      "2023-09-19 19:54:49.797 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:49.797 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca2bc2c4a3b4c68989773e87fc7d6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:53.455 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.657748 secs\n",
      "2023-09-19 19:54:53.483 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 6}\n",
      "2023-09-19 19:54:53.484 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:53.484 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee763666ef6e4f419d64182f167a56bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:54:56.571 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.086232 secs\n",
      "2023-09-19 19:54:56.603 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 6}\n",
      "2023-09-19 19:54:56.603 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:54:56.604 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab50a7ef3974b0f816b7bde3a1ce51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:00.570 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.966806 secs\n",
      "2023-09-19 19:55:00.610 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 6}\n",
      "2023-09-19 19:55:00.611 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:00.611 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a9ae852eb149f6af9011a76b1f450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:03.990 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.379423 secs\n",
      "2023-09-19 19:55:04.030 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 6}\n",
      "2023-09-19 19:55:04.030 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:04.031 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f106b049f242e08f70827ee2414184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:07.365 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.334109 secs\n",
      "2023-09-19 19:55:07.403 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 6}\n",
      "2023-09-19 19:55:07.404 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:07.405 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da25550a9d34990940b00650de11631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:11.451 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.046508 secs\n",
      "2023-09-19 19:55:11.478 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 7}\n",
      "2023-09-19 19:55:11.479 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:11.479 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673ae1e77c814efeb124f4e085848e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:15.290 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.811086 secs\n",
      "2023-09-19 19:55:15.320 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 7}\n",
      "2023-09-19 19:55:15.320 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:15.320 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3aa637bd904111b828c57797f1e02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:20.613 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.293106 secs\n",
      "2023-09-19 19:55:20.653 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 7}\n",
      "2023-09-19 19:55:20.653 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:20.654 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a52371bbf39494289b096a42daca58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:24.027 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.373463 secs\n",
      "2023-09-19 19:55:24.062 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 7}\n",
      "2023-09-19 19:55:24.062 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:24.063 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97897cea98e04bb6b4e835ba8d1060f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:26.977 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.913370 secs\n",
      "2023-09-19 19:55:27.008 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 7}\n",
      "2023-09-19 19:55:27.009 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:27.009 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d95116490b479785de3ddc34f55eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:30.293 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.283438 secs\n",
      "2023-09-19 19:55:30.323 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 8}\n",
      "2023-09-19 19:55:30.323 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:30.324 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d59b2f300d4338ac1e695d9c4cc24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:34.063 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.739102 secs\n",
      "2023-09-19 19:55:34.095 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 8}\n",
      "2023-09-19 19:55:34.096 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:34.096 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58391864d336409aa5f8f8e67676d96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:37.642 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.545267 secs\n",
      "2023-09-19 19:55:37.675 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 8}\n",
      "2023-09-19 19:55:37.675 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:37.676 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c296199dd42708012f07e827f9689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:41.193 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.517091 secs\n",
      "2023-09-19 19:55:41.226 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 8}\n",
      "2023-09-19 19:55:41.226 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:41.227 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd80e94e3214394a3573214dc8078c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:44.773 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.546228 secs\n",
      "2023-09-19 19:55:44.807 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 8}\n",
      "2023-09-19 19:55:44.808 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:44.808 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ac1af7bc3446b4b9e7d47ed1e91d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:48.141 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.332943 secs\n",
      "2023-09-19 19:55:48.172 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 9}\n",
      "2023-09-19 19:55:48.172 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:48.173 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a85bf9cff045779e5b0ba9810e2025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:52.087 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.914926 secs\n",
      "2023-09-19 19:55:52.120 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 9}\n",
      "2023-09-19 19:55:52.121 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:52.121 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f3af9a638545818080a81388338ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:55.782 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.660589 secs\n",
      "2023-09-19 19:55:55.816 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 9}\n",
      "2023-09-19 19:55:55.816 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:55.816 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f52dbc7bc324be8bece025c2320edd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:55:59.140 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.323260 secs\n",
      "2023-09-19 19:55:59.172 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 9}\n",
      "2023-09-19 19:55:59.172 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:55:59.173 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c3e1caec4f40a59394945ed03d2f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:02.673 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.500091 secs\n",
      "2023-09-19 19:56:02.703 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 9}\n",
      "2023-09-19 19:56:02.703 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:02.704 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ef54ed46de4ba3b5d7891f1ba443ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:05.149 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.445046 secs\n",
      "2023-09-19 19:56:05.178 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 10}\n",
      "2023-09-19 19:56:05.178 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:05.179 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4d389dbd33465181b7ba05e6d76874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:09.523 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.344880 secs\n",
      "2023-09-19 19:56:09.556 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 10}\n",
      "2023-09-19 19:56:09.557 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:09.557 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221d566f58974f3ca9d0c68e5d8959c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:13.718 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.160555 secs\n",
      "2023-09-19 19:56:13.753 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 10}\n",
      "2023-09-19 19:56:13.754 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:13.754 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acc870e130f4055b5679280bcd18d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:16.619 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.864992 secs\n",
      "2023-09-19 19:56:16.652 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 10}\n",
      "2023-09-19 19:56:16.653 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:16.653 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4226014851a44f738176c65783153050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:19.719 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.065386 secs\n",
      "2023-09-19 19:56:19.757 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 10}\n",
      "2023-09-19 19:56:19.758 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:19.759 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b387cb2df27c44f8a1be21d4cd35c1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:22.268 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.509187 secs\n",
      "2023-09-19 19:56:22.298 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 11}\n",
      "2023-09-19 19:56:22.298 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:22.299 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37672cd6ebb49a68d9390f7fe69f04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:26.829 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.530647 secs\n",
      "2023-09-19 19:56:26.862 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 11}\n",
      "2023-09-19 19:56:26.863 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:26.864 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e05c80b9be48b189ce4b03211c76b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:30.971 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.107414 secs\n",
      "2023-09-19 19:56:31.009 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 11}\n",
      "2023-09-19 19:56:31.010 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:31.010 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72416e94e30b4e5098ba9373492bcef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:34.310 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.299812 secs\n",
      "2023-09-19 19:56:34.344 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 11}\n",
      "2023-09-19 19:56:34.344 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:34.345 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f4fba5f03848b4878dac725210cab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:37.892 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.546972 secs\n",
      "2023-09-19 19:56:37.924 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 11}\n",
      "2023-09-19 19:56:37.926 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:37.931 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df34a4f57a44a79b6610e83746ff0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:41.162 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.231600 secs\n",
      "2023-09-19 19:56:41.192 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 12}\n",
      "2023-09-19 19:56:41.193 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:41.193 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380546d0826d45ac886b007b1e0c9492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:44.637 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.443998 secs\n",
      "2023-09-19 19:56:44.674 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 12}\n",
      "2023-09-19 19:56:44.675 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:44.675 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a1450d47724965a7417047b186f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:49.611 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.935565 secs\n",
      "2023-09-19 19:56:49.649 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 12}\n",
      "2023-09-19 19:56:49.649 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:49.650 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04946e930d434e158ffe3634d88d1662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:53.209 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.559151 secs\n",
      "2023-09-19 19:56:53.242 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 12}\n",
      "2023-09-19 19:56:53.242 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:53.243 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e6fafa4205407c931b08968dcad815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:56:56.863 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.620026 secs\n",
      "2023-09-19 19:56:56.896 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 12}\n",
      "2023-09-19 19:56:56.896 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:56:56.897 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cab8a3ff5144392b1868a6ccf985f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:00.089 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.192062 secs\n",
      "2023-09-19 19:57:00.116 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 13}\n",
      "2023-09-19 19:57:00.116 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:00.117 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d71096f10a9435ca1326629103114e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:03.758 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.641136 secs\n",
      "2023-09-19 19:57:03.795 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 13}\n",
      "2023-09-19 19:57:03.795 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:03.796 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e1ec1eca00471cbf2e2c4ae478bbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:08.926 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.130063 secs\n",
      "2023-09-19 19:57:08.963 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 13}\n",
      "2023-09-19 19:57:08.963 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:08.964 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6543a468cf684cc8ab61a5c61a266127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:12.807 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.843162 secs\n",
      "2023-09-19 19:57:12.837 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 13}\n",
      "2023-09-19 19:57:12.838 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:12.841 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c5b142db964eaa92c29f30900ad105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:16.615 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.774999 secs\n",
      "2023-09-19 19:57:16.663 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 13}\n",
      "2023-09-19 19:57:16.664 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:16.664 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a41a58278c4615a8041c2254c2f5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:20.236 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.571472 secs\n",
      "2023-09-19 19:57:20.266 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 14}\n",
      "2023-09-19 19:57:20.269 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:20.272 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df257b162d0f442194b8514f913328c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:23.848 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.575276 secs\n",
      "2023-09-19 19:57:23.881 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 14}\n",
      "2023-09-19 19:57:23.881 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:23.882 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5662934918894a859cf890affe789e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:28.306 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.424226 secs\n",
      "2023-09-19 19:57:28.342 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 14}\n",
      "2023-09-19 19:57:28.343 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:28.343 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b06f5a784cf468082011bd828af36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:31.896 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.553325 secs\n",
      "2023-09-19 19:57:31.930 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 14}\n",
      "2023-09-19 19:57:31.930 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:31.931 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c13ec37ac46c8bb853a0aaa1e2302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:35.257 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.326320 secs\n",
      "2023-09-19 19:57:35.290 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 14}\n",
      "2023-09-19 19:57:35.291 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:35.291 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153c50e5657648e0836d275e88ef4ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:38.680 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.388951 secs\n",
      "2023-09-19 19:57:38.714 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 15}\n",
      "2023-09-19 19:57:38.715 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:38.715 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d363d7b419864a38aca207d6af47271b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:42.756 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.040398 secs\n",
      "2023-09-19 19:57:42.787 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 15}\n",
      "2023-09-19 19:57:42.788 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:42.788 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f6e624a1f4d498a5f183d4b4347ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:47.131 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.342874 secs\n",
      "2023-09-19 19:57:47.167 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 15}\n",
      "2023-09-19 19:57:47.168 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:47.169 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c96aad6165e4df3a15d8fa86a3f8cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:51.079 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.910835 secs\n",
      "2023-09-19 19:57:51.116 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 15}\n",
      "2023-09-19 19:57:51.117 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:51.118 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ddc1649284406399c97eaa190fa368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:55.273 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.155868 secs\n",
      "2023-09-19 19:57:55.318 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 15}\n",
      "2023-09-19 19:57:55.319 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:57:55.320 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca4b42be3794a84963eaed206bcc6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:57:59.978 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.658352 secs\n",
      "2023-09-19 19:58:00.042 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 16}\n",
      "2023-09-19 19:58:00.043 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:00.044 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b470c41c454e4cad86bc0a5624693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:04.038 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.993917 secs\n",
      "2023-09-19 19:58:04.069 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 16}\n",
      "2023-09-19 19:58:04.070 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:04.071 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc941c9d411c4425be678746189fa8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:10.318 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.247306 secs\n",
      "2023-09-19 19:58:10.359 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 16}\n",
      "2023-09-19 19:58:10.361 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:10.363 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953f0ef4a4714b35bbcbfcc603a7bd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:14.763 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.399974 secs\n",
      "2023-09-19 19:58:14.799 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 16}\n",
      "2023-09-19 19:58:14.799 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:14.800 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b313c4f7b9584733bc8332b49f79e780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:18.131 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.330524 secs\n",
      "2023-09-19 19:58:18.163 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 16}\n",
      "2023-09-19 19:58:18.163 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:18.163 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6688e9f591439db576a6b1995d1ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:21.736 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.572943 secs\n",
      "2023-09-19 19:58:21.768 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 17}\n",
      "2023-09-19 19:58:21.768 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:21.769 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d1b10824c944eabd8e7d2d9e588929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:25.618 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.849324 secs\n",
      "2023-09-19 19:58:25.651 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 17}\n",
      "2023-09-19 19:58:25.651 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:25.652 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e919cfe22c4478978955518f2b726e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:31.380 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.728002 secs\n",
      "2023-09-19 19:58:31.421 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 17}\n",
      "2023-09-19 19:58:31.422 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:31.422 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44a33b2aca648afaee7c4092fdbd452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:35.472 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.049342 secs\n",
      "2023-09-19 19:58:35.505 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 17}\n",
      "2023-09-19 19:58:35.506 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:35.506 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a60083316740dba61b040be3ad3c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:38.732 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.226163 secs\n",
      "2023-09-19 19:58:38.767 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 17}\n",
      "2023-09-19 19:58:38.768 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:38.768 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace0107574b24f98bf620a6b7a4649b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:42.826 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.057663 secs\n",
      "2023-09-19 19:58:42.864 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 18}\n",
      "2023-09-19 19:58:42.865 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:42.867 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d443b985ce7470eb1dd747e3ff42be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:46.201 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.334189 secs\n",
      "2023-09-19 19:58:46.233 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 18}\n",
      "2023-09-19 19:58:46.234 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:46.234 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fb110f3b594028b150140d5126f7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:52.088 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 5.853984 secs\n",
      "2023-09-19 19:58:52.126 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 18}\n",
      "2023-09-19 19:58:52.126 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:52.127 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42568c842ac34b4787e68ff2917727b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:55.791 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.664725 secs\n",
      "2023-09-19 19:58:55.826 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 18}\n",
      "2023-09-19 19:58:55.832 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:55.849 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3c10658e3b4e18a556a1f9e4286fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:58:58.673 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.823481 secs\n",
      "2023-09-19 19:58:58.705 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 18}\n",
      "2023-09-19 19:58:58.705 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:58:58.705 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea9cb6c33764dff913515cff74c8007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:02.648 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.942204 secs\n",
      "2023-09-19 19:59:02.704 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 19}\n",
      "2023-09-19 19:59:02.704 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:02.705 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67309ebdea0042f49ae2e14fcd87eab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:06.706 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 4.001454 secs\n",
      "2023-09-19 19:59:06.739 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 19}\n",
      "2023-09-19 19:59:06.740 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:06.740 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a040fd2b024a919a194f2c213db8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:10.148 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.408294 secs\n",
      "2023-09-19 19:59:10.187 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 19}\n",
      "2023-09-19 19:59:10.187 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:10.188 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5972e062ff2459ebbec5a1707283585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:13.313 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.124841 secs\n",
      "2023-09-19 19:59:13.346 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 19}\n",
      "2023-09-19 19:59:13.346 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:13.347 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7016ac2fde674cb38bbc1208ac6670ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:16.105 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.757975 secs\n",
      "2023-09-19 19:59:16.139 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beams': 1, 'penalty_alpha': 0.9, 'top_k': 19}\n",
      "2023-09-19 19:59:16.140 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:16.141 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2a70f4b39841f29d2d64cca9106813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:19.382 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.241144 secs\n",
      "2023-09-19 19:59:19.410 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 1}\n",
      "2023-09-19 19:59:19.410 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:19.411 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60c733297a3454f9dd07a3e4f24d74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:21.855 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.444310 secs\n",
      "2023-09-19 19:59:21.885 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 1}\n",
      "2023-09-19 19:59:21.886 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:21.887 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f734b440d54e7eae2c6671104171e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:25.591 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 3.704998 secs\n",
      "2023-09-19 19:59:25.626 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 1}\n",
      "2023-09-19 19:59:25.626 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:25.627 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401c08e5f88d4f7a8610909d73ffc725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:27.859 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.232459 secs\n",
      "2023-09-19 19:59:27.892 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 1}\n",
      "2023-09-19 19:59:27.892 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:27.893 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95df5e8511646b78effab958eb995e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:29.757 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 1.864611 secs\n",
      "2023-09-19 19:59:29.791 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 1}\n",
      "2023-09-19 19:59:29.792 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:29.792 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce1a2a5e5aa416297de865574c07ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:32.149 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 2.356278 secs\n",
      "2023-09-19 19:59:32.185 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 19:59:32.185 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:32.186 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a1cc34dd5c4525b8dcbc7efe78b68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 19:59:55.254 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 23.068136 secs\n",
      "2023-09-19 19:59:55.287 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 19:59:55.287 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 19:59:55.288 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a26f1268d9c43a9b98535d56a55c678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:00:02.095 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.806967 secs\n",
      "2023-09-19 20:00:02.125 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:00:02.126 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:00:02.126 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9ea23593aa45c19bc3492d6fc3c0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:00:08.653 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.526702 secs\n",
      "2023-09-19 20:00:08.707 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:00:08.708 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:00:08.708 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db94ad810f97418c94f3bb20e3902fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:00:15.380 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.671699 secs\n",
      "2023-09-19 20:00:15.422 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:00:15.422 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:00:15.423 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22cdcc2535044ba9dcdd6cd07031d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:00:21.714 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.291529 secs\n",
      "2023-09-19 20:00:21.750 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:00:21.751 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:00:21.751 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d433f9b26941c680da7b2d1d512eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:00:48.798 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 27.047162 secs\n",
      "2023-09-19 20:00:48.829 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:00:48.830 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:00:48.831 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556105cbdf5741b6913d0a75a9091c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:01:25.700 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 36.869031 secs\n",
      "2023-09-19 20:01:25.735 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:01:25.736 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:01:25.737 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3cb7313a9045a8a0a24fcea6ed5451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:01:39.684 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 13.947095 secs\n",
      "2023-09-19 20:01:39.720 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:01:39.720 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:01:39.721 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33dba5ae9244c22b5d83fd146508605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:01:48.210 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.489368 secs\n",
      "2023-09-19 20:01:48.242 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:01:48.243 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:01:48.243 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93265b15296347a0991744c89a7e942e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:01:54.841 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.597558 secs\n",
      "2023-09-19 20:01:54.871 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 4}\n",
      "2023-09-19 20:01:54.872 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:01:54.872 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7f54f3c54c4f338f62732aa782c9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:02:29.047 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 34.175341 secs\n",
      "2023-09-19 20:02:29.079 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 4}\n",
      "2023-09-19 20:02:29.080 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:02:29.080 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13be0163b6ba48eb8a6a99b1d7a87383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:03:08.870 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 39.789889 secs\n",
      "2023-09-19 20:03:08.938 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 4}\n",
      "2023-09-19 20:03:08.939 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:03:08.939 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd13f3c5c3934082b59ad56a58d131b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:03:25.944 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 17.004169 secs\n",
      "2023-09-19 20:03:25.987 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 4}\n",
      "2023-09-19 20:03:25.988 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:03:25.989 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646bd8f5123346d1870c20b115d0b304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:03:36.314 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 10.325440 secs\n",
      "2023-09-19 20:03:36.348 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 4}\n",
      "2023-09-19 20:03:36.349 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:03:36.349 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f66bb630b1e400b893b514b7c7f2354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:03:53.656 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 17.306569 secs\n",
      "2023-09-19 20:03:53.689 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:03:53.690 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:03:53.690 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29456eccd31b4f149b9713b8208e601d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:04.792 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 11.102650 secs\n",
      "2023-09-19 20:04:04.822 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:04:04.823 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:04.824 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ebe358ad64f60a52af8d9c0c71da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:12.931 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.107944 secs\n",
      "2023-09-19 20:04:12.970 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:04:12.971 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:12.971 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04114dc935c74fc4b1222500720635be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:22.267 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 9.295713 secs\n",
      "2023-09-19 20:04:22.303 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:04:22.303 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:22.304 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd412625e9a847a3855b24454c4cd033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:29.879 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 7.575637 secs\n",
      "2023-09-19 20:04:29.913 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:04:29.913 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:29.914 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93826720f1440198fa4247cf8af9772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:37.606 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 7.692171 secs\n",
      "2023-09-19 20:04:37.634 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:04:37.635 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:37.635 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771dbbe048f64df48972b072e550fffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:48.480 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 10.844977 secs\n",
      "2023-09-19 20:04:48.514 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:04:48.514 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:48.515 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5681d58f9924dc5a4e10b1bc00644aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:04:59.556 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 11.041215 secs\n",
      "2023-09-19 20:04:59.607 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:04:59.608 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:04:59.608 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc16bccd42674a43bb5a51094fa27e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:15.556 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 15.947150 secs\n",
      "2023-09-19 20:05:15.592 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:05:15.593 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:15.593 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9bac7873294a9c904e51ddae345e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:26.213 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 10.619648 secs\n",
      "2023-09-19 20:05:26.246 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:05:26.247 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:26.247 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search with sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a03f730e8eb49459bf1d5d82f7bf28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:34.906 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.658402 secs\n",
      "2023-09-19 20:05:34.943 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:05:34.944 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:34.944 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471c5e1ffac449fe92b1303af7576f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:41.874 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.929465 secs\n",
      "2023-09-19 20:05:41.906 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:05:41.907 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:41.907 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2b237776034275aa92729a8ee32b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:49.113 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 7.206037 secs\n",
      "2023-09-19 20:05:49.144 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:05:49.144 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:49.144 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949ccc8dd57e4343a397ace4632ed3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:05:55.663 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.519051 secs\n",
      "2023-09-19 20:05:55.700 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:05:55.701 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:05:55.701 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f562d0a0fab49a19830c6425f6b3a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:01.868 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.167338 secs\n",
      "2023-09-19 20:06:01.899 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 2}\n",
      "2023-09-19 20:06:01.900 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:01.900 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43745d5a92e74c3cb852c4ef74a49cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:08.347 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 6.447312 secs\n",
      "2023-09-19 20:06:08.381 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:06:08.382 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:08.383 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76f00b11c6d46ecafc5891742d188b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:16.624 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.241307 secs\n",
      "2023-09-19 20:06:16.654 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:06:16.655 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:16.656 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6115c5acffa3400797b9ac507c87f782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:30.613 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 13.957323 secs\n",
      "2023-09-19 20:06:30.647 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:06:30.648 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:30.648 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3376f782f5f4d53aa6c75e7250f71b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:44.430 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 13.781677 secs\n",
      "2023-09-19 20:06:44.466 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:06:44.467 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:44.467 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8b0c5fa67541148ff39278b0288a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:06:53.367 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 8.899678 secs\n",
      "2023-09-19 20:06:53.409 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': False, 'generation_seed': 42, 'max_new_tokens': 120, 'num_beam_groups': 1, 'num_beams': 3}\n",
      "2023-09-19 20:06:53.410 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:06:53.412 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Beam Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ab7f16902b4bad91457f64be7ba4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:07:00.545 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 7.133789 secs\n",
      "2023-09-19 20:07:00.581 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 3}\n",
      "2023-09-19 20:07:00.581 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:07:00.582 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191cec4bc72b4c598dbdb19bb28fe8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:12:07.408 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 306.825971 secs\n",
      "2023-09-19 20:12:07.446 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 3}\n",
      "2023-09-19 20:12:07.446 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:12:07.447 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d12d7bb62841d1b0e2a2fe99385744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:13:28.637 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 81.189946 secs\n",
      "2023-09-19 20:13:28.676 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 3}\n",
      "2023-09-19 20:13:28.677 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:13:28.677 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c26e017b9b64136823161401c0df02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:15:04.691 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 96.014133 secs\n",
      "2023-09-19 20:15:04.728 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 3}\n",
      "2023-09-19 20:15:04.728 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:15:04.728 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3326e5b12f7438189d40a508d8424ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:16:13.220 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 68.491536 secs\n",
      "2023-09-19 20:16:13.256 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 3}\n",
      "2023-09-19 20:16:13.256 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:16:13.257 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc6e3c353404d8a9c7803e6599b416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:18:02.300 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 109.043472 secs\n",
      "2023-09-19 20:18:02.350 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 4}\n",
      "2023-09-19 20:18:02.351 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:18:02.351 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d72dc279f144b01a3bb84d7aedaef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:19:56.395 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 114.044131 secs\n",
      "2023-09-19 20:19:56.433 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 4}\n",
      "2023-09-19 20:19:56.434 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:19:56.434 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1fcd9b3674daba2a58f8ba1571209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:21:59.599 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 123.164692 secs\n",
      "2023-09-19 20:21:59.646 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 4}\n",
      "2023-09-19 20:21:59.647 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:21:59.647 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42d2077882848e9892bf4f1aac56242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:25:44.432 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 224.785036 secs\n",
      "2023-09-19 20:25:44.478 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 4}\n",
      "2023-09-19 20:25:44.479 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:25:44.479 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd0135023b34889bba147817c215f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:28:29.954 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 165.475122 secs\n",
      "2023-09-19 20:28:29.995 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 4}\n",
      "2023-09-19 20:28:29.996 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:28:29.996 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f52abe50a84d07a16f73db001c1bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:30:52.599 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 142.602964 secs\n",
      "2023-09-19 20:30:52.636 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 5}\n",
      "2023-09-19 20:30:52.637 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:30:52.637 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d630e8e73ac1424c8884da348969e084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:33:57.374 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 184.736818 secs\n",
      "2023-09-19 20:33:57.422 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 5}\n",
      "2023-09-19 20:33:57.423 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:33:57.423 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e66f21a7a104d94aea60b94602e7f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:37:07.899 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 190.476428 secs\n",
      "2023-09-19 20:37:07.961 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 5}\n",
      "2023-09-19 20:37:07.962 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:37:07.962 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb19c8853df4b7aa05c54eb6230f403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:40:20.622 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 192.659562 secs\n",
      "2023-09-19 20:40:20.686 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 5}\n",
      "2023-09-19 20:40:20.687 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:40:20.688 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fda9dadb2cb4518b8410eddeadf7d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:43:30.260 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 189.571946 secs\n",
      "2023-09-19 20:43:30.322 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 5}\n",
      "2023-09-19 20:43:30.323 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:43:30.324 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2223cc1a4f8412e992941ebdc65f922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:46:31.753 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 181.429213 secs\n",
      "2023-09-19 20:46:31.805 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 6}\n",
      "2023-09-19 20:46:31.805 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:46:31.806 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d67f6c33eb644a886fdc25eea4cf631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 20:56:00.517 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 568.711193 secs\n",
      "2023-09-19 20:56:00.579 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 6}\n",
      "2023-09-19 20:56:00.580 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 20:56:00.580 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd08df0341a42748ec58be792bf3520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 21:03:11.515 - llmsearch.utils.mem_utils:179 - INFO - Finished running inference, took 430.935405 secs\n",
      "2023-09-19 21:03:11.578 - llmsearch.utils.mem_utils:145 - INFO - Starting inference with generation parameters - {'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 120, 'mirostat_eta': 0.1, 'mirostat_mode': 2, 'mirostat_tau': 6}\n",
      "2023-09-19 21:03:11.579 - llmsearch.utils.mem_utils:149 - INFO - Performing inference with batch_size - 512\n",
      "2023-09-19 21:03:11.580 - llmsearch.utils.model_utils:102 - INFO - Detected generation type - Sampling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68e78281332423a9e0aa86625c9e605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here - 2\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"X\"], y=tuner_ob.dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2, outputs2 = tuner_ob.get_score(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d27c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49238087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d93c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt, out1, out2 in zip(samples_to_tune_on['y'], outputs1, outputs2):\n",
    "    if out1 != out2:\n",
    "        print(f\"Ground Truth - {gt}\")\n",
    "        print(f\"out1 - {out1}\")\n",
    "        print(f\"out2 - {out2}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(duration = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.best_estimator_.set_params(**clf.best_params_).get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c02681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "clone(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a846cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27049",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
