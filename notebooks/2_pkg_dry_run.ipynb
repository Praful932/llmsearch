{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25b6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device - mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "\n",
    "print(f\"Device - {device}\")\n",
    "\n",
    "def beep(duration = 1, frequency=440, rhythm=1):\n",
    "    sample_rate = 44100  # Standard audio sample rate\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    audio_data = np.sin(2*np.pi*frequency*t)  # Generate a sine wave\n",
    "    audio_data *= np.where(np.arange(len(audio_data)) % rhythm == 0, 1, 0)  # Apply rhythm\n",
    "    display(Audio(audio_data, rate=sample_rate, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2fe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/praful932/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abf12efe6c4471bb7d1fe98f141d0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca4a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"train\"][:sample_size])\n",
    "samples_to_tune_on = samples_to_tune_on.rename_columns(column_mapping = {'dialogue' : 'X', 'summary' : \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40ddb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = False)\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ba55bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conversation: Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\\nSummary:\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "X = samples_to_tune_on[0]['X']\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(\"Conversation: {X}\\nSummary:\")\n",
    "\n",
    "pt.format_prompt(X = X).to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cddba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    \n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True)\n",
    "    return result['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d990828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87baa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new estimator with generation keys - {}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import get_total_available_ram, get_gpu_information\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "seed = 42\n",
    "\n",
    "total_available_ram_memory = get_total_available_ram()\n",
    "\n",
    "print(f\"Total available ram memory - {total_available_ram_memory}\\n\")\n",
    "\n",
    "tuner_ob = Tuner(model = model,tokenizer = tokenizer,dataset = samples_to_tune_on,\n",
    "                 device = device, batch_size = 512,\n",
    "                 tokenizer_encoding_kwargs={'padding': True, 'truncation': True, 'max_length': 512},\n",
    "                 tokenizer_decoding_kwargs = {'skip_special_tokens' : True,  'spaces_between_special_tokens' : False}, \n",
    "                 scorer = get_rouge_score, prompt_template = pt, is_encoder_decoder = True, seed = seed, column_mapping = {\"text_column_name\": \"X\", \"label_column_name\": \"y\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier\n",
    "from llmsearch.utils.model_utils import seed_everything\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "parameters and how they affect do_sample == False\n",
    "1. temperature - output does not change - greedy decoding\n",
    "2. top_k - output does not change - greedy decoding\n",
    "3. repetition_penalty - output changes\n",
    "4. no_repeat_ngram_size - output changes\n",
    "\"\"\"\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "initial_generation_params1 = {\n",
    "    'max_new_tokens' : 120,\n",
    "}\n",
    "# score, outputs1 = tuner_ob.get_score(initial_generation_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bec4531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5 fold means, whole sample set of 100 examples will be split into 80:20 ratio\\nfor each hyper_parameter set we have a model f(hyper_params)\\n    - we will evaluate this model and get the cross val score (test on each 20 samples 5 times, while training on the rest 80 each time)\\n    - we get the score on the quality of hyperparams by evaluating the model with the hyperparams on the unseen 1 fold\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_param_grid = {\n",
    "    \"max_new_tokens\": [120],\n",
    "    \"temperature\": list(np.linspace(start=0.1, stop=1.0,num=10)),\n",
    "    'top_k' : list(map(int,np.linspace(start=10, stop=50,num=5))),\n",
    "    \"top_p\": [0.75, 0.8, 0.9, 1.0],\n",
    "    'do_sample' : [True, False],\n",
    "    'generation_seed' : [42],\n",
    "    'repetition_penalty' : [1.0, 1.2],\n",
    "    'no_repeat_ngram_size' : [0,2,3],\n",
    "}\n",
    "\n",
    "hyp_param_grid_2= {\n",
    "    \"max_new_tokens\": [120],\n",
    "    \"temperature\": list(np.linspace(start=0.1, stop=1.0,num=10000)),\n",
    "    'top_k' : list(map(int,np.linspace(start=10, stop=50,num=5000))),\n",
    "    \"top_p\": list(map(int,np.linspace(start=10, stop=50,num=5000))),\n",
    "    'do_sample' : [True],\n",
    "    'generation_seed' : [42],\n",
    "#     'repetition_penalty' : [1.0, 1.2],\n",
    "#     'no_repeat_ngram_size' : [0,2,3],\n",
    "}\n",
    "\n",
    "scorer = make_scorer(score_func=get_rouge_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=tuner_ob.estimator,\n",
    "    param_distributions=hyp_param_grid_2,\n",
    "    n_iter = 2,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state = 42,\n",
    "    n_jobs=1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "5 fold means, whole sample set of 100 examples will be split into 80:20 ratio\n",
    "for each hyper_parameter set we have a model f(hyper_params)\n",
    "    - we will evaluate this model and get the cross val score (test on each 20 samples 5 times, while training on the rest 80 each time)\n",
    "    - we get the score on the quality of hyperparams by evaluating the model with the hyperparams on the unseen 1 fold\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce54ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuner_ob.dataset['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a5c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a):\n",
    "    for item in a:\n",
    "        print(f\"input to function is {a}\")\n",
    "        foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1967d672",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cloning\n",
      "{} \n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Before cloning\n",
      "{} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a974d7c4574cd99e62fef81bb8aad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:719: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:2671: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 6, Total available gpu memory - None\n",
      "\n",
      "[CV 1/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.6999099909990999, top_k=17, top_p=36;, score=0.172 total time=  14.0s\n",
      "Before cloning\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed1a2a000b04762af677a7561512020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 2/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.6999099909990999, top_k=17, top_p=36;, score=0.177 total time=   3.8s\n",
      "Before cloning\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f3f981bbb6451096d3da64475eba96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 3/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.6999099909990999, top_k=17, top_p=36;, score=0.169 total time=   3.0s\n",
      "Before cloning\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b17c6a0b6a4381b94a6464ac4edb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 4/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.6999099909990999, top_k=17, top_p=36;, score=0.187 total time=   7.1s\n",
      "Before cloning\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67625bc87704c79a0fb93bf5e248333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 5/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.6999099909990999, top_k=17, top_p=36;, score=0.198 total time=   2.4s\n",
      "Before cloning\n",
      "{'top_p': 36, 'top_k': 17, 'temperature': 0.6999099909990999, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6a0a0adfe1418fac5d903c9a33d421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 1/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.5357335733573357, top_k=45, top_p=15;, score=0.197 total time=   3.0s\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a02c3de99343d69f251406fb855fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 2/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.5357335733573357, top_k=45, top_p=15;, score=0.197 total time=   5.1s\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22de22d98f6b4d7f9e53269cd76f9f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 3/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.5357335733573357, top_k=45, top_p=15;, score=0.220 total time=   2.4s\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf90d44fd0645a19018afb2333e15a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 4/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.5357335733573357, top_k=45, top_p=15;, score=0.179 total time=   3.1s\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Performing inference with batch_size - 512\n",
      "Performing inference using - {'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aa9b594f1e43af94986705523626eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 5, Total available gpu memory - None\n",
      "\n",
      "[CV 5/5] END do_sample=True, generation_seed=42, max_new_tokens=120, temperature=0.5357335733573357, top_k=45, top_p=15;, score=0.209 total time=   3.0s\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "After setting\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n",
      "Before cloning\n",
      "{'top_p': 15, 'top_k': 45, 'temperature': 0.5357335733573357, 'max_new_tokens': 120, 'generation_seed': 42, 'do_sample': True} \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=LLMEstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                                                 disable_batch_size_cache=False,\n",
       "                                                 do_sample=True,\n",
       "                                                 generation_seed=42,\n",
       "                                                 is_encoder_decoder=True,\n",
       "                                                 is_fitted_=True,\n",
       "                                                 max_new_tokens=120,\n",
       "                                                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Bloc...\n",
       "                                                        0.10234023402340235,\n",
       "                                                        0.10243024302430244,\n",
       "                                                        0.10252025202520253,\n",
       "                                                        0.10261026102610261, ...],\n",
       "                                        &#x27;top_k&#x27;: [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...],\n",
       "                                        &#x27;top_p&#x27;: [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=LLMEstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                                                 disable_batch_size_cache=False,\n",
       "                                                 do_sample=True,\n",
       "                                                 generation_seed=42,\n",
       "                                                 is_encoder_decoder=True,\n",
       "                                                 is_fitted_=True,\n",
       "                                                 max_new_tokens=120,\n",
       "                                                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Bloc...\n",
       "                                                        0.10234023402340235,\n",
       "                                                        0.10243024302430244,\n",
       "                                                        0.10252025202520253,\n",
       "                                                        0.10261026102610261, ...],\n",
       "                                        &#x27;top_k&#x27;: [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...],\n",
       "                                        &#x27;top_p&#x27;: [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                    disable_batch_size_cache=False, do_sample=True,\n",
       "                    generation_seed=42, is_encoder_decoder=True,\n",
       "                    is_fitted_=True, max_new_tokens=120,\n",
       "                    model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSe...\n",
       "                    tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=32100, model_max_length=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens={&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;extra_id_0&gt;&#x27;, &#x27;&lt;extra_id_1&gt;&#x27;, &#x27;&lt;extra_id_2&gt;&#x27;, &#x27;&lt;extra_id_3&gt;&#x27;, &#x27;&lt;extra_id_4&gt;&#x27;, &#x27;&lt;extra_id_5&gt;&#x27;, &#x27;&lt;extra_id_6&gt;&#x27;, &#x27;&lt;extra_id_7&gt;&#x27;, &#x27;&lt;extra_id_8&gt;&#x27;, &#x27;&lt;extra_id_9&gt;&#x27;, &#x27;&lt;extra_id_10&gt;&#x27;, &#x27;&lt;extra_id_11&gt;&#x27;, &#x27;&lt;extra_id_12&gt;&#x27;, &#x27;&lt;extra_id_13&gt;&#x27;, &#x27;&lt;extra_id_14&gt;&#x27;, &#x27;&lt;extra_id_15&gt;&#x27;, &#x27;&lt;extra_id_16&gt;&#x27;, &#x27;&lt;extra_id_17&gt;&#x27;, &#x27;&lt;extra_id_18&gt;&#x27;, &#x27;&lt;extra_id_19&gt;&#x27;, &#x27;&lt;extra_id_20&gt;&#x27;, &#x27;&lt;extra_id_21&gt;&#x27;, &#x27;&lt;extra_id_22&gt;&#x27;, &#x27;&lt;extra_id_23&gt;&#x27;, &#x27;&lt;extra_id_24&gt;&#x27;, &#x27;&lt;extra_id_25&gt;&#x27;, &#x27;&lt;extra_id_26&gt;&#x27;, &#x27;&lt;extra_id_27&gt;&#x27;, &#x27;&lt;extra_id_28&gt;&#x27;, &#x27;&lt;extra_id_29&gt;&#x27;, &#x27;&lt;extra_id_30&gt;&#x27;, &#x27;&lt;extra_id_31&gt;&#x27;, &#x27;&lt;extra_id_32&gt;&#x27;, &#x27;&lt;extra_id_33&gt;&#x27;, &#x27;&lt;extra_id_34&gt;&#x27;, &#x27;&lt;extra_id_35&gt;&#x27;, &#x27;&lt;extra_id_36&gt;&#x27;, &#x27;&lt;extra_id_37&gt;&#x27;, &#x27;&lt;extra_id_38&gt;&#x27;, &#x27;&lt;extra_id_39&gt;&#x27;, &#x27;&lt;extra_id_40&gt;&#x27;, &#x27;&lt;extra_id_41&gt;&#x27;, &#x27;&lt;extra_id_42&gt;&#x27;, &#x27;&lt;extra_id_43&gt;&#x27;, &#x27;&lt;extra_id_44&gt;&#x27;, &#x27;&lt;extra_id_45&gt;&#x27;, &#x27;&lt;extra_id_46&gt;&#x27;, &#x27;&lt;extra_id_47&gt;&#x27;, &#x27;&lt;extra_id_48&gt;&#x27;, &#x27;&lt;extra_id_49&gt;&#x27;, &#x27;&lt;extra_id_50&gt;&#x27;, &#x27;&lt;extra_id_51&gt;&#x27;, &#x27;&lt;extra_id_52&gt;&#x27;, &#x27;&lt;extra_id_53&gt;&#x27;, &#x27;&lt;extra_id_54&gt;&#x27;, &#x27;&lt;extra_id_55&gt;&#x27;, &#x27;&lt;extra_id_56&gt;&#x27;, &#x27;&lt;extra_id_57&gt;&#x27;, &#x27;&lt;extra_id_58&gt;&#x27;, &#x27;&lt;extra_id_59&gt;&#x27;, &#x27;&lt;extra_id_60&gt;&#x27;, &#x27;&lt;extra_id_61&gt;&#x27;, &#x27;&lt;extra_id_62&gt;&#x27;, &#x27;&lt;extra_id_63&gt;&#x27;, &#x27;&lt;extra_id_64&gt;&#x27;, &#x27;&lt;extra_id_65&gt;&#x27;, &#x27;&lt;extra_id_66&gt;&#x27;, &#x27;&lt;extra_id_67&gt;&#x27;, &#x27;&lt;extra_id_68&gt;&#x27;, &#x27;&lt;extra_id_69&gt;&#x27;, &#x27;&lt;extra_id_70&gt;&#x27;, &#x27;&lt;extra_id_71&gt;&#x27;, &#x27;&lt;extra_id_72&gt;&#x27;, &#x27;&lt;extra_id_73&gt;&#x27;, &#x27;&lt;extra_id_74&gt;&#x27;, &#x27;&lt;extra_id_75&gt;&#x27;, &#x27;&lt;extra_id_76&gt;&#x27;, &#x27;&lt;extra_id_77&gt;&#x27;, &#x27;&lt;extra_id_78&gt;&#x27;, &#x27;&lt;extra_id_79&gt;&#x27;, &#x27;&lt;extra_id_80&gt;&#x27;, &#x27;&lt;extra_id_81&gt;&#x27;, &#x27;&lt;extra_id_82&gt;&#x27;, &#x27;&lt;extra_id_83&gt;&#x27;, &#x27;&lt;extra_id_84&gt;&#x27;, &#x27;&lt;extra_id_85&gt;&#x27;, &#x27;&lt;extra_id_86&gt;&#x27;, &#x27;&lt;extra_id_87&gt;&#x27;, &#x27;&lt;extra_id_88&gt;&#x27;, &#x27;&lt;extra_id_89&gt;&#x27;, &#x27;&lt;extra_id_90&gt;&#x27;, &#x27;&lt;extra_id_91&gt;&#x27;, &#x27;&lt;extra_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                    tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                               &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512,\n",
       "                                               &#x27;padding&#x27;: True,\n",
       "                                               &#x27;truncation&#x27;: True},\n",
       "                    top_k=45, top_p=15)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                    disable_batch_size_cache=False, do_sample=True,\n",
       "                    generation_seed=42, is_encoder_decoder=True,\n",
       "                    is_fitted_=True, max_new_tokens=120,\n",
       "                    model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSe...\n",
       "                    tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=32100, model_max_length=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens={&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;extra_id_0&gt;&#x27;, &#x27;&lt;extra_id_1&gt;&#x27;, &#x27;&lt;extra_id_2&gt;&#x27;, &#x27;&lt;extra_id_3&gt;&#x27;, &#x27;&lt;extra_id_4&gt;&#x27;, &#x27;&lt;extra_id_5&gt;&#x27;, &#x27;&lt;extra_id_6&gt;&#x27;, &#x27;&lt;extra_id_7&gt;&#x27;, &#x27;&lt;extra_id_8&gt;&#x27;, &#x27;&lt;extra_id_9&gt;&#x27;, &#x27;&lt;extra_id_10&gt;&#x27;, &#x27;&lt;extra_id_11&gt;&#x27;, &#x27;&lt;extra_id_12&gt;&#x27;, &#x27;&lt;extra_id_13&gt;&#x27;, &#x27;&lt;extra_id_14&gt;&#x27;, &#x27;&lt;extra_id_15&gt;&#x27;, &#x27;&lt;extra_id_16&gt;&#x27;, &#x27;&lt;extra_id_17&gt;&#x27;, &#x27;&lt;extra_id_18&gt;&#x27;, &#x27;&lt;extra_id_19&gt;&#x27;, &#x27;&lt;extra_id_20&gt;&#x27;, &#x27;&lt;extra_id_21&gt;&#x27;, &#x27;&lt;extra_id_22&gt;&#x27;, &#x27;&lt;extra_id_23&gt;&#x27;, &#x27;&lt;extra_id_24&gt;&#x27;, &#x27;&lt;extra_id_25&gt;&#x27;, &#x27;&lt;extra_id_26&gt;&#x27;, &#x27;&lt;extra_id_27&gt;&#x27;, &#x27;&lt;extra_id_28&gt;&#x27;, &#x27;&lt;extra_id_29&gt;&#x27;, &#x27;&lt;extra_id_30&gt;&#x27;, &#x27;&lt;extra_id_31&gt;&#x27;, &#x27;&lt;extra_id_32&gt;&#x27;, &#x27;&lt;extra_id_33&gt;&#x27;, &#x27;&lt;extra_id_34&gt;&#x27;, &#x27;&lt;extra_id_35&gt;&#x27;, &#x27;&lt;extra_id_36&gt;&#x27;, &#x27;&lt;extra_id_37&gt;&#x27;, &#x27;&lt;extra_id_38&gt;&#x27;, &#x27;&lt;extra_id_39&gt;&#x27;, &#x27;&lt;extra_id_40&gt;&#x27;, &#x27;&lt;extra_id_41&gt;&#x27;, &#x27;&lt;extra_id_42&gt;&#x27;, &#x27;&lt;extra_id_43&gt;&#x27;, &#x27;&lt;extra_id_44&gt;&#x27;, &#x27;&lt;extra_id_45&gt;&#x27;, &#x27;&lt;extra_id_46&gt;&#x27;, &#x27;&lt;extra_id_47&gt;&#x27;, &#x27;&lt;extra_id_48&gt;&#x27;, &#x27;&lt;extra_id_49&gt;&#x27;, &#x27;&lt;extra_id_50&gt;&#x27;, &#x27;&lt;extra_id_51&gt;&#x27;, &#x27;&lt;extra_id_52&gt;&#x27;, &#x27;&lt;extra_id_53&gt;&#x27;, &#x27;&lt;extra_id_54&gt;&#x27;, &#x27;&lt;extra_id_55&gt;&#x27;, &#x27;&lt;extra_id_56&gt;&#x27;, &#x27;&lt;extra_id_57&gt;&#x27;, &#x27;&lt;extra_id_58&gt;&#x27;, &#x27;&lt;extra_id_59&gt;&#x27;, &#x27;&lt;extra_id_60&gt;&#x27;, &#x27;&lt;extra_id_61&gt;&#x27;, &#x27;&lt;extra_id_62&gt;&#x27;, &#x27;&lt;extra_id_63&gt;&#x27;, &#x27;&lt;extra_id_64&gt;&#x27;, &#x27;&lt;extra_id_65&gt;&#x27;, &#x27;&lt;extra_id_66&gt;&#x27;, &#x27;&lt;extra_id_67&gt;&#x27;, &#x27;&lt;extra_id_68&gt;&#x27;, &#x27;&lt;extra_id_69&gt;&#x27;, &#x27;&lt;extra_id_70&gt;&#x27;, &#x27;&lt;extra_id_71&gt;&#x27;, &#x27;&lt;extra_id_72&gt;&#x27;, &#x27;&lt;extra_id_73&gt;&#x27;, &#x27;&lt;extra_id_74&gt;&#x27;, &#x27;&lt;extra_id_75&gt;&#x27;, &#x27;&lt;extra_id_76&gt;&#x27;, &#x27;&lt;extra_id_77&gt;&#x27;, &#x27;&lt;extra_id_78&gt;&#x27;, &#x27;&lt;extra_id_79&gt;&#x27;, &#x27;&lt;extra_id_80&gt;&#x27;, &#x27;&lt;extra_id_81&gt;&#x27;, &#x27;&lt;extra_id_82&gt;&#x27;, &#x27;&lt;extra_id_83&gt;&#x27;, &#x27;&lt;extra_id_84&gt;&#x27;, &#x27;&lt;extra_id_85&gt;&#x27;, &#x27;&lt;extra_id_86&gt;&#x27;, &#x27;&lt;extra_id_87&gt;&#x27;, &#x27;&lt;extra_id_88&gt;&#x27;, &#x27;&lt;extra_id_89&gt;&#x27;, &#x27;&lt;extra_id_90&gt;&#x27;, &#x27;&lt;extra_id_91&gt;&#x27;, &#x27;&lt;extra_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                    tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                               &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512,\n",
       "                                               &#x27;padding&#x27;: True,\n",
       "                                               &#x27;truncation&#x27;: True},\n",
       "                    top_k=45, top_p=15)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=LLMEstimatorWrapper(batch_size=512, device='mps',\n",
       "                                                 disable_batch_size_cache=False,\n",
       "                                                 do_sample=True,\n",
       "                                                 generation_seed=42,\n",
       "                                                 is_encoder_decoder=True,\n",
       "                                                 is_fitted_=True,\n",
       "                                                 max_new_tokens=120,\n",
       "                                                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Bloc...\n",
       "                                                        0.10234023402340235,\n",
       "                                                        0.10243024302430244,\n",
       "                                                        0.10252025202520253,\n",
       "                                                        0.10261026102610261, ...],\n",
       "                                        'top_k': [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...],\n",
       "                                        'top_p': [10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, 10, 10, 10, 10, 10,\n",
       "                                                  10, 10, ...]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"X\"], y=tuner_ob.dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a41bff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_p': 15,\n",
       " 'top_k': 45,\n",
       " 'temperature': 0.5357335733573357,\n",
       " 'max_new_tokens': 120,\n",
       " 'generation_seed': 42,\n",
       " 'do_sample': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e8919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;, disable_batch_size_cache=False,\n",
       "                 do_sample=True, generation_seed=42, is_encoder_decoder=True,\n",
       "                 is_fitted_=True, max_new_tokens=120,\n",
       "                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfA...\n",
       "                 tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=32100, model_max_length=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens={&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;extra_id_0&gt;&#x27;, &#x27;&lt;extra_id_1&gt;&#x27;, &#x27;&lt;extra_id_2&gt;&#x27;, &#x27;&lt;extra_id_3&gt;&#x27;, &#x27;&lt;extra_id_4&gt;&#x27;, &#x27;&lt;extra_id_5&gt;&#x27;, &#x27;&lt;extra_id_6&gt;&#x27;, &#x27;&lt;extra_id_7&gt;&#x27;, &#x27;&lt;extra_id_8&gt;&#x27;, &#x27;&lt;extra_id_9&gt;&#x27;, &#x27;&lt;extra_id_10&gt;&#x27;, &#x27;&lt;extra_id_11&gt;&#x27;, &#x27;&lt;extra_id_12&gt;&#x27;, &#x27;&lt;extra_id_13&gt;&#x27;, &#x27;&lt;extra_id_14&gt;&#x27;, &#x27;&lt;extra_id_15&gt;&#x27;, &#x27;&lt;extra_id_16&gt;&#x27;, &#x27;&lt;extra_id_17&gt;&#x27;, &#x27;&lt;extra_id_18&gt;&#x27;, &#x27;&lt;extra_id_19&gt;&#x27;, &#x27;&lt;extra_id_20&gt;&#x27;, &#x27;&lt;extra_id_21&gt;&#x27;, &#x27;&lt;extra_id_22&gt;&#x27;, &#x27;&lt;extra_id_23&gt;&#x27;, &#x27;&lt;extra_id_24&gt;&#x27;, &#x27;&lt;extra_id_25&gt;&#x27;, &#x27;&lt;extra_id_26&gt;&#x27;, &#x27;&lt;extra_id_27&gt;&#x27;, &#x27;&lt;extra_id_28&gt;&#x27;, &#x27;&lt;extra_id_29&gt;&#x27;, &#x27;&lt;extra_id_30&gt;&#x27;, &#x27;&lt;extra_id_31&gt;&#x27;, &#x27;&lt;extra_id_32&gt;&#x27;, &#x27;&lt;extra_id_33&gt;&#x27;, &#x27;&lt;extra_id_34&gt;&#x27;, &#x27;&lt;extra_id_35&gt;&#x27;, &#x27;&lt;extra_id_36&gt;&#x27;, &#x27;&lt;extra_id_37&gt;&#x27;, &#x27;&lt;extra_id_38&gt;&#x27;, &#x27;&lt;extra_id_39&gt;&#x27;, &#x27;&lt;extra_id_40&gt;&#x27;, &#x27;&lt;extra_id_41&gt;&#x27;, &#x27;&lt;extra_id_42&gt;&#x27;, &#x27;&lt;extra_id_43&gt;&#x27;, &#x27;&lt;extra_id_44&gt;&#x27;, &#x27;&lt;extra_id_45&gt;&#x27;, &#x27;&lt;extra_id_46&gt;&#x27;, &#x27;&lt;extra_id_47&gt;&#x27;, &#x27;&lt;extra_id_48&gt;&#x27;, &#x27;&lt;extra_id_49&gt;&#x27;, &#x27;&lt;extra_id_50&gt;&#x27;, &#x27;&lt;extra_id_51&gt;&#x27;, &#x27;&lt;extra_id_52&gt;&#x27;, &#x27;&lt;extra_id_53&gt;&#x27;, &#x27;&lt;extra_id_54&gt;&#x27;, &#x27;&lt;extra_id_55&gt;&#x27;, &#x27;&lt;extra_id_56&gt;&#x27;, &#x27;&lt;extra_id_57&gt;&#x27;, &#x27;&lt;extra_id_58&gt;&#x27;, &#x27;&lt;extra_id_59&gt;&#x27;, &#x27;&lt;extra_id_60&gt;&#x27;, &#x27;&lt;extra_id_61&gt;&#x27;, &#x27;&lt;extra_id_62&gt;&#x27;, &#x27;&lt;extra_id_63&gt;&#x27;, &#x27;&lt;extra_id_64&gt;&#x27;, &#x27;&lt;extra_id_65&gt;&#x27;, &#x27;&lt;extra_id_66&gt;&#x27;, &#x27;&lt;extra_id_67&gt;&#x27;, &#x27;&lt;extra_id_68&gt;&#x27;, &#x27;&lt;extra_id_69&gt;&#x27;, &#x27;&lt;extra_id_70&gt;&#x27;, &#x27;&lt;extra_id_71&gt;&#x27;, &#x27;&lt;extra_id_72&gt;&#x27;, &#x27;&lt;extra_id_73&gt;&#x27;, &#x27;&lt;extra_id_74&gt;&#x27;, &#x27;&lt;extra_id_75&gt;&#x27;, &#x27;&lt;extra_id_76&gt;&#x27;, &#x27;&lt;extra_id_77&gt;&#x27;, &#x27;&lt;extra_id_78&gt;&#x27;, &#x27;&lt;extra_id_79&gt;&#x27;, &#x27;&lt;extra_id_80&gt;&#x27;, &#x27;&lt;extra_id_81&gt;&#x27;, &#x27;&lt;extra_id_82&gt;&#x27;, &#x27;&lt;extra_id_83&gt;&#x27;, &#x27;&lt;extra_id_84&gt;&#x27;, &#x27;&lt;extra_id_85&gt;&#x27;, &#x27;&lt;extra_id_86&gt;&#x27;, &#x27;&lt;extra_id_87&gt;&#x27;, &#x27;&lt;extra_id_88&gt;&#x27;, &#x27;&lt;extra_id_89&gt;&#x27;, &#x27;&lt;extra_id_90&gt;&#x27;, &#x27;&lt;extra_id_91&gt;&#x27;, &#x27;&lt;extra_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                 tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                            &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                 tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512, &#x27;padding&#x27;: True,\n",
       "                                            &#x27;truncation&#x27;: True},\n",
       "                 top_k=10, top_p=0.75)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;, disable_batch_size_cache=False,\n",
       "                 do_sample=True, generation_seed=42, is_encoder_decoder=True,\n",
       "                 is_fitted_=True, max_new_tokens=120,\n",
       "                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfA...\n",
       "                 tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=32100, model_max_length=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens={&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;extra_id_0&gt;&#x27;, &#x27;&lt;extra_id_1&gt;&#x27;, &#x27;&lt;extra_id_2&gt;&#x27;, &#x27;&lt;extra_id_3&gt;&#x27;, &#x27;&lt;extra_id_4&gt;&#x27;, &#x27;&lt;extra_id_5&gt;&#x27;, &#x27;&lt;extra_id_6&gt;&#x27;, &#x27;&lt;extra_id_7&gt;&#x27;, &#x27;&lt;extra_id_8&gt;&#x27;, &#x27;&lt;extra_id_9&gt;&#x27;, &#x27;&lt;extra_id_10&gt;&#x27;, &#x27;&lt;extra_id_11&gt;&#x27;, &#x27;&lt;extra_id_12&gt;&#x27;, &#x27;&lt;extra_id_13&gt;&#x27;, &#x27;&lt;extra_id_14&gt;&#x27;, &#x27;&lt;extra_id_15&gt;&#x27;, &#x27;&lt;extra_id_16&gt;&#x27;, &#x27;&lt;extra_id_17&gt;&#x27;, &#x27;&lt;extra_id_18&gt;&#x27;, &#x27;&lt;extra_id_19&gt;&#x27;, &#x27;&lt;extra_id_20&gt;&#x27;, &#x27;&lt;extra_id_21&gt;&#x27;, &#x27;&lt;extra_id_22&gt;&#x27;, &#x27;&lt;extra_id_23&gt;&#x27;, &#x27;&lt;extra_id_24&gt;&#x27;, &#x27;&lt;extra_id_25&gt;&#x27;, &#x27;&lt;extra_id_26&gt;&#x27;, &#x27;&lt;extra_id_27&gt;&#x27;, &#x27;&lt;extra_id_28&gt;&#x27;, &#x27;&lt;extra_id_29&gt;&#x27;, &#x27;&lt;extra_id_30&gt;&#x27;, &#x27;&lt;extra_id_31&gt;&#x27;, &#x27;&lt;extra_id_32&gt;&#x27;, &#x27;&lt;extra_id_33&gt;&#x27;, &#x27;&lt;extra_id_34&gt;&#x27;, &#x27;&lt;extra_id_35&gt;&#x27;, &#x27;&lt;extra_id_36&gt;&#x27;, &#x27;&lt;extra_id_37&gt;&#x27;, &#x27;&lt;extra_id_38&gt;&#x27;, &#x27;&lt;extra_id_39&gt;&#x27;, &#x27;&lt;extra_id_40&gt;&#x27;, &#x27;&lt;extra_id_41&gt;&#x27;, &#x27;&lt;extra_id_42&gt;&#x27;, &#x27;&lt;extra_id_43&gt;&#x27;, &#x27;&lt;extra_id_44&gt;&#x27;, &#x27;&lt;extra_id_45&gt;&#x27;, &#x27;&lt;extra_id_46&gt;&#x27;, &#x27;&lt;extra_id_47&gt;&#x27;, &#x27;&lt;extra_id_48&gt;&#x27;, &#x27;&lt;extra_id_49&gt;&#x27;, &#x27;&lt;extra_id_50&gt;&#x27;, &#x27;&lt;extra_id_51&gt;&#x27;, &#x27;&lt;extra_id_52&gt;&#x27;, &#x27;&lt;extra_id_53&gt;&#x27;, &#x27;&lt;extra_id_54&gt;&#x27;, &#x27;&lt;extra_id_55&gt;&#x27;, &#x27;&lt;extra_id_56&gt;&#x27;, &#x27;&lt;extra_id_57&gt;&#x27;, &#x27;&lt;extra_id_58&gt;&#x27;, &#x27;&lt;extra_id_59&gt;&#x27;, &#x27;&lt;extra_id_60&gt;&#x27;, &#x27;&lt;extra_id_61&gt;&#x27;, &#x27;&lt;extra_id_62&gt;&#x27;, &#x27;&lt;extra_id_63&gt;&#x27;, &#x27;&lt;extra_id_64&gt;&#x27;, &#x27;&lt;extra_id_65&gt;&#x27;, &#x27;&lt;extra_id_66&gt;&#x27;, &#x27;&lt;extra_id_67&gt;&#x27;, &#x27;&lt;extra_id_68&gt;&#x27;, &#x27;&lt;extra_id_69&gt;&#x27;, &#x27;&lt;extra_id_70&gt;&#x27;, &#x27;&lt;extra_id_71&gt;&#x27;, &#x27;&lt;extra_id_72&gt;&#x27;, &#x27;&lt;extra_id_73&gt;&#x27;, &#x27;&lt;extra_id_74&gt;&#x27;, &#x27;&lt;extra_id_75&gt;&#x27;, &#x27;&lt;extra_id_76&gt;&#x27;, &#x27;&lt;extra_id_77&gt;&#x27;, &#x27;&lt;extra_id_78&gt;&#x27;, &#x27;&lt;extra_id_79&gt;&#x27;, &#x27;&lt;extra_id_80&gt;&#x27;, &#x27;&lt;extra_id_81&gt;&#x27;, &#x27;&lt;extra_id_82&gt;&#x27;, &#x27;&lt;extra_id_83&gt;&#x27;, &#x27;&lt;extra_id_84&gt;&#x27;, &#x27;&lt;extra_id_85&gt;&#x27;, &#x27;&lt;extra_id_86&gt;&#x27;, &#x27;&lt;extra_id_87&gt;&#x27;, &#x27;&lt;extra_id_88&gt;&#x27;, &#x27;&lt;extra_id_89&gt;&#x27;, &#x27;&lt;extra_id_90&gt;&#x27;, &#x27;&lt;extra_id_91&gt;&#x27;, &#x27;&lt;extra_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                 tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                            &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                 tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512, &#x27;padding&#x27;: True,\n",
       "                                            &#x27;truncation&#x27;: True},\n",
       "                 top_k=10, top_p=0.75)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "EstimatorWrapper(batch_size=512, device='mps', disable_batch_size_cache=False,\n",
       "                 do_sample=True, generation_seed=42, is_encoder_decoder=True,\n",
       "                 is_fitted_=True, max_new_tokens=120,\n",
       "                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfA...\n",
       "                 tokenizer=T5Tokenizer(name_or_path='google/flan-t5-small', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),\n",
       "                 tokenizer_decoding_kwargs={'skip_special_tokens': True,\n",
       "                                            'spaces_between_special_tokens': False},\n",
       "                 tokenizer_encoding_kwargs={'max_length': 512, 'padding': True,\n",
       "                                            'truncation': True},\n",
       "                 top_k=10, top_p=0.75)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c08e1269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': T5ForConditionalGeneration(\n",
       "   (shared): Embedding(32128, 512)\n",
       "   (encoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 512)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 6)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedActDense(\n",
       "               (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-7): 7 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedActDense(\n",
       "               (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 512)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 6)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedActDense(\n",
       "               (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-7): 7 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "               (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedActDense(\n",
       "               (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "               (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       " ),\n",
       " 'tokenizer': T5Tokenizer(name_or_path='google/flan-t5-small', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),\n",
       " 'is_encoder_decoder': True,\n",
       " 'device': 'mps',\n",
       " 'scorer': make_scorer(get_rouge_score),\n",
       " 'batch_size': 512,\n",
       " 'disable_batch_size_cache': False,\n",
       " 'tokenizer_encoding_kwargs': {'padding': True,\n",
       "  'truncation': True,\n",
       "  'max_length': 512},\n",
       " 'tokenizer_decoding_kwargs': {'skip_special_tokens': True,\n",
       "  'spaces_between_special_tokens': False},\n",
       " 'pred_function': None,\n",
       " 'top_p': 0.8,\n",
       " 'top_k': 10,\n",
       " 'temperature': 0.4,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'max_new_tokens': 120,\n",
       " 'generation_seed': 42,\n",
       " 'do_sample': True,\n",
       " 'is_fitted_': True,\n",
       " 'optimal_batch_size': 512}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.set_params(**clf.best_params_).get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c02681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "clone(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a846cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27049",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
