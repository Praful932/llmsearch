{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25b6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device - mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "\n",
    "print(f\"Device - {device}\")\n",
    "\n",
    "def beep(duration = 1, frequency=440, rhythm=1):\n",
    "    sample_rate = 44100  # Standard audio sample rate\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    audio_data = np.sin(2*np.pi*frequency*t)  # Generate a sine wave\n",
    "    audio_data *= np.where(np.arange(len(audio_data)) % rhythm == 0, 1, 0)  # Apply rhythm\n",
    "    display(Audio(audio_data, rate=sample_rate, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2fe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/praful932/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5245dd27e8469aae6609eac7786917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca4a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"train\"][:sample_size])\n",
    "samples_to_tune_on = samples_to_tune_on.rename_columns(column_mapping = {'dialogue' : 'X', 'summary' : \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40ddb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = False)\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ba55bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conversation: Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\\nSummary:\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "X = samples_to_tune_on[0]['X']\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(\"Conversation: {X}\\nSummary:\")\n",
    "\n",
    "pt.format_prompt(X = X).to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cddba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True)\n",
    "\n",
    "    return result['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87baa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available ram memory - 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import get_total_available_ram, get_gpu_information\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "seed = 42\n",
    "\n",
    "total_available_ram_memory = get_total_available_ram()\n",
    "\n",
    "print(f\"Total available ram memory - {total_available_ram_memory}\\n\")\n",
    "\n",
    "tuner_ob = Tuner(model = \"model\",tokenizer = tokenizer,dataset = samples_to_tune_on,device = device, batch_size = 512,tokenizer_encoding_kwargs={'padding': True, 'truncation': True, 'max_length': 512},tokenizer_decoding_kwargs = {'skip_special_tokens' : True,  'spaces_between_special_tokens' : False}, scorer = get_rouge_score, prompt_template = pt, is_encoder_decoder = True, seed = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier\n",
    "from llmsearch.utils.model_utils import seed_everything\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "parameters and how they affect do_sample == False\n",
    "1. temperature - output does not change - greedy decoding\n",
    "2. top_k - output does not change - greedy decoding\n",
    "3. repetition_penalty - output changes\n",
    "4. no_repeat_ngram_size - output changes\n",
    "\"\"\"\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "initial_generation_params1 = {\n",
    "    'max_new_tokens' : 120,\n",
    "}\n",
    "# score, outputs1 = tuner_ob.get_score(initial_generation_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bec4531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5 fold means, whole sample set of 100 examples will be split into 80:20 ratio\\nfor each hyper_parameter set we have a model f(hyper_params)\\n    - we will evaluate this model and get the cross val score (test on each 20 samples 5 times, while training on the rest 80 each time)\\n    - we get the score on the quality of hyperparams by evaluating the model with the hyperparams on the unseen 1 fold\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_param_grid = {\n",
    "    \"max_new_tokens\": [120],\n",
    "    \"temperature\": list(np.linspace(start=0.1, stop=1.0,num=10)),\n",
    "    'top_k' : list(map(int,np.linspace(start=10, stop=50,num=5))),\n",
    "    \"top_p\": [0.75, 0.8, 0.9, 1.0],\n",
    "    'do_sample' : [True, False],\n",
    "    'generation_seed' : [42],\n",
    "    'repetition_penalty' : [1.0, 1.2],\n",
    "    'no_repeat_ngram_size' : [0,2,3],\n",
    "}\n",
    "\n",
    "scorer = make_scorer(score_func=get_rouge_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=tuner_ob.estimator,\n",
    "    param_distributions=hyp_param_grid,\n",
    "    n_iter = 1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state = 42,\n",
    "    n_jobs=1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "5 fold means, whole sample set of 100 examples will be split into 80:20 ratio\n",
    "for each hyper_parameter set we have a model f(hyper_params)\n",
    "    - we will evaluate this model and get the cross val score (test on each 20 samples 5 times, while training on the rest 80 each time)\n",
    "    - we get the score on the quality of hyperparams by evaluating the model with the hyperparams on the unseen 1 fold\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68535402",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isclass() missing 1 required positional argument: 'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: isclass() missing 1 required positional argument: 'object'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.isclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce54ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuner_ob.dataset['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a5c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a):\n",
    "    for item in a:\n",
    "        print(f\"input to function is {a}\")\n",
    "        foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1967d672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling overriden clone\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "calling overriden clone\n",
      "Cloning - top_p 0.75\n",
      "in native - 0.75\n",
      "Cloning - top_k 10\n",
      "in native - 10\n",
      "Cloning - temperature 0.4\n",
      "in native - 0.4\n",
      "Cloning - repetition_penalty 1.0\n",
      "in native - 1.0\n",
      "Cloning - no_repeat_ngram_size 3\n",
      "in native - 3\n",
      "Cloning - max_new_tokens 120\n",
      "in native - 120\n",
      "Cloning - generation_seed 42\n",
      "in native - 42\n",
      "Cloning - do_sample True\n",
      "in native - True\n",
      "Performing inference with batch_size - 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3987caef0dc4a28a9da2edf09127f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END do_sample=True, generation_seed=42, max_new_tokens=120, no_repeat_ngram_size=3, repetition_penalty=1.0, temperature=0.4, top_k=10, top_p=0.75;, score=nan total time=   0.1s\n",
      "calling overriden clone\n",
      "Cloning - top_p 0.75\n",
      "in native - 0.75\n",
      "Cloning - top_k 10\n",
      "in native - 10\n",
      "Cloning - temperature 0.4\n",
      "in native - 0.4\n",
      "Cloning - repetition_penalty 1.0\n",
      "in native - 1.0\n",
      "Cloning - no_repeat_ngram_size 3\n",
      "in native - 3\n",
      "Cloning - max_new_tokens 120\n",
      "in native - 120\n",
      "Cloning - generation_seed 42\n",
      "in native - 42\n",
      "Cloning - do_sample True\n",
      "in native - True\n",
      "Performing inference with batch_size - 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:825: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 814, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "  File \"/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-VBcDHvMD-py3.8/lib/python3.8/site-packages/sklearn/utils/_response.py\", line 109, in _get_response_values\n",
      "    y_pred, pos_label = estimator.predict(X), None\n",
      "  File \"/Users/praful932/myfiles/code/llmsearch/notebooks/../llmsearch/tuner/tuner.py\", line 99, in predict\n",
      "    output, self.optimal_batch_size = infer_data(\n",
      "  File \"/Users/praful932/myfiles/code/llmsearch/notebooks/../llmsearch/utils/mem_utils.py\", line 79, in inner_wrapper\n",
      "    res = func(*args,batch_size = batch_size,disable_batch_size_cache=disable_batch_size_cache, **kwargs)\n",
      "  File \"/Users/praful932/myfiles/code/llmsearch/notebooks/../llmsearch/utils/model_utils.py\", line 105, in infer_data\n",
      "    output_ids = model.generate(inputs=input_ids,attention_mask=attention_mask, **generation_kwargs)\n",
      "AttributeError: 'str' object has no attribute 'generate'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce93fc2f64a24c83bdfe104b05689180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END do_sample=True, generation_seed=42, max_new_tokens=120, no_repeat_ngram_size=3, repetition_penalty=1.0, temperature=0.4, top_k=10, top_p=0.75;, score=nan total time=   0.1s\n",
      "calling overriden clone\n",
      "Cloning - top_p 0.75\n",
      "in native - 0.75\n",
      "Cloning - top_k 10\n",
      "in native - 10\n",
      "Cloning - temperature 0.4\n",
      "in native - 0.4\n",
      "Cloning - repetition_penalty 1.0\n",
      "in native - 1.0\n",
      "Cloning - no_repeat_ngram_size 3\n",
      "in native - 3\n",
      "Cloning - max_new_tokens 120\n",
      "in native - 120\n",
      "Cloning - generation_seed 42\n",
      "in native - 42\n",
      "Cloning - do_sample True\n",
      "in native - True\n",
      "Performing inference with batch_size - 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d98692b075340da98dd1227aba52707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END do_sample=True, generation_seed=42, max_new_tokens=120, no_repeat_ngram_size=3, repetition_penalty=1.0, temperature=0.4, top_k=10, top_p=0.75;, score=nan total time=   0.1s\n",
      "calling overriden clone\n",
      "Cloning - top_p 0.75\n",
      "in native - 0.75\n",
      "Cloning - top_k 10\n",
      "in native - 10\n",
      "Cloning - temperature 0.4\n",
      "in native - 0.4\n",
      "Cloning - repetition_penalty 1.0\n",
      "in native - 1.0\n",
      "Cloning - no_repeat_ngram_size 3\n",
      "in native - 3\n",
      "Cloning - max_new_tokens 120\n",
      "in native - 120\n",
      "Cloning - generation_seed 42\n",
      "in native - 42\n",
      "Cloning - do_sample True\n",
      "in native - True\n",
      "Performing inference with batch_size - 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68208329b7fb4fefaf6101972cb0243d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END do_sample=True, generation_seed=42, max_new_tokens=120, no_repeat_ngram_size=3, repetition_penalty=1.0, temperature=0.4, top_k=10, top_p=0.75;, score=nan total time=   0.1s\n",
      "calling overriden clone\n",
      "Cloning - top_p 0.75\n",
      "in native - 0.75\n",
      "Cloning - top_k 10\n",
      "in native - 10\n",
      "Cloning - temperature 0.4\n",
      "in native - 0.4\n",
      "Cloning - repetition_penalty 1.0\n",
      "in native - 1.0\n",
      "Cloning - no_repeat_ngram_size 3\n",
      "in native - 3\n",
      "Cloning - max_new_tokens 120\n",
      "in native - 120\n",
      "Cloning - generation_seed 42\n",
      "in native - 42\n",
      "Cloning - do_sample True\n",
      "in native - True\n",
      "Performing inference with batch_size - 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398dac0d9ce24562a8621a5cfcf8f80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END do_sample=True, generation_seed=42, max_new_tokens=120, no_repeat_ngram_size=3, repetition_penalty=1.0, temperature=0.4, top_k=10, top_p=0.75;, score=nan total time=   0.1s\n",
      "calling overriden clone\n",
      "calling overriden clone\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                                              disable_batch_size_cache=False,\n",
       "                                              do_sample=True,\n",
       "                                              generation_seed=42,\n",
       "                                              is_encoder_decoder=True,\n",
       "                                              is_fitted_=True,\n",
       "                                              max_new_tokens=120, model=&#x27;model&#x27;,\n",
       "                                              no_repeat_ngram_size=3,\n",
       "                                              repetition_penalty=1.0,\n",
       "                                              scorer=make_scorer(get_rouge_score),\n",
       "                                              temperature=0.4,\n",
       "                                              tokenizer=T5Tokenizer(name_or_path=...\n",
       "                   param_distributions={&#x27;do_sample&#x27;: [True, False],\n",
       "                                        &#x27;generation_seed&#x27;: [42],\n",
       "                                        &#x27;max_new_tokens&#x27;: [120],\n",
       "                                        &#x27;no_repeat_ngram_size&#x27;: [0, 2, 3],\n",
       "                                        &#x27;repetition_penalty&#x27;: [1.0, 1.2],\n",
       "                                        &#x27;temperature&#x27;: [0.1, 0.2,\n",
       "                                                        0.30000000000000004,\n",
       "                                                        0.4, 0.5, 0.6,\n",
       "                                                        0.7000000000000001, 0.8,\n",
       "                                                        0.9, 1.0],\n",
       "                                        &#x27;top_k&#x27;: [10, 20, 30, 40, 50],\n",
       "                                        &#x27;top_p&#x27;: [0.75, 0.8, 0.9, 1.0]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;,\n",
       "                                              disable_batch_size_cache=False,\n",
       "                                              do_sample=True,\n",
       "                                              generation_seed=42,\n",
       "                                              is_encoder_decoder=True,\n",
       "                                              is_fitted_=True,\n",
       "                                              max_new_tokens=120, model=&#x27;model&#x27;,\n",
       "                                              no_repeat_ngram_size=3,\n",
       "                                              repetition_penalty=1.0,\n",
       "                                              scorer=make_scorer(get_rouge_score),\n",
       "                                              temperature=0.4,\n",
       "                                              tokenizer=T5Tokenizer(name_or_path=...\n",
       "                   param_distributions={&#x27;do_sample&#x27;: [True, False],\n",
       "                                        &#x27;generation_seed&#x27;: [42],\n",
       "                                        &#x27;max_new_tokens&#x27;: [120],\n",
       "                                        &#x27;no_repeat_ngram_size&#x27;: [0, 2, 3],\n",
       "                                        &#x27;repetition_penalty&#x27;: [1.0, 1.2],\n",
       "                                        &#x27;temperature&#x27;: [0.1, 0.2,\n",
       "                                                        0.30000000000000004,\n",
       "                                                        0.4, 0.5, 0.6,\n",
       "                                                        0.7000000000000001, 0.8,\n",
       "                                                        0.9, 1.0],\n",
       "                                        &#x27;top_k&#x27;: [10, 20, 30, 40, 50],\n",
       "                                        &#x27;top_p&#x27;: [0.75, 0.8, 0.9, 1.0]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: EstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;, disable_batch_size_cache=False,\n",
       "                 do_sample=True, generation_seed=42, is_encoder_decoder=True,\n",
       "                 is_fitted_=True, max_new_tokens=120, model=&#x27;model&#x27;,\n",
       "                 no_repeat_ngram_size=3, repetition_penalty=1.0,\n",
       "                 scorer=make_scorer(get_rouge_score), temperature=0.4,\n",
       "                 tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=...a_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                 tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                            &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                 tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512, &#x27;padding&#x27;: True,\n",
       "                                            &#x27;truncation&#x27;: True},\n",
       "                 top_k=10, top_p=0.75)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>EstimatorWrapper(batch_size=512, device=&#x27;mps&#x27;, disable_batch_size_cache=False,\n",
       "                 do_sample=True, generation_seed=42, is_encoder_decoder=True,\n",
       "                 is_fitted_=True, max_new_tokens=120, model=&#x27;model&#x27;,\n",
       "                 no_repeat_ngram_size=3, repetition_penalty=1.0,\n",
       "                 scorer=make_scorer(get_rouge_score), temperature=0.4,\n",
       "                 tokenizer=T5Tokenizer(name_or_path=&#x27;google/flan-t5-small&#x27;, vocab_size=...a_id_92&gt;&#x27;, &#x27;&lt;extra_id_93&gt;&#x27;, &#x27;&lt;extra_id_94&gt;&#x27;, &#x27;&lt;extra_id_95&gt;&#x27;, &#x27;&lt;extra_id_96&gt;&#x27;, &#x27;&lt;extra_id_97&gt;&#x27;, &#x27;&lt;extra_id_98&gt;&#x27;, &#x27;&lt;extra_id_99&gt;&#x27;]}, clean_up_tokenization_spaces=True),\n",
       "                 tokenizer_decoding_kwargs={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                            &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                 tokenizer_encoding_kwargs={&#x27;max_length&#x27;: 512, &#x27;padding&#x27;: True,\n",
       "                                            &#x27;truncation&#x27;: True},\n",
       "                 top_k=10, top_p=0.75)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=EstimatorWrapper(batch_size=512, device='mps',\n",
       "                                              disable_batch_size_cache=False,\n",
       "                                              do_sample=True,\n",
       "                                              generation_seed=42,\n",
       "                                              is_encoder_decoder=True,\n",
       "                                              is_fitted_=True,\n",
       "                                              max_new_tokens=120, model='model',\n",
       "                                              no_repeat_ngram_size=3,\n",
       "                                              repetition_penalty=1.0,\n",
       "                                              scorer=make_scorer(get_rouge_score),\n",
       "                                              temperature=0.4,\n",
       "                                              tokenizer=T5Tokenizer(name_or_path=...\n",
       "                   param_distributions={'do_sample': [True, False],\n",
       "                                        'generation_seed': [42],\n",
       "                                        'max_new_tokens': [120],\n",
       "                                        'no_repeat_ngram_size': [0, 2, 3],\n",
       "                                        'repetition_penalty': [1.0, 1.2],\n",
       "                                        'temperature': [0.1, 0.2,\n",
       "                                                        0.30000000000000004,\n",
       "                                                        0.4, 0.5, 0.6,\n",
       "                                                        0.7000000000000001, 0.8,\n",
       "                                                        0.9, 1.0],\n",
       "                                        'top_k': [10, 20, 30, 40, 50],\n",
       "                                        'top_p': [0.75, 0.8, 0.9, 1.0]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"X\"], y=tuner_ob.dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1d850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.best_estimator_.set_params(**clf.best_params_).get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c02681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "clone(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a846cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27049",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
