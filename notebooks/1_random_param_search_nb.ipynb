{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "from typing import List, Any, Dict, Union, Tuple\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/praful932/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "dataset = datasets.load_dataset(\"samsum\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built(), torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'dialogue', 'summary'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][:100].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def infer_single_sample(\n",
    "    model, input_text: str, tokenizer_encoding_kwargs: Dict, generation_kwargs: Dict\n",
    "):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    input_ids = tokenizer(\n",
    "        text=input_text, **tokenizer_encoding_kwargs, return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, **generation_kwargs)[0]\n",
    "    decoded_output = tokenizer.decode(token_ids=output_ids, skip_special_tokens=True)\n",
    "    end = time.time()\n",
    "    latency = (end - start) * 1000\n",
    "    return decoded_output, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8702ad1c3a54352af0b74364b28d160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-vDz_II8E-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:2256: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.43016766753043256, 'rouge2': 0.1959839090835039, 'rougeL': 0.35965262176158924, 'rougeLsum': 0.3998597106214661}\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(\n",
    "    model, prompt, dataset, tokenizer_encoding_kwargs, generation_kwargs\n",
    "):\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for item in tqdm(samples_to_tune_on):\n",
    "        model_input = prompt.format(input_text=item[\"dialogue\"])\n",
    "        output, _ = infer_single_sample(\n",
    "            model, model_input, tokenizer_encoding_kwargs, generation_kwargs\n",
    "        )\n",
    "        preds.append(output)\n",
    "        gts.append(item[\"summary\"])\n",
    "        # print(f\"Model Output : {output}\")\n",
    "        # print(f\"Label : {item['summary']}\")\n",
    "        # print()\n",
    "        # print(\"===\" * 10)\n",
    "\n",
    "    metric = get_rouge_score(y_true=gts, y_pred=preds)\n",
    "    return metric, preds, gts\n",
    "\n",
    "\n",
    "# take 1st 100 samples\n",
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"validation\"][:sample_size])\n",
    "\n",
    "tokenizer_encoding_kwargs = {\n",
    "    \"max_length\": 1000,\n",
    "    \"truncation\": True,\n",
    "    \"padding\": False,\n",
    "}\n",
    "prompt = \"{input_text} Summarize:\"\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"temperature\": 1,\n",
    "    \"max_new_tokens\": 100,\n",
    "}\n",
    "\n",
    "# Check metric on these examples before any generation param tuning\n",
    "\n",
    "metric, _, _ = get_metrics(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    dataset=samples_to_tune_on,\n",
    "    tokenizer_encoding_kwargs=tokenizer_encoding_kwargs,\n",
    "    generation_kwargs=generation_kwargs,\n",
    ")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Dict\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "tokenizer_encoding_kwargs = {\n",
    "    \"max_length\": 1000,\n",
    "    \"truncation\": True,\n",
    "    \"padding\": \"max_length\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True)\n",
    "\n",
    "    return result[\"rouge2\"]\n",
    "\n",
    "\n",
    "def infer_single_sample(\n",
    "    model, input_text: str, tokenizer_encoding_kwargs: Dict, generation_kwargs: Dict\n",
    "):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    input_ids = tokenizer(\n",
    "        text=input_text, **tokenizer_encoding_kwargs, return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, **generation_kwargs)\n",
    "    decoded_output = tokenizer.batch_decode(\n",
    "        sequences=output_ids, skip_special_tokens=True\n",
    "    )\n",
    "    end = time.time()\n",
    "    latency = (end - start) * 1000\n",
    "    return decoded_output, latency\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def batcher(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def infer_batches(\n",
    "    model,\n",
    "    model_inputs: List,\n",
    "    tokenizer_encoding_kwargs: Dict,\n",
    "    generation_kwargs: Dict,\n",
    "):\n",
    "    gc.collect()\n",
    "    start = time.time()\n",
    "    input_ids = tokenizer(\n",
    "        text=model_inputs, **tokenizer_encoding_kwargs, return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, **generation_kwargs)\n",
    "    decoded_output = tokenizer.batch_decode(\n",
    "        sequences=output_ids, skip_special_tokens=True\n",
    "    )\n",
    "    end = time.time()\n",
    "    latency = (end - start) * 1000\n",
    "    return decoded_output, latency\n",
    "\n",
    "\n",
    "class EstimatorWrapper(BaseEstimator):\n",
    "    def __init__(self, model, inference_batch_size, **kwargs):\n",
    "        self.model = model\n",
    "        self.inference_batch_size = inference_batch_size\n",
    "        for k, v in kwargs.items():\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = []\n",
    "        model_generation_params = {\n",
    "            attr: self.__getattribute__(attr)\n",
    "            for attr in self.model_generation_param_keys\n",
    "        }\n",
    "        for batch in tqdm(batcher(X, batch_size=self.inference_batch_size)):\n",
    "            batch_output, _ = infer_batches(\n",
    "                model=self.model,\n",
    "                model_inputs=batch,\n",
    "                tokenizer_encoding_kwargs=tokenizer_encoding_kwargs,\n",
    "                generation_kwargs=model_generation_params,\n",
    "            )\n",
    "            output.extend(batch_output)\n",
    "        return output\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
    "        parameters of the form ``<component>__<parameter>`` so that it's\n",
    "        possible to update each component of a nested object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : dict\n",
    "            Estimator parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : estimator instance\n",
    "            Estimator instance.\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        self.model_generation_param_keys = params.keys()\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition(\"__\")\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            # TODO(1.4): remove specific handling of \"base_estimator\".\n",
    "            # The \"base_estimator\" key is special. It was deprecated and\n",
    "            # renamed to \"estimator\" for several estimators. This means we\n",
    "            # need to translate it here and set sub-parameters on \"estimator\",\n",
    "            # but only if the user did not explicitly set a value for\n",
    "            # \"base_estimator\".\n",
    "            if (\n",
    "                key == \"base_estimator\"\n",
    "                and valid_params[key] == \"deprecated\"\n",
    "                and self.__module__.startswith(\"sklearn.\")\n",
    "            ):\n",
    "                warnings.warn(\n",
    "                    (\n",
    "                        f\"Parameter 'base_estimator' of {self.__class__.__name__} is\"\n",
    "                        \" deprecated in favor of 'estimator'. See\"\n",
    "                        f\" {self.__class__.__name__}'s docstring for more details.\"\n",
    "                    ),\n",
    "                    FutureWarning,\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                key = \"estimator\"\n",
    "\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# def get_best_hyperparams(model, dataset, hyp_param_grid : Dict, metric : function, output_extraction_function = None, model_generation_attribute = \"generate\"):\n",
    "\n",
    "\n",
    "# take 1st 100 samples\n",
    "samples_to_tune_on = dataset[\"validation\"][:20]\n",
    "hyp_param_grid = {\n",
    "    \"min_new_tokens\": [5],\n",
    "    \"max_new_tokens\": [100],\n",
    "    \"num_beams\": [1, 2, 3],\n",
    "    \"temperature\": [0.3, 0.7, 1],\n",
    "    \"epsilon_cutoff\": [3e-4, 6e-4, 9e-4, 0],\n",
    "    \"repetition_penalty\": [0.3, 0.7, 1.0],\n",
    "    \"no_repeat_ngram_size\": [0, 2, 3, 4],\n",
    "}\n",
    "model_estimator = EstimatorWrapper(model=model, inference_batch_size=2)\n",
    "scorer = make_scorer(score_func=get_rouge_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=model_estimator,\n",
    "    param_distributions=hyp_param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scorer,\n",
    "    cv=2,\n",
    "    random_state=seed,\n",
    "    n_jobs=1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b11b4c8ce4653a17c7ce01b64093a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praful932/Library/Caches/pypoetry/virtualenvs/bhaasha-vDz_II8E-py3.8/lib/python3.8/site-packages/transformers/generation/utils.py:686: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84927c8135341bea3318e88e857c2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff60def1c5d490594075015b1bc4890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c299309930041e08de26d4dd6efd048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b543dc716c554d6c85f63acc75c873ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703689617e354ad7a8db0464c5a54c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d232a5f12b4dc2b76f72516a4d209f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be8312647774cccb9ef3f66d19c11eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f592b3ab524334946cef1a79d6333a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95c753665ae4e05a9cfe25ea88b3baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5492e4cc90d24d2799a3615489a212bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48125fd53c3d45efa5caee40f7ed49bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dbd0e7c1ae4a26b0c15d828a04e2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db05e5acb184eee8bae5b8c5244146b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9284f9ecf8e343be891150db1ea3ee72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73daedded45f4734a07ffd3be4cff163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b200378ca1b4a6b9bdeae894f5ca0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e43a96301940ee8c449485780c9680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c636486b2bb46728f4aeca5b2ee3f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84c19a889f741d294e3a80b51d3c0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=2,\n",
       "                   estimator=EstimatorWrapper(inference_batch_size=2,\n",
       "                                              model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k)...\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")),\n",
       "                   n_jobs=1,\n",
       "                   param_distributions={&#x27;epsilon_cutoff&#x27;: [0.0003, 0.0006,\n",
       "                                                           0.0009, 0],\n",
       "                                        &#x27;max_new_tokens&#x27;: [100],\n",
       "                                        &#x27;min_new_tokens&#x27;: [5],\n",
       "                                        &#x27;no_repeat_ngram_size&#x27;: [0, 2, 3, 4],\n",
       "                                        &#x27;num_beams&#x27;: [1, 2, 3],\n",
       "                                        &#x27;repetition_penalty&#x27;: [0.3, 0.7, 1.0],\n",
       "                                        &#x27;temperature&#x27;: [0.3, 0.7, 1]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=2,\n",
       "                   estimator=EstimatorWrapper(inference_batch_size=2,\n",
       "                                              model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k)...\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")),\n",
       "                   n_jobs=1,\n",
       "                   param_distributions={&#x27;epsilon_cutoff&#x27;: [0.0003, 0.0006,\n",
       "                                                           0.0009, 0],\n",
       "                                        &#x27;max_new_tokens&#x27;: [100],\n",
       "                                        &#x27;min_new_tokens&#x27;: [5],\n",
       "                                        &#x27;no_repeat_ngram_size&#x27;: [0, 2, 3, 4],\n",
       "                                        &#x27;num_beams&#x27;: [1, 2, 3],\n",
       "                                        &#x27;repetition_penalty&#x27;: [0.3, 0.7, 1.0],\n",
       "                                        &#x27;temperature&#x27;: [0.3, 0.7, 1]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: EstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>EstimatorWrapper(inference_batch_size=2,\n",
       "                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_featur...\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>EstimatorWrapper(inference_batch_size=2,\n",
       "                 model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_featur...\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "))</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=EstimatorWrapper(inference_batch_size=2,\n",
       "                                              model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k)...\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")),\n",
       "                   n_jobs=1,\n",
       "                   param_distributions={'epsilon_cutoff': [0.0003, 0.0006,\n",
       "                                                           0.0009, 0],\n",
       "                                        'max_new_tokens': [100],\n",
       "                                        'min_new_tokens': [5],\n",
       "                                        'no_repeat_ngram_size': [0, 2, 3, 4],\n",
       "                                        'num_beams': [1, 2, 3],\n",
       "                                        'repetition_penalty': [0.3, 0.7, 1.0],\n",
       "                                        'temperature': [0.3, 0.7, 1]},\n",
       "                   random_state=42, scoring=make_scorer(get_rouge_score),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=samples_to_tune_on[\"dialogue\"], y=samples_to_tune_on[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.7,\n",
       " 'repetition_penalty': 0.7,\n",
       " 'num_beams': 2,\n",
       " 'no_repeat_ngram_size': 2,\n",
       " 'min_new_tokens': 5,\n",
       " 'max_new_tokens': 100,\n",
       " 'epsilon_cutoff': 0.0006}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['13817023',\n",
       "  '13716628',\n",
       "  '13829420',\n",
       "  '13819648',\n",
       "  '13728448',\n",
       "  '13814197',\n",
       "  '13820419',\n",
       "  '13864382',\n",
       "  '13729454',\n",
       "  '13810148',\n",
       "  '13862537',\n",
       "  '13862991',\n",
       "  '13682191',\n",
       "  '13864649',\n",
       "  '13730658',\n",
       "  '13819903',\n",
       "  '13729688',\n",
       "  '13727903',\n",
       "  '13810095',\n",
       "  '13829676'],\n",
       " 'dialogue': [\"A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\",\n",
       "  'Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\\r\\nRob: I used to get one every year as a child! Loved them! \\r\\nEmma: Yeah, i remember! they were filled with chocolates!\\r\\nLauren: they are different these days! much more sophisticated! Haha!\\r\\nRob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\\r\\nEmma: what do you fit inside?\\r\\nLauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\\r\\nEmma: WOW! That’s brill! X\\r\\nLauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\\r\\nRob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\\r\\nLauren: i reckon it prepares them for Christmas \\r\\nEmma: and makes it more about traditions and being kind to other people\\r\\nLauren: my children get very excited every time they get one!\\r\\nEmma: i can see why! :)',\n",
       "  \"Jackie: Madison is pregnant\\r\\nJackie: but she doesn't wanna talk about it\\r\\nIggy: why\\r\\nJackie: I don't know why because she doesn't wanna talk about it\\r\\nIggy: ok\\r\\nJackie: I wanted to prepare you for it because people get super excited and ask lots of questions\\r\\nJackie: and she looked way more anxious than excited\\r\\nIggy: she's probably worrying about it\\r\\nIggy: she's taking every commitment really seriously\\r\\nJackie: it could be money problems or relationship problems\\r\\nIggy: or maybe she wants an abortion\\r\\nJackie: it could be all of the above\\r\\nIggy: but you know what?\\r\\nIggy: once my friend was pregnant and I couldn't bring myself to be happy about it\\r\\nJackie: why?\\r\\nIggy: I felt they were immature and I couldn't picture this couple as parents\\r\\nJackie: I felt similar way on Patricia's wedding\\r\\nIggy: Patricia Stevens?\\r\\nJackie: yes\\r\\nIggy: so we're talking about the same person\\r\\nJackie: what a coincidence\\r\\nJackie: so she's pregnant?\\r\\nIggy: she thought she was\\r\\nJackie: damn...\",\n",
       "  \"Marla: <file_photo>\\r\\nMarla: look what I found under my bed\\r\\nKiki: lol\\r\\nTamara: is that someone's underwear?\\r\\nMarla: it certainly isn't mine, my ass is big but it isn't huge\\r\\nKiki: it looks like male underwear\\r\\nTamara: not necessarily, maybe some butch had fun in your room while you were gone\\r\\nMarla: ok but how can you leave your underwear after hooking up? wtf is wrong with people\\r\\nKiki: she or he could be too wasted to notice\\r\\nTamara: or maybe someone put their pants there to piss you off\\r\\nMarla: that makes no sense\\r\\nMarla: it's so fucking childish\\r\\nKiki: if it's childish then it must have been your sister's idea\\r\\nMarla: she's 13, she doesn't have underwear that isn't pink\\r\\nTamara: maybe it belonged to one of your exes?\\r\\nKiki: she would have recognized it\\r\\nMarla: lol we're doing total CSI investigation on one pair of boxers :D\\r\\nKiki: <file_gif>\\r\\nTamara: lol\\r\\nTamara: I think your sister convinced someone to put their underwear in your room as a dare\\r\\nMarla: sounds legit\\r\\nKiki: Tamara, you just cracked the case!\\r\\nTamara: <file_gif>\\r\\nTamara: always happy to help\",\n",
       "  'Robert: Hey give me the address of this music shop you mentioned before\\r\\nRobert: I have to buy guitar cable\\r\\nFred: <file_other>\\r\\nFred: Catch it on google maps\\r\\nRobert: thx m8\\r\\nFred: ur welcome',\n",
       "  \"Keith: Meg, pls buy some milk and cereals, I see now we've run out of them\\r\\nMegan: hm, sure, I can do that\\r\\nMegan: but did you check in the drawer next to the fridge?\\r\\nKeith: nope, let me have a look\\r\\nKeith: ok, false alarm, we have cereal and milk :D\\r\\nMegan: <file_gif>\",\n",
       "  \"Samantha: <file_video>\\r\\nEvelyn: LOL\\r\\nHolly: Is SHE making that noise??\\r\\nSamatha: Yes (＾▽＾)\\r\\nHolly: How possible?? :o\\r\\nSamantha: Idk, I'm also surprised!!\\r\\nEvelyn: xD\",\n",
       "  \"Theresa: have you been at Tom's new place?\\nLuis: yes, it's nice\\nMarion: He invited us for a dinner\\nAdam: where is it?\\nMarion: a bit outside the city\\nAdam: where exactly?\\nMarion: Fiesole\\nLuis: very nice!\",\n",
       "  \"Jane: Hello\\r\\nVegano Resto: Hello, how may I help you today?\\r\\nJane: I would like to make a reservation.\\r\\nJane: For 6 people, tonight around 20:00\\r\\nVegano Resto: Let me just check.\\r\\nVegano Resto: Ah, I'm afraid that there is no room at 20:00.\\r\\nVegano Resto: However, I could offer you a table for six at 18:30 or at 21:00\\r\\nVegano Resto: Would either of those times suit you?\\r\\nJane: Oh dear.\\r\\nJane: Let me just ask my friends.\\r\\nVegano Resto: No problem.\\r\\nJane: 21:00 will be ok.\\r\\nVegano Resto: Perfect. So tonight at 21:00 for six people under your name.\\r\\nJane: great, thank you!\",\n",
       "  'Nancy: Howdy, how y\\'all doin\\'?\\r\\nTina: Is that a Texan drawl, girl?\\r\\nNancy: Yes ma\\'am! Loving it out here!\\r\\nTina: How\\'s the job going? Kids behaving themselves?\\r\\nNancy: Mostly! They laugh at my accent though!\\r\\nTina: Well, they probably haven\\'t met a Welsh person before!\\r\\nNancy: No shit! They ask me to repeat everything! Best one is \"Water\", course, it\\'s mostly \"Waarderr\" here! \\r\\nTina: LOL. I\\'d love to hear that, you picked up the accent yet?\\r\\nNancy: Nah, 21 years in Cardiff isn\\'t easily removed! \\r\\nTina: We\\'re missing you here, the pub is quiet these days without your laugh!\\r\\nNancy: Miss you too! I\\'m coming home in 6 weeks, though. Last fortnight I\\'m going travelling with 3 other Brits working here, a Geordie girl, a guy from Belfast and Annie, who\\'s from Glasgow.\\r\\nTina: My God, I\\'m so jealous! I bet they had even more trouble being understood out there! See you after your trip!',\n",
       "  \"Laura: I need a new printer :/\\nLaura: thinking about this one\\nLaura: <file_other>\\nJamie: you're sure you need a new one?\\nJamie: I mean you can buy a second hand one\\nLaura: could be\",\n",
       "  \"Barbara: got everything?\\nHaylee: yeah almost\\nHaylee: i'm in dairy section\\nHaylee: but can't find this youghurt u wanted\\nBarbara: the coconut milk one?\\nHaylee: yeah\\nBarbara: hmmm yeah that's a mystery. cause it's not dairy but it's yoghurt xD\\nHaylee: exactly xD  \\nHaylee: ok i asked sb. they put it next to eggs lol\\nBarbara: lol\",\n",
       "  'Norbert: we need to hurry to catch the tour\\r\\nWendy: ok, am buying something. be right out!\\r\\nNorbert: ok. am not waiting long though. missed the last one because of you\\r\\nWendy: just be patient for once.\\r\\nNorbert: im always patient\\r\\nWendy: at the register now\\r\\nNorbert: alright',\n",
       "  \"Lidia: hi guys, how was your day?\\nCecil: amazing\\nLidia: where did you go?\\nCheryl: to the Jandia Peninsula\\nCheryl: sorry, Cecil is driving\\nLidia: and how was it?\\nCheryl: I liked it a lot\\nCheryl: Peter took very nice pics\\nPeter: <file_photo> <file_photo>\\nPeter: but it was very windy\\nLidia: yes, it's always windy here\\nPeter: really? Also in summer?\\nLidia: sure, the name Fuerteventura means strong wind\\nCheryl: wow, it's fascinating\\nLidia: so do you have any plans for tomorrow\\nCheryl: Cecil wants to explore more the south of the island\\nPeter: I'm just a passenger, so have no voice\\nCheryl: c'mon, it's not true\\nPeter: I'm just joking\\nCheryl: we will decide after dinner\\nCecil: ok, so let me know\\nCheryl: we will\",\n",
       "  'Nickola: Have you found it?\\r\\nSophie: No! Still looking :(\\r\\nNickola: Check pockets and handbags. \\r\\nSophie: Checked them all twice already... ',\n",
       "  'Rosie: What\\'s your favorite b-movie?\\r\\nElle: um, hard to say. Why do you ask?\\r\\nDennis: Toxic avenger for sure\\r\\nRosie: I have to write an essay and I chose bad movies as my topic and I\\'m just looking for inspiration\\r\\nElle: plan 9 from outer space is definitely something worth mentioning\\r\\nRosie: Yeah, I\\'ve seen it. And I will also cover \"The Room\". I\\'m just looking for something a bit more niche\\r\\nDennis: There\\'s Troma Studio for ya - toxic avenger, poultrygeist - the latter is exceptionally awful - and it\\'s a musical\\r\\nRosie: Is it one of those intentionally bad movies?\\r\\nDennis: most definitely\\r\\nRosie: ok, thank you, I\\'ll check it out\\r\\nElle: oh, there\\'s also jesus christ vampire hunter\\r\\nRosie: what? :D\\r\\nElle: it\\'s even worse than it sounds\\r\\nDennis: and when it comes to more recent movies there are those stupid animal-based horror movies like sharknado or zombeavers\\r\\nRosie: I\\'ve heard of sharknado and zombeavers sound just awesome\\r\\nRosie: thanks guys, you helped me a lot :)',\n",
       "  \"Julia: What is your biggest dream\\r\\nJulia: I mean the kind that can be achieved\\r\\nJames: Everyone say I have nice voice\\r\\nJames: My mom liked very much when I was reading outloud\\r\\nJames: I've had this dream for some time now, to become a voice actor\\r\\nJames: Be a part of cartoon or video game as a voice actor reading a character\\r\\nJulia: Wow. Nice one.\\r\\nJulia: Btw you do have a nice voice\\r\\nJulia: I could listen to you as a radio speaker.\\r\\nJames: Thanks\\r\\nJames: I've worked in radio, but it was during college so I had little time for this\\r\\nJulia: Shame.\\r\\nJames: I know. But nothing is lost. I still have microphone at home and with a bit of help I could make homemade radio station\\r\\nJulia: That's actually a great idea\\r\\nJulia: I cheer for you!\",\n",
       "  \"Poppy: I literally cannot think any more today!\\r\\nAlice: Yeah, I'm in the same shape. What a long day!\\r\\nPoppy: Lunch went by in a flash because I had errands, which makes the day so slow!\\r\\nAlice: I didn't get lunch, so that's even worse!\\r\\nPoppy: Oh, poor you! Aren't you starving?\\r\\nAlice: I'll live. Only three more hours!\\r\\nPoppy: LOL! Not that you're counting...\\r\\nAlice: Damn straight I'm counting! LOL!\\r\\nPoppy: Well, I'm def going for drinks after work. Want to join?\\r\\nAlice: Who else is there?\\r\\nPoppy: Nobody yet but I was going to put the word out.\\r\\nAlice: Sure, sounds fun. I'll invite some people up here if that's okay?\\r\\nPoppy: Oh, got your eye on anyone?\\r\\nAlice: Fred!\\r\\nPoppy: Fred? Really?\\r\\nAlice: Sure, why not? He's single, my age and not bad looking.\\r\\nPoppy: He's a dork!\\r\\nAlice: But a cute one!\\r\\nPoppy: If you say so. Not my type!\\r\\nAlice: That's a relief!\\r\\nPoppy: He's all yours!\\r\\nAlice: Good. So I'll invite him and a bunch of others. See you at Nick's?\\r\\nPoppy: Perfect. About 5:30.\\r\\nAlice: Great!\\r\\nPoppy: Can't wait! So ready for a beer!\\r\\nAlice: GnT for me!\",\n",
       "  \"Sash: need to see u\\r\\nCaron: y\\r\\nCaron: i'm out from 12\\r\\nSash: will be before\\r\\nSash: then\\r\\nCaron: k\\r\\nSash: open the door:\\r\\nCaron: what time u coming I need to go out\\r\\nSash: soon\\r\\nCaron: hurry up I need to go out\",\n",
       "  \"Giuseppe: Hi man\\r\\nMatteo: Yo\\r\\nGiuseppe: How's it going with Gosia?\\r\\nMatteo: I don't know, she's a little strange\\r\\nGiuseppe: Why?\\r\\nMatteo: She always criticizes me because I like football and video games\\r\\nGiuseppe: Damn\\r\\nMatteo: Yeah...\\r\\nGiuseppe: Ok, I don't like games either, but...\\r\\nMatteo: You boring guy\\r\\nGiuseppe: Lol\\r\\nMatteo: Anyway I like her a lot\\r\\nGiuseppe: I can understand that, she's hot, if you ever dump her make sure you tell me\\r\\nMatteo: Get your hands off her, man\\r\\nGiuseppe: Just kidding\\r\\nMatteo: Lollolol\"],\n",
       " 'summary': ['A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. ',\n",
       "  'Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.',\n",
       "  \"Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant. \",\n",
       "  'Marla found a pair of boxers under her bed.',\n",
       "  'Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.',\n",
       "  \"Megan needn't buy milk and cereals. They're in the drawer next to the fridge.\",\n",
       "  'Samantha and Evelyn after watching the video cannot believe she is able to make that noise.',\n",
       "  \"Tom's new place is in Fiesole. Luis and Marion has been there. \",\n",
       "  'Jane made a 9 PM reservation for 6 people tonight at Vegano Resto. ',\n",
       "  \"Nancy's working in Texas, but the kids laugh at her Welsh accent. She's coming home in 6 weeks. Earlier than that she's going to travel with 3 other Brits.\",\n",
       "  'Laura is going to buy a printer.',\n",
       "  \"Haylee can't find the coconut milk yoghurt.\",\n",
       "  'Wendy is shopping, but she needs to hurry up to catch the tour.',\n",
       "  'Cecil, Cheryl and Peter went to the Jandia Peninsula today. Cecil would like to explore the south of the island tomorrow, but they will decide what to do after dinner. ',\n",
       "  \"Sophie still hasn't found it despite checking pockets and handbags twice.\",\n",
       "  'Dennis and Elle are helping Rosie think of bad movies for her essay. ',\n",
       "  'James has a dream of becoming a voice actor. He considers making a home radio station.',\n",
       "  \"Poppy and Alice are meeting for drinks after work at Nick's at 5:30. Alice fancies Fred, she will invite him and a bunch of other coworkers.\",\n",
       "  \"Sash needs to see Caron who'll be out from 12. \",\n",
       "  'Matteo is not sure about his relationship with Gosia but likes her a lot. ']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_to_tune_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cbfde38e0342059cc8cacdb7b74e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m samples_to_tune_on \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_dict(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][:sample_size])\n\u001b[0;32m----> 3\u001b[0m metric, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples_to_tune_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_encoding_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_encoding_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(metric)\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mget_metrics\u001b[0;34m(model, prompt, dataset, tokenizer_encoding_kwargs, generation_kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     gts\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(f\"Model Output : {output}\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(f\"Label : {item['summary']}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# print(\"===\" * 10)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mget_rouge_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric, preds, gts\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mget_rouge_score\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rouge_score\u001b[39m(y_true : List, y_pred : List):\n\u001b[0;32m---> 19\u001b[0m     preds, gts \u001b[38;5;241m=\u001b[39m \u001b[43mpostprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     result \u001b[38;5;241m=\u001b[39m rouge_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mgts, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mpostprocess_text\u001b[0;34m(preds, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess_text\u001b[39m(preds, labels):\n\u001b[0;32m----> 2\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [pred\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m      3\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [label\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# rougeLSum expects newline after each sentence\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess_text\u001b[39m(preds, labels):\n\u001b[0;32m----> 2\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m      3\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [label\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# rougeLSum expects newline after each sentence\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"validation\"][:sample_size])\n",
    "metric, _, _ = get_metrics(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    dataset=samples_to_tune_on,\n",
    "    tokenizer_encoding_kwargs=tokenizer_encoding_kwargs,\n",
    "    generation_kwargs=clf.best_params_,\n",
    ")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
