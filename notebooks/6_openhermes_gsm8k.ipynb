{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clone of nb 10 - check inf of slm and benchmark on dataset running on run pod\n",
    "\n",
    "- copybara openhermes 2.5 model llmsearch on gsm8k\n",
    "\n",
    "install exllama\n",
    "wget https://github.com/turboderp/exllamav2/releases/download/v0.0.14/exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "pip install -q exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "TODO : inference for one of the samples not reproducible in llmsearch and raw inference even after setting seed, check\n",
    "looks like exllamv2 is not deterministic - https://github.com/turboderp/exllamav2/issues/232\n",
    "TODO : test awq, gptq, bnb inf time?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = \"cuda:0\"\n",
    "seed_everything(seed=seed)\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "\n",
    "class Exllamav2HF(PreTrainedModel):\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        if shared.args.cache_8bit:\n",
    "            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "        else:\n",
    "            self.ex_cache = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "        self.past_seq = None\n",
    "        if shared.args.cfg_cache:\n",
    "            if shared.args.cache_8bit:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "            else:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "            self.past_seq_negative = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        use_cache = kwargs.get('use_cache', True)\n",
    "        labels = kwargs.get('labels', None)\n",
    "        past_key_values = kwargs.get('past_key_values', None)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            if not shared.args.cfg_cache:\n",
    "                print(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n",
    "                return\n",
    "\n",
    "            input_ids = args[0]\n",
    "            is_negative = True\n",
    "            past_seq = self.past_seq_negative\n",
    "            ex_cache = self.ex_cache_negative\n",
    "        else:\n",
    "            input_ids = kwargs['input_ids']\n",
    "            is_negative = False\n",
    "            past_seq = self.past_seq\n",
    "            ex_cache = self.ex_cache\n",
    "\n",
    "        seq = input_ids[0].tolist()\n",
    "        if is_negative and past_key_values is not None:\n",
    "            seq = past_key_values + seq\n",
    "\n",
    "        seq_tensor = torch.tensor(seq)\n",
    "        reset = True\n",
    "\n",
    "        # Make the forward call\n",
    "        if labels is None:\n",
    "            if past_seq is not None:\n",
    "                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n",
    "                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n",
    "                if len(indices) > 0:\n",
    "                    longest_prefix = indices[0].item()\n",
    "                else:\n",
    "                    longest_prefix = min_length\n",
    "\n",
    "                if longest_prefix > 0:\n",
    "                    reset = False\n",
    "                    ex_cache.current_seq_len = longest_prefix\n",
    "                    if len(seq_tensor) - longest_prefix > 1:\n",
    "                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "                    elif len(seq_tensor) == longest_prefix:\n",
    "                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n",
    "                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n",
    "                        ex_cache.current_seq_len -= 1\n",
    "\n",
    "            if reset:\n",
    "                ex_cache.current_seq_len = 0\n",
    "                if len(seq_tensor) > 1:\n",
    "                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "\n",
    "            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n",
    "        else:\n",
    "            ex_cache.current_seq_len = 0\n",
    "            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n",
    "\n",
    "        if is_negative:\n",
    "            self.past_seq_negative = seq_tensor\n",
    "        else:\n",
    "            self.past_seq = seq_tensor\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, logits.shape[-1])\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HF(config)\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class Shared:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.gpu_split = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = Shared.Args()\n",
    "\n",
    "def setup_dataset(tokenizer):\n",
    "    valid_dataset_with_out = datasets.load_dataset(\"samsum\")['validation']\n",
    "    valid_dataset_with_out = DatasetWrapper(valid_dataset_with_out, tokenizer, input_key = \"dialogue\", output_key = \"summary\")\n",
    "\n",
    "    valid_dataset_without_out = datasets.load_dataset(\"samsum\")['validation']\n",
    "    valid_dataset_without_out = DatasetWrapper(valid_dataset_without_out, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "\n",
    "\n",
    "    test_dataset = datasets.load_dataset(\"samsum\")['test']\n",
    "    test_dataset = DatasetWrapper(test_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    test_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "\n",
    "    valid_dataset_with_out.apply_chat_template(add_gen_prompt=False)\n",
    "    # valid_dataset_with_out.tokenize_dataset()\n",
    "\n",
    "    valid_dataset_without_out.apply_chat_template(add_gen_prompt=True)\n",
    "\n",
    "    return valid_dataset_without_out, test_dataset\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, hf_dataset, tokenizer, prompt_template = \"Summarize : {dialogue}\", input_key = \"\", output_key = \"\", system_prompt = \"\", add_output = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x : {\"chat_format\" : ([{'role' : \"system\", \"content\" : system_prompt}] if system_prompt else []) + [\n",
    "            {\n",
    "                'role' : \"user\", \"content\" : prompt_template.format(**{input_key : x[input_key]})\n",
    "            }\n",
    "        ] + ([{'role' : 'assistant', \"content\" : x[output_key]}] if add_output else [])})\n",
    "\n",
    "    def apply_chat_template(self, add_gen_prompt = True):\n",
    "        \"\"\"Converts the dataset to a chat based format\"\"\"\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x: {\"formatted_chat\": self.tokenizer.apply_chat_template(x[\"chat_format\"], tokenize=False, add_generation_prompt=add_gen_prompt)})\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def perform_single_example_inference(example, model, gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True, use_aggregator=False)\n",
    "    return np.mean(result['rouge2'])\n",
    "\n",
    "def load_model(model_dir):\n",
    "    pass\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_dir = '/workspace/capybarahermes-2.5-gptq/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ/'\n",
    "model = Exllamav2HF.from_pretrained(pretrained_model_name_or_path = model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([SingleTokenStoppingCriteria(token_id=32000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "sampled_dataset = gsm8k_dataset['train'].shuffle(seed=42).select(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Answer - He has 6 - 2 = <<6-2=4>>4 cats.\n",
      "He has 4 - 1 = <<4-1=3>>3 parrots.\n",
      "He has 4 + 6 = <<4+6=10>>10 snakes.\n",
      "He has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\n",
      "#### 19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "text = \"\"\"\\\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Q: {question}\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "- decide which metric to use\n",
    "- add in evaluation for that metric\n",
    "- run dummy eval on a small set\n",
    "- then run search\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(text)\n",
    "\n",
    "idx = 3\n",
    "# sample = gsm8k_dataset['train'][idx]\n",
    "sample = sampled_dataset[1]\n",
    "question = sample['question']\n",
    "answer = sample['answer']\n",
    "\n",
    "formatted_pt = pt.format(question=question)\n",
    "\n",
    "# print(formatted_pt)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly assistant who can solve math problems\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": formatted_pt},\n",
    "]\n",
    "\n",
    "ct_sample = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n",
    "print(ct_sample)\n",
    "# print(question)\n",
    "print(f\"Answer - {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.model_utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mperform_single_example_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mperform_single_example_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# tokenized_input.to(device)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# print(tokenized_input)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt tokens - {prompt_tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# print(model_out.tolist()[0])\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces_between_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompletion_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_token_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completion Tokens - {completion_tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2805/3807337382.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "perform_single_example_inference??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 239\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\n",
      "\n",
      "We know that s = x + 6 (Frankie has six more snakes than cats).\n",
      "We also know that p = x - 1 (Frankie has one less parrot than cats).\n",
      "\n",
      "We know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\n",
      "\n",
      "Now, we can substitute the expressions for x, s, and p from the previous equations into this equation:\n",
      "(x - 1) + (x + 6) + 2 = 6\n",
      "\n",
      "Combine like terms:\n",
      "2x + 5 = 6\n",
      "\n",
      "Subtract 5 from both sides:\n",
      "2x = 1\n",
      "\n",
      "Divide by 2:\n",
      "x = 0.5\n",
      "\n",
      "Since the number of pets must be a whole number, this problem has no solution with positive integers for the number of cats, snakes, and parrots. It's possible that Frankie has a negative number of cats, snakes, or parrots, but that doesn't make sense in the real world. The problem might have a mistake or be impossible to solve with the given information.<|im_end|><|im_start|>assistant\n",
      "Apologies for the confusion in my previous response. Let's re-evaluate the problem.\n",
      "\n",
      "Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\n",
      "\n",
      "We know that s = x + 6 (Frankie has six more snakes than cats).\n",
      "We also know that p = x - 1 (Frankie has one less parrot than cats).\n",
      "\n",
      "We know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\n",
      "\n",
      "Now, we can substitute the expressions for x, s, and p from the previous equations into this equation:\n",
      "(x - 1) + (x + 6\n",
      "Completion Tokens - 500\n",
      "CPU times: user 6.91 s, sys: 6.87 ms, total: 6.92 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "seed_everything(seed = 42)\n",
    "transformers.set_seed(42)\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    # 'stopping_criteria' : stopping_criteria,\n",
    "    'do_sample' : False,\n",
    "    # 'use_cache' : False,\n",
    "}\n",
    "output = perform_single_example_inference(ct_sample, model, gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer_from_out(s):\n",
    "    pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "    match = pattern.search(s)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t['answer'].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "        print(y_t_answer, y_p_answer)\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores)/len(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 06:44:40.394 - llmsearch.tuner.tuner:82 - DEBUG - Initializing new estimator with generation parameters - {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tuner_ob = Tuner(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset = sampled_dataset,\n",
    "    device = 'cuda:0',\n",
    "    batch_size = 1,\n",
    "    tokenizer_encoding_kwargs={'padding': True, 'truncation': True, 'add_special_tokens' : False},\n",
    "    tokenizer_decoding_kwargs={'spaces_between_special_tokens' : False},\n",
    "    scorer = get_score,\n",
    "    prompt_template = pt,\n",
    "    is_encoder_decoder = False,\n",
    "    seed = seed,\n",
    "    column_mapping = {'input_cols' : [\"question\"],'eval_cols' : ['answer']},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       " 'answer': 'Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16',\n",
       " 'X': 'Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?',\n",
       " 'y': {'answer': 'Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\\nKyle has 24 x 2 = <<24*2=48>>48 sea shells.\\nLeigh has 48 / 3 = <<48/3=16>>16 sea shells.\\n#### 16'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_ob.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hf-internal-testing/llama-tokenizer': 2048}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 06:49:55.179 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.SingleTokenStoppingCriteria object at 0x7f9cd85ccca0>], 'generation_seed': 42, 'do_sample': False}\n",
      "2024-03-02 06:49:55.180 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 1\n",
      "2024-03-02 06:49:55.182 - llmsearch.utils.model_utils:97 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50878d39d4314a43815daba2bd644c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 06:50:07.352 - llmsearch.utils.model_utils:132 - DEBUG - Input - 'Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?'\n",
      "2024-03-02 06:50:07.353 - llmsearch.utils.model_utils:133 - DEBUG - Model Output - 'A: Mimi picked up 2 dozen seashells. A dozen is 12, so she picked up 2 x 12 = 24 seashells. Kyle found twice as many seashells as Mimi, so he found 2 x 24 = 48 seashells. Leigh grabbed one-third of the seashells that Kyle found, so she grabbed 48 / 3 = 16 seashells. The answer is 16.<|im_end|>'\n",
      "2024-03-02 06:50:07.354 - llmsearch.utils.model_utils:132 - DEBUG - Input - \"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?\"\n",
      "2024-03-02 06:50:07.356 - llmsearch.utils.model_utils:133 - DEBUG - Model Output - \"A: Frankie has 2 dogs, which is 2 x 4 = 8 legs. He has 6 pets with 4 legs, so he has 6 x 4 = 24 legs total from those pets. He has 6 snakes, so he has 6 x 0 = 0 legs (since snakes don't have legs). He has 1 less parrot than cats, so he has cats. He has cats, so he has legs from cats. He has 6 snakes, so he has 6 x 0 = 0 legs (since snakes don't have legs). He has 24 legs from his 6 pets with 4 legs, 0 legs from snakes, and legs from cats. So he has 24 + 0 + legs = 24 + 6 = 30 legs total. He has 2 dogs, 6 pets with 4 legs, and 1 parrot, so he has 2 + 6 + 1 = 9 pets that have legs. He has 30 legs total, so he has 30 / 4 = 7.5. Since he can't have half a pet, he must have 7 pets that have legs. He has 6 snakes, so he has 6 + 7 = 13 pets in total. The answer is 13.<|im_end|>\"\n",
      "2024-03-02 06:50:07.357 - llmsearch.utils.model_utils:132 - DEBUG - Input - 'Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?'\n",
      "2024-03-02 06:50:07.358 - llmsearch.utils.model_utils:133 - DEBUG - Model Output - \"A: Let's start with the uncle's gift. Auntie gave Olaf 6 toy cars, 1 more than the uncle. So the uncle gave Olaf 6 - 1 = 5 toy cars. Dad gave Olaf 10 toy cars, 5 less than Mum. So Mum gave Olaf 10 + 5 = 15 toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. So Grandpa gave Olaf 6 * 2 = 12 toy cars. Olaf originally had 150 toy cars. After receiving all these gifts, Olaf has 150 + 6 + 5 + 1 + 12 = 179 toy cars. The answer is 179.<|im_end|>\"\n",
      "2024-03-02 06:50:07.360 - llmsearch.utils.model_utils:132 - DEBUG - Input - \"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?\"\n",
      "2024-03-02 06:50:07.361 - llmsearch.utils.model_utils:133 - DEBUG - Model Output - 'A: Emma spends $8 a day for 7 days, so she spends 8 * 7 = 56 dollars. She has 100 - 56 = 44 dollars left in her account. The answer is 44.<|im_end|>'\n",
      "2024-03-02 06:50:07.362 - llmsearch.utils.model_utils:132 - DEBUG - Input - 'Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?'\n",
      "2024-03-02 06:50:07.363 - llmsearch.utils.model_utils:133 - DEBUG - Model Output - 'A: On the first day, Ezekiel hiked 10 kilometers. On the second day, he hiked half the full hike distance, which is 50 / 2 = 25 kilometers. So, on the first two days, he hiked 10 + 25 = 35 kilometers. The full hike is 50 kilometers. So, on the third day, he had to hike 50 - 35 = 15 kilometers. The answer is 15.<|im_end|>'\n",
      "2024-03-02 06:50:07.528 - llmsearch.utils.mem_utils:175 - DEBUG - Setting batch_size cache value - 1 for this particular configuration\n",
      "2024-03-02 06:50:07.529 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 12.347782 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n",
      "19 13\n",
      "196 179\n",
      "4 44\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "# select 100 examples\n",
    "# 100 examples\n",
    "# before - 0.47, after - 0.47\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "    'do_sample' : False,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A: Mimi picked up 2 dozen seashells. A dozen is 12, so she picked up 2 x 12 = 24 seashells. Kyle found twice as many seashells as Mimi, so he found 2 x 24 = 48 seashells. Leigh grabbed one-third of the seashells that Kyle found, so she grabbed 48 / 3 = 16 seashells. The answer is 16.<|im_end|>',\n",
       " \"A: Frankie has 2 dogs, which is 2 x 4 = 8 legs. He has 6 pets with 4 legs, so he has 6 x 4 = 24 legs total from those pets. He has 6 snakes, so he has 6 x 0 = 0 legs (since snakes don't have legs). He has 1 less parrot than cats, so he has cats. He has cats, so he has legs from cats. He has 6 snakes, so he has 6 x 0 = 0 legs (since snakes don't have legs). He has 24 legs from his 6 pets with 4 legs, 0 legs from snakes, and legs from cats. So he has 24 + 0 + legs = 24 + 6 = 30 legs total. He has 2 dogs, 6 pets with 4 legs, and 1 parrot, so he has 2 + 6 + 1 = 9 pets that have legs. He has 30 legs total, so he has 30 / 4 = 7.5. Since he can't have half a pet, he must have 7 pets that have legs. He has 6 snakes, so he has 6 + 7 = 13 pets in total. The answer is 13.<|im_end|>\",\n",
       " \"A: Let's start with the uncle's gift. Auntie gave Olaf 6 toy cars, 1 more than the uncle. So the uncle gave Olaf 6 - 1 = 5 toy cars. Dad gave Olaf 10 toy cars, 5 less than Mum. So Mum gave Olaf 10 + 5 = 15 toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. So Grandpa gave Olaf 6 * 2 = 12 toy cars. Olaf originally had 150 toy cars. After receiving all these gifts, Olaf has 150 + 6 + 5 + 1 + 12 = 179 toy cars. The answer is 179.<|im_end|>\",\n",
       " 'A: Emma spends $8 a day for 7 days, so she spends 8 * 7 = 56 dollars. She has 100 - 56 = 44 dollars left in her account. The answer is 44.<|im_end|>',\n",
       " 'A: On the first day, Ezekiel hiked 10 kilometers. On the second day, he hiked half the full hike distance, which is 50 / 2 = 25 kilometers. So, on the first two days, he hiked 10 + 25 = 35 kilometers. The full hike is 50 kilometers. So, on the third day, he had to hike 50 - 35 = 15 kilometers. The answer is 15.<|im_end|>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Answer - He has 6 - 2 = <<6-2=4>>4 cats.\n",
      "He has 4 - 1 = <<4-1=3>>3 parrots.\n",
      "He has 4 + 6 = <<4+6=10>>10 snakes.\n",
      "He has a total of 2 + 4 + 3 + 10 = <<2+4+3+10=19>>19 pets.\n",
      "#### 19\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 239\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\n",
      "\n",
      "We know that s = x + 6 (Frankie has six more snakes than cats).\n",
      "We also know that p = x - 1 (Frankie has one less parrot than cats).\n",
      "\n",
      "We know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\n",
      "\n",
      "Now, we can substitute the expressions for x, s, and p from the previous equations into this equation:\n",
      "\n",
      "x + (x + 6) + 2 = 6\n",
      "2x + 6 + 2 = 6\n",
      "2x + 8 = 6\n",
      "\n",
      "Now, we can solve for x:\n",
      "\n",
      "2x = 6 - 8\n",
      "2x = -2\n",
      "x = -1\n",
      "\n",
      "Since we can't have a negative number of cats, there must be an error in the problem statement. Please check the problem statement and provide the correct information.<|im_end|>\n",
      "Completion Tokens - 259\n",
      "CPU times: user 3.58 s, sys: 3.06 ms, total: 3.58 s\n",
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "    'do_sample' : False,\n",
    "}\n",
    "output = perform_single_example_inference(ct_sample, model, gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 239\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\n",
      "\n",
      "We know that s = x + 6 (Frankie has six more snakes than cats).\n",
      "We also know that p = x - 1 (Frankie has one less parrot than cats).\n",
      "\n",
      "We know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\n",
      "\n",
      "Now, we can substitute the expressions for x, s, and p from the first two equations into the third equation:\n",
      "\n",
      "x + (x + 6) + 2 = 6\n",
      "2x + 6 + 2 = 6\n",
      "2x + 8 = 6\n",
      "\n",
      "Now, we can solve for x:\n",
      "\n",
      "2x = 6 - 8\n",
      "2x = -2\n",
      "x = -1\n",
      "\n",
      "Since we can't have a negative number of cats, there must be an error in the problem statement. Please check the problem statement and provide the correct information.<|im_end|>\n",
      "Completion Tokens - 261\n",
      "CPU times: user 3.58 s, sys: 2.08 ms, total: 3.58 s\n",
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'generation_seed' : 42,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "#     'temperature' : 0.1\n",
    "#     'do_sample' : True,\n",
    "}\n",
    "output = perform_single_example_inference(ct_sample, model, gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Julie read 12 pages yesterday. Today, she read twice as many pages, which is 12 * 2 = 24 pages. So far, she has read 12 + 24 = 36 pages. There are 120 - 36 = 84 pages remaining. Julie wants to read half of the remaining pages tomorrow, so she should read 84 / 2 = 42 pages. The answer is 42.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_single_example_inference(example, model, gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = valid_dataset.hf_dataset[3]['formatted_chat']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
