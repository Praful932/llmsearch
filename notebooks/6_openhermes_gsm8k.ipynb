{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clone of nb 10 - check inf of slm and benchmark on dataset running on run pod\n",
    "\n",
    "- copybara openhermes 2.5 model llmsearch on gsm8k\n",
    "\n",
    "install exllama\n",
    "wget https://github.com/turboderp/exllamav2/releases/download/v0.0.14/exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "pip install -q exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = \"cuda:0\"\n",
    "seed_everything(seed=seed)\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "\n",
    "class Exllamav2HF(PreTrainedModel):\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        if shared.args.cache_8bit:\n",
    "            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "        else:\n",
    "            self.ex_cache = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "        self.past_seq = None\n",
    "        if shared.args.cfg_cache:\n",
    "            if shared.args.cache_8bit:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "            else:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "            self.past_seq_negative = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        use_cache = kwargs.get('use_cache', True)\n",
    "        labels = kwargs.get('labels', None)\n",
    "        past_key_values = kwargs.get('past_key_values', None)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            if not shared.args.cfg_cache:\n",
    "                print(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n",
    "                return\n",
    "\n",
    "            input_ids = args[0]\n",
    "            is_negative = True\n",
    "            past_seq = self.past_seq_negative\n",
    "            ex_cache = self.ex_cache_negative\n",
    "        else:\n",
    "            input_ids = kwargs['input_ids']\n",
    "            is_negative = False\n",
    "            past_seq = self.past_seq\n",
    "            ex_cache = self.ex_cache\n",
    "\n",
    "        seq = input_ids[0].tolist()\n",
    "        if is_negative and past_key_values is not None:\n",
    "            seq = past_key_values + seq\n",
    "\n",
    "        seq_tensor = torch.tensor(seq)\n",
    "        reset = True\n",
    "\n",
    "        # Make the forward call\n",
    "        if labels is None:\n",
    "            if past_seq is not None:\n",
    "                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n",
    "                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n",
    "                if len(indices) > 0:\n",
    "                    longest_prefix = indices[0].item()\n",
    "                else:\n",
    "                    longest_prefix = min_length\n",
    "\n",
    "                if longest_prefix > 0:\n",
    "                    reset = False\n",
    "                    ex_cache.current_seq_len = longest_prefix\n",
    "                    if len(seq_tensor) - longest_prefix > 1:\n",
    "                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "                    elif len(seq_tensor) == longest_prefix:\n",
    "                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n",
    "                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n",
    "                        ex_cache.current_seq_len -= 1\n",
    "\n",
    "            if reset:\n",
    "                ex_cache.current_seq_len = 0\n",
    "                if len(seq_tensor) > 1:\n",
    "                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "\n",
    "            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n",
    "        else:\n",
    "            ex_cache.current_seq_len = 0\n",
    "            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n",
    "\n",
    "        if is_negative:\n",
    "            self.past_seq_negative = seq_tensor\n",
    "        else:\n",
    "            self.past_seq = seq_tensor\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, logits.shape[-1])\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HF(config)\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class Shared:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.gpu_split = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = Shared.Args()\n",
    "\n",
    "def setup_dataset(tokenizer):\n",
    "    valid_dataset_with_out = datasets.load_dataset(\"samsum\")['validation']\n",
    "    valid_dataset_with_out = DatasetWrapper(valid_dataset_with_out, tokenizer, input_key = \"dialogue\", output_key = \"summary\")\n",
    "\n",
    "    valid_dataset_without_out = datasets.load_dataset(\"samsum\")['validation']\n",
    "    valid_dataset_without_out = DatasetWrapper(valid_dataset_without_out, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "\n",
    "\n",
    "    test_dataset = datasets.load_dataset(\"samsum\")['test']\n",
    "    test_dataset = DatasetWrapper(test_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    test_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "\n",
    "    valid_dataset_with_out.apply_chat_template(add_gen_prompt=False)\n",
    "    # valid_dataset_with_out.tokenize_dataset()\n",
    "\n",
    "    valid_dataset_without_out.apply_chat_template(add_gen_prompt=True)\n",
    "\n",
    "    return valid_dataset_without_out, test_dataset\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, hf_dataset, tokenizer, prompt_template = \"Summarize : {dialogue}\", input_key = \"\", output_key = \"\", system_prompt = \"\", add_output = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x : {\"chat_format\" : ([{'role' : \"system\", \"content\" : system_prompt}] if system_prompt else []) + [\n",
    "            {\n",
    "                'role' : \"user\", \"content\" : prompt_template.format(**{input_key : x[input_key]})\n",
    "            }\n",
    "        ] + ([{'role' : 'assistant', \"content\" : x[output_key]}] if add_output else [])})\n",
    "\n",
    "    def apply_chat_template(self, add_gen_prompt = True):\n",
    "        \"\"\"Converts the dataset to a chat based format\"\"\"\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x: {\"formatted_chat\": self.tokenizer.apply_chat_template(x[\"chat_format\"], tokenize=False, add_generation_prompt=add_gen_prompt)})\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def perform_single_example_inference(example, model, gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=y_true)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True, use_aggregator=False)\n",
    "    return np.mean(result['rouge2'])\n",
    "\n",
    "def load_model(model_dir):\n",
    "    pass\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_dir = '/workspace/capybarahermes-2.5-gptq/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ/'\n",
    "model = Exllamav2HF.from_pretrained(pretrained_model_name_or_path = model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d67e2cb240d47479ae6570d548d1732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.31M/2.31M [00:00<00:00, 6.41MB/s]\n",
      "Downloading data: 100%|██████████| 419k/419k [00:00<00:00, 427kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3fe0575c2b4afa85f6a8b6e30534e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1fd19cde1b4cb3a4afe9c1b658e2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([SingleTokenStoppingCriteria(token_id=32000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "text = \"\"\"\\\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Q: {question}\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "- decide which metric to use\n",
    "- add in evaluation for that metric\n",
    "- run dummy eval on a small set\n",
    "- then run search\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(text)\n",
    "\n",
    "idx = 3\n",
    "sample = gsm8k_dataset['train'][idx]\n",
    "\n",
    "question = sample['question']\n",
    "answer = sample['answer']\n",
    "\n",
    "formatted_pt = pt.format(question=question)\n",
    "\n",
    "print(formatted_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Answer - Maila read 12 x 2 = <<12*2=24>>24 pages today.\n",
      "So she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\n",
      "There are 120 - 36 = <<120-36=84>>84 pages left to be read.\n",
      "Since she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\n",
      "#### 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly assistant who can solve math problems\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": formatted_pt},\n",
    "]\n",
    "\n",
    "ct_sample = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n",
    "print(ct_sample)\n",
    "# print(question)\n",
    "print(f\"Answer - {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 240\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Julie read 12 pages yesterday. Today, she read twice as many pages, which is 12 * 2 = 24 pages. So far, she has read 12 + 24 = 36 pages. There are 120 - 36 = 84 pages remaining. Julie wants to read half of the remaining pages tomorrow, so she should read 84 / 2 = 42 pages. The answer is 42.<|im_end|>\n",
      "Completion Tokens - 104\n",
      "CPU times: user 2 s, sys: 0 ns, total: 2 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 200,\n",
    "    'generation_seed' : 42,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "#     'temperature' : 0.1\n",
    "#     'do_sample' : True,\n",
    "}\n",
    "output = perform_single_example_inference(ct_sample, model, gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Julie read 12 pages yesterday. Today, she read twice as many pages, which is 12 * 2 = 24 pages. So far, she has read 12 + 24 = 36 pages. There are 120 - 36 = 84 pages remaining. Julie wants to read half of the remaining pages tomorrow, so she should read 84 / 2 = 42 pages. The answer is 42.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_single_example_inference(example, model, gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = valid_dataset.hf_dataset[3]['formatted_chat']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
