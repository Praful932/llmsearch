{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Check how the model pads input - done\n",
    "Check how different enecoding & decoding params affect the encoding & decoding - done\n",
    "check performance of model on a sample set at different batch sizes (inadvertently check how the model performs when padding is present) - till now 1 bs gives best performance\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPTQConfig, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from llmsearch.model_downloader import download_model_from_hf\n",
    "from llmsearch.utils.model_utils import batcher, decoder_parser\n",
    "\n",
    "import awq\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0+cu121', '0.2.4')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')\n",
    "\n",
    "torch.__version__, awq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "def perform_single_example_inference(example, model, tokenizer,gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output, prompt_tokens, completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders\n",
    "\n",
    "class MultiTokenEOSCriteria(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\n",
    "\n",
    "    This code is not thread safe. The same object cannot be used simultaneously in multiple threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_ids : List[int],\n",
    "    ) -> None:\n",
    "        self.sequence_ids = torch.tensor(sequence_ids, dtype = torch.int32, device = \"cuda:0\")\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = self.sequence_ids.shape[0] + 2\n",
    "        self.state_initialized = False\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "    def set_state(self, batch_size, input_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_length = input_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.state_initialized = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_size = None\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "\n",
    "        ret_val = False\n",
    "\n",
    "        if not self.state_initialized:\n",
    "            # 1st call to __call__ for this batch\n",
    "            self.set_state(input_ids.shape[0], input_ids.shape[1])\n",
    "\n",
    "        # IDs of all the tokens except the prompt\n",
    "        lookback_ids_batch = input_ids[:, self.input_length :]\n",
    "        # look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        # no elements yet to look back\n",
    "        if lookback_ids_batch.nelement() == 0:\n",
    "            return False\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                # look back only as far as the last token of the stop sequence\n",
    "                self.done_tracker[i] = self.sequence_ids == lookback_ids_batch[i][-(self.sequence_ids.shape[0]):]\n",
    "        ret_val = False not in self.done_tracker\n",
    "        if ret_val:\n",
    "            # print(f\"finish, \", self.sequence_ids, lookback_ids_batch)\n",
    "            self.reset()\n",
    "        return ret_val\n",
    "\n",
    "\n",
    "def load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_kwargs,temp_model_dir, model_branch = \"main\"):\n",
    "    output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = model_branch)\n",
    "\n",
    "    model_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "    tokenizer_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "\n",
    "    model_name_or_path = model_loader_kwargs.pop('pretrained_model_name_or_path')\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=model_name_or_path,\n",
    "        **model_loader_kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # pad token is null in config -https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ/blob/eb64c310c44905321d012962db9ac0d47c3a64fa/tokenizer_config.json#L53\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_loader_backend_map = {\n",
    "    # \"exllama_2_hf\": load_model_with_exllama_2_hf_backend,\n",
    "    # \"hf\": load_model_with_hf_backend,\n",
    "    # 'auto_gptq' : load_model_with_autogptq_backend,\n",
    "    'awq' : load_model_with_awq_backend,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists in /workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ. Checking the model files...\n",
      "Checksum validated: model.safetensors  645dfc7f09074aaf25e642f3c6a4f7ea399a0ff2605fa650e4e74078832546de\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  6.48it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 50.63it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "\n",
    "temp_model_dir = Path(f\"/workspace/temp_model_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "model_loader_kwargs = {\n",
    "    'device_map' : {'' : 0},\n",
    "    'fuse_layers' : True,\n",
    "}\n",
    "\n",
    "tokenizer_loader_kwargs = {\n",
    "    'use_fast' : False,\n",
    "    'legacy' : False,\n",
    "    'padding_side' : 'left',\n",
    "}\n",
    "\n",
    "model, tokenizer = load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_loader_kwargs,temp_model_dir, model_branch = \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "# Add prompt template\n",
    "processed_dataset = preprocess_dataset(gsm8k_dataset['train'], tokenizer,pt = pt, pt_cols = pt_cols, system_prompt = system_prompt, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer_from_out(s):\n",
    "    pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "    match = pattern.search(s)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t['answer'].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug, set_verbosity_warning\n",
    "set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "bm_sample_size = 50\n",
    "bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:42:34.449 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac13d390>], 'generation_seed': 42}\n",
      "2024-03-31 09:42:34.450 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 1\n",
      "2024-03-31 09:42:34.451 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ba832f7b3a44369d6b84700e9402ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/50\n",
      "Batch 2/50\n",
      "Batch 3/50\n",
      "Batch 4/50\n",
      "Batch 5/50\n",
      "Batch 6/50\n",
      "Batch 7/50\n",
      "Batch 8/50\n",
      "Batch 9/50\n",
      "Batch 10/50\n",
      "Batch 11/50\n",
      "Batch 12/50\n",
      "Batch 13/50\n",
      "Batch 14/50\n",
      "Batch 15/50\n",
      "Batch 16/50\n",
      "Batch 17/50\n",
      "Batch 18/50\n",
      "Batch 19/50\n",
      "Batch 20/50\n",
      "Batch 21/50\n",
      "Batch 22/50\n",
      "Batch 23/50\n",
      "Batch 24/50\n",
      "Batch 25/50\n",
      "Batch 26/50\n",
      "Batch 27/50\n",
      "Batch 28/50\n",
      "Batch 29/50\n",
      "Batch 30/50\n",
      "Batch 31/50\n",
      "Batch 32/50\n",
      "Batch 33/50\n",
      "Batch 34/50\n",
      "Batch 35/50\n",
      "Batch 36/50\n",
      "Batch 37/50\n",
      "Batch 38/50\n",
      "Batch 39/50\n",
      "Batch 40/50\n",
      "Batch 41/50\n",
      "Batch 42/50\n",
      "Batch 43/50\n",
      "Batch 44/50\n",
      "Batch 45/50\n",
      "Batch 46/50\n",
      "Batch 47/50\n",
      "Batch 48/50\n",
      "Batch 49/50\n",
      "Batch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:45:19.481 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 165.030281 secs\n",
      "2024-03-31 09:45:19.646 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac13c490>], 'generation_seed': 42}\n",
      "2024-03-31 09:45:19.648 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 2\n",
      "2024-03-31 09:45:19.649 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258a3df6fac7476da85a21c5aa63053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/25\n",
      "Batch 2/25\n",
      "Batch 3/25\n",
      "Batch 4/25\n",
      "Batch 5/25\n",
      "Batch 6/25\n",
      "Batch 7/25\n",
      "Batch 8/25\n",
      "Batch 9/25\n",
      "Batch 10/25\n",
      "Batch 11/25\n",
      "Batch 12/25\n",
      "Batch 13/25\n",
      "Batch 14/25\n",
      "Batch 15/25\n",
      "Batch 16/25\n",
      "Batch 17/25\n",
      "Batch 18/25\n",
      "Batch 19/25\n",
      "Batch 20/25\n",
      "Batch 21/25\n",
      "Batch 22/25\n",
      "Batch 23/25\n",
      "Batch 24/25\n",
      "Batch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:47:09.802 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 110.153773 secs\n",
      "2024-03-31 09:47:09.972 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac0f2170>], 'generation_seed': 42}\n",
      "2024-03-31 09:47:09.974 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 4\n",
      "2024-03-31 09:47:09.974 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05fb12529cb4035b9b756f6b82e80b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/13\n",
      "Batch 2/13\n",
      "Batch 3/13\n",
      "Batch 4/13\n",
      "Batch 5/13\n",
      "Batch 6/13\n",
      "Batch 7/13\n",
      "Batch 8/13\n",
      "Batch 9/13\n",
      "Batch 10/13\n",
      "Batch 11/13\n",
      "Batch 12/13\n",
      "Batch 13/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:48:18.874 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 68.899233 secs\n",
      "2024-03-31 09:48:19.082 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac13cb80>], 'generation_seed': 42}\n",
      "2024-03-31 09:48:19.083 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 8\n",
      "2024-03-31 09:48:19.083 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c84a0f79674d1fa906a5d6992f18e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/7\n",
      "Batch 2/7\n",
      "Batch 3/7\n",
      "Batch 4/7\n",
      "Batch 5/7\n",
      "Batch 6/7\n",
      "Batch 7/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:48:56.765 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 37.682039 secs\n",
      "2024-03-31 09:48:56.960 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac0f1cf0>], 'generation_seed': 42}\n",
      "2024-03-31 09:48:56.961 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 16\n",
      "2024-03-31 09:48:56.962 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216fc48cb58d4f47a3295d13bb8a6604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/4\n",
      "Batch 2/4\n",
      "Batch 3/4\n",
      "Batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:49:24.041 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 27.078649 secs\n",
      "2024-03-31 09:49:24.217 - llmsearch.utils.mem_utils:153 - INFO - Starting inference with generation parameters - {'max_new_tokens': 500, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f10ac13c340>], 'generation_seed': 42}\n",
      "2024-03-31 09:49:24.217 - llmsearch.utils.mem_utils:157 - INFO - Performing inference with batch_size - 32\n",
      "2024-03-31 09:49:24.218 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a6852548304287986f4809e5254137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:49:27.279 - llmsearch.utils.mem_utils:191 - INFO - Unable to fit batch size - 32, Reducing batch size to - 16\n",
      "2024-03-31 09:49:27.461 - llmsearch.utils.model_utils:99 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae863a6447db4ded800b76741ec64880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/4\n",
      "Batch 2/4\n",
      "Batch 3/4\n",
      "Batch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 09:49:54.694 - llmsearch.utils.mem_utils:187 - INFO - Finished running inference, took 30.476015 secs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "# TODO : bs = 16 error, figure this out\n",
    "# TODO : run llmsearch\n",
    "\n",
    "class MultiTokenEOSCriteria2(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\n",
    "\n",
    "    This code is not thread safe. The same object cannot be used simultaneously in multiple threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_ids : List[int],\n",
    "    ) -> None:\n",
    "        self.sequence_ids = torch.tensor(sequence_ids, dtype = torch.int32, device = \"cuda:0\")\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = self.sequence_ids.shape[0] + 2\n",
    "        self.state_initialized = False\n",
    "        self.prompt_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "    def set_state(self, batch_size, prompt_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.prompt_length = prompt_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.state_initialized = True\n",
    "\n",
    "    def reset(self):\n",
    "        # print(\"Resetting\")\n",
    "        self.batch_size = None\n",
    "        self.prompt_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        \"\"\"\n",
    "        This is called after a new token is generated\n",
    "        \"\"\"\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "\n",
    "        ret_val = False\n",
    "\n",
    "        if not self.state_initialized:\n",
    "            # Every batch should set this state\n",
    "            # print(f\"Setting state, batch_size - {input_ids.shape[0]}, batch prompt length - {input_ids.shape[1] - 1}\")\n",
    "            self.set_state(input_ids.shape[0], input_ids.shape[1] - 1)\n",
    "\n",
    "        # IDs of all the tokens except the prompt\n",
    "        lookback_ids_batch = input_ids[:, self.prompt_length :]\n",
    "        # look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        # print(f\"Current input length - {input_ids.shape[1]}, completion length - {abs(self.prompt_length - input_ids.shape[1])}\")\n",
    "        # print(f\"Current input - {tokenizer.batch_decode(input_ids, **{'spaces_between_special_tokens' : False})}\")\n",
    "\n",
    "        # no elements yet to look back\n",
    "        if lookback_ids_batch.nelement() == 0:\n",
    "            return False\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                # look back only as far as the last token of the stop sequence\n",
    "                # print(len(self.done_tracker), lookback_ids_batch.shape, self.batch_size, self.prompt_length)\n",
    "                self.done_tracker[i] = self.sequence_ids == lookback_ids_batch[i][-(self.sequence_ids.shape[0]):]\n",
    "        ret_val = False not in self.done_tracker\n",
    "        if ret_val:\n",
    "            # ASSUMPTION: Relies on the assumption that generation will only stop when the stop token is generated\n",
    "            self.reset()\n",
    "        return ret_val\n",
    "\n",
    "\n",
    "batch_size_list = [1, 2, 4, 8, 16, 32]\n",
    "score_at_diff_batch_sizes = {}\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "\n",
    "    cm()\n",
    "\n",
    "    multi_token_stop_criteria_ob = MultiTokenEOSCriteria2(sequence_ids = [32000])\n",
    "    stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "\n",
    "    tuner_ob = Tuner(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        dataset = bm_samples,\n",
    "        device = 'cuda:0',\n",
    "        batch_size = batch_size,\n",
    "        tokenizer_encoding_kwargs={'padding': 'longest', 'add_special_tokens' : False},\n",
    "        tokenizer_decoding_kwargs={'spaces_between_special_tokens' : False},\n",
    "        scorer = get_score,\n",
    "        prompt_template = langchain.PromptTemplate.from_template(\"{X}\"),\n",
    "        is_encoder_decoder = False,\n",
    "        seed = seed,\n",
    "        column_mapping = {'input_cols' : [\"X\"],'eval_cols' : ['answer']},\n",
    "        callbacks_after_inference = [multi_token_stop_criteria_ob.reset],\n",
    "    )\n",
    "\n",
    "\n",
    "    # is stopping criteria cache not being reset properly?\n",
    "    # check if reset condition is working properly\n",
    "\n",
    "\n",
    "    gen_params1 = {\n",
    "        'max_new_tokens' : 500,\n",
    "        # max_new_tokens take precendece over stopping criteria\n",
    "        'stopping_criteria' : stopping_criteria,\n",
    "        'generation_seed' : 42,\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    scores, outputs = tuner_ob.get_score(gen_params1)\n",
    "    end = time.time()\n",
    "\n",
    "    score_at_diff_batch_sizes[batch_size] = {\n",
    "        'score' : scores,\n",
    "        'outputs' : outputs,\n",
    "        'optimal_batch_size' : tuner_ob.estimator._optimal_batch_size,\n",
    "        'latency_mins' : (end  - start) / 60,\n",
    "    }\n",
    "\n",
    "# Done - Problem - Stopping criteria does not reset when generation reaches max new tokens so no stop token is generated\n",
    "# When does generation stop - max new tokens is reached or stop token is generated\n",
    "\n",
    "# 2m44s for 50 samples\n",
    "# 100 samples - 5m28s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 1\n",
      "Score - 0.64\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 2\n",
      "Score - 0.5\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 4\n",
      "Score - 0.32\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 8\n",
      "Score - 0.22\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 16\n",
      "Score - 0.1\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 32\n",
      "Score - 0.1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_size in score_at_diff_batch_sizes.keys():\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "    print(f\"Score - {score_at_diff_batch_sizes[batch_size]['score']}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(bm_samples[0]['X'], **{'padding': 'longest', 'add_special_tokens' : False})['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells. Kyle found twice as many shells as Mimi, so he found 24 * 2 = 48 seashells. Leigh grabbed one-third of the shells that Kyle found, so Leigh had 48 / 3 = 16 seashells. The answer is 16.<|im_end|>',\n",
       " \"A: Let's break down the information given:\\n\\n1. Frankie has 6 more snakes than cats. Let's represent the number of cats as C. So, the number of snakes is C + 6.\\n2. Frankie has one less parrot than cats. So, the number of parrots is C - 1.\\n3. Six of his pets have four legs, which are the dogs.\\n4. He has 2 dogs.\\n\\nNow, let's find the total number of pets:\\n\\n1. C (cats) + (C + 6) (snakes) + (C - 1) (parrots) + 2 (dogs) + 6 (four-legged pets)\\n2. C + C + 6 + C - 1 + 6 + 2\\n3. 4C + 5\\n\\nSince we know that Frankie has 2 dogs, we can substitute 2 for 6 in the equation:\\n\\n4C + 5 = total number of pets\\n4C + 5 = C + 6 + (C - 1) + 2\\n\\nNow, let's solve for C:\\n\\n4C + 5 = 2C + 5 + C + 6 + 2\\n4C - 2C = 13\\n2C = 13\\nC = 13/2\\nC = 6.5\\n\\nHowever, since we can't have a fraction of a pet, we made an assumption that not all pets were accounted for in the given information. In this case, we can conclude that Frankie has at least 6 cats, which means he has 6 + 6 = 12 pets in total (6 snakes and 1 parrot). But remember, this is a minimum number, and there could be more pets. The answer is at least 12.<|im_end|>\",\n",
       " \"A: Let's break down the information given:\\n\\n1. Olaf initially had 150 toy cars.\\n2. Grandpa gave Olaf twice as many as Uncle, so Grandpa gave 2 * Uncle's cars.\\n3. Dad gave Olaf 10 cars, 5 less than Mum, so Mum gave Olaf 10 + 5 = 15 cars.\\n4. Auntie gave Olaf 6 cars, 1 more than Uncle, so Uncle gave Olaf 6 - 1 = 5 cars.\\n\\nNow let's calculate how many cars each family member gave:\\n\\n- Grandpa: 2 * Uncle's cars\\n- Dad: 10 cars\\n- Mum: 15 cars\\n- Auntie: 6 cars\\n\\nUncle's cars = 5 cars (as mentioned in the question)\\n\\nSo, Grandpa gave 2 * 5 = 10 cars.\\n\\nNow let's add all the cars given by the family members:\\n\\n10 (Grandpa) + 10 (Dad) + 15 (Mum) + 6 (Auntie) = 41 cars\\n\\nFinally, let's add the initial 150 cars to the 41 cars given by the family members:\\n\\n150 (initial) + 41 (from family) = 191 cars\\n\\nThe answer is 191.<|im_end|>\",\n",
       " 'A: Emma starts with $100 in her account. She spends $8 each day for a week, which is 7 days. So she spends 8 * 7 = $56 during the week. She has $100 - $56 = $44 left in her account. She asks for as many $5 bills as her account can give her, which is $44 / $5 = 8 $5 bills. So she takes out 8 * $5 = $40 from her account. She leaves the remaining $44 - $40 = $4 in her account. The answer is $4.<|im_end|>',\n",
       " 'A: On the first day, Ezekiel hiked 10 kilometers. On the second day, he covered half the full hike distance, which is 50 kilometers / 2 = 25 kilometers. So, for the first two days, he hiked 10 + 25 = 35 kilometers. To finish the hike, he had to hike 50 kilometers in total. Therefore, on the third day, he had to hike 50 - 35 = 15 kilometers. The answer is 15.<|im_end|>',\n",
       " 'A: On the first day, James collects 50 tins. On the second day, he collects 3 times that number, which is 3 * 50 = 150 tins. On the third day, he collects 50 tins fewer than the second day, which is 150 - 50 = 100 tins.\\n\\nSo far, he has collected 50 + 150 + 100 = 300 tins in the first three days.\\n\\nHe wants to collect 500 tins in total, so he needs to collect 500 - 300 = 200 tins in the remaining days of the week.\\n\\nSince he collects an equal number of tins each day for the rest of the week, he collects 200 tins / 5 days = 40 tins per day. The answer is 40.<|im_end|>',\n",
       " \"A: The bullseye is worth 50 points. The third dart is worth half the points of the bullseye, so it's worth 50 / 2 = 25 points. The second dart received no points. Adding up the scores, we have 50 + 25 + 0 = 75. The answer is 75.<|im_end|>\",\n",
       " 'A: First, we need to find the total width of the splashes made by each type of rock.\\n\\nFor pebbles, there are 6 pebbles and each pebble makes a splash 1/4 meter wide. So, the total width of the pebble splashes is 6 * (1/4) = 6/4 = 1.5 meters.\\n\\nFor rocks, there are 3 rocks and each rock makes a splash 1/2 meter wide. So, the total width of the rock splashes is 3 * (1/2) = 3/2 = 1.5 meters.\\n\\nFor boulders, there are 2 boulders and each boulder makes a splash 2 meters wide. So, the total width of the boulder splashes is 2 * 2 = 4 meters.\\n\\nNow, we add up the total width of all the splashes: 1.5 (pebbles) + 1.5 (rocks) + 4 (boulders) = 1.5 + 1.5 + 4 = 7 meters.\\n\\nThe answer is 7.<|im_end|>',\n",
       " 'A: Pauly has 3 dozen eggs, which is 3 * 12 = 36 eggs. Since there are 3 people and each omelet requires 4 eggs, Pauly can make 36 / 4 = 9 omelets. Each person gets 9 / 3 = 3 omelets. The answer is 3.<|im_end|>',\n",
       " 'A: The first stack has 7 blocks. The second stack is 3 blocks taller, so it has 7 + 3 = 10 blocks. The third stack is 6 blocks shorter, so it has 10 - 6 = 4 blocks. The fourth stack is 10 blocks taller, so it has 4 + 10 = 14 blocks. The fifth stack has twice as many blocks as the second stack, so it has 10 * 2 = 20 blocks. In total, Thomas used 7 + 10 + 4 + 14 + 20 = 55 blocks. The answer is 55.<|im_end|>',\n",
       " \"A: Gordon has two shopping bags, one weighing 3 pounds and the other weighing 7 pounds. The total weight of Gordon's bags is 3 + 7 = 10 pounds. Trace has five shopping bags, all weighing the same as Gordon's two bags combined. So, the weight of one of Trace's bags is 10 pounds / 5 bags = 2 pounds. The answer is 2.<|im_end|>\",\n",
       " \"A: Let's denote the total number of books as x. We know that 35% of the books are for children, so 65% of the books are for adults (since 100% - 35% = 65%). We are also given that 104 books are for adults. So, we can set up the equation: 0.65x = 104. To find x, we can divide both sides by 0.65: x = 104 / 0.65 = 160. The answer is 160.<|im_end|>\",\n",
       " 'A: Nicky trades two cards worth $8 each, so the total value of the cards he trades is 2 * $8 = $16. He receives a card worth $21 in return. So, his profit is the difference between the value of the card he received and the value of the cards he traded, which is $21 - $16 = $5. The answer is $5.<|im_end|>',\n",
       " 'A: First, we need to find the total number of seconds of recorded video. Patricia has 130 + 145 + 85 + 60 + 180 = 600 seconds of video.\\n\\nTo find out how many minutes each player gets on average, we need to divide the total seconds by the number of players (which is 5). So, 600 seconds / 5 players = 120 seconds per player.\\n\\nSince there are 60 seconds in a minute, we can convert 120 seconds to minutes by dividing by 60: 120 / 60 = 2 minutes per player. The answer is 2.<|im_end|>',\n",
       " 'A: First, we need to find the total cost of the meal. Julie orders $10, Letitia orders $20, and Anton orders $30. So the total cost is $10 + $20 + $30 = $60.\\n\\nThey want to split a 20% tip evenly, which means each person will pay 1/3 of the total tip. To find the tip amount, we calculate 20% of $60: 0.20 * $60 = $12.\\n\\nNow, we divide the tip amount by 3 to find how much each person should pay: $12 / 3 = $4.\\n\\nThe answer is each person should pay $4 in tips.<|im_end|>',\n",
       " \"First, let's find out how much Niko spent on the 9 pairs of socks: 9 pairs * $2 per pair = $18.\\n\\nFor the 4 pairs of socks where he wants a 25% profit, let's find out how much profit he wants to make: $18 * 0.25 = $4.5.\\n\\nNow, let's find out the original price of these 4 pairs of socks: $18 - $4.5 = $13.5.\\n\\nTo make a 25% profit, he will sell these 4 pairs for 1.25 times the original price: $13.5 * 1.25 = $16.875.\\n\\nFor the other 5 pairs of socks, he wants to make an extra $0.2 per pair. So, the total profit for these 5 pairs will be: 5 pairs * $0.2 = $1.\\n\\nNow, let's find out the selling price for each of these 5 pairs: $2 original price + $0.2 = $2.2.\\n\\nNow, let's calculate the total profit from these 5 pairs: 5 pairs * $2.2 - $1 = $10.\\n\\nFinally, let's find out the total profit from selling all 9 pairs: $16.875 (from 4 pairs) + $10 (from 5 pairs) = $26.875.\\n\\nThe answer is $26.875.<|im_end|>\",\n",
       " 'A: The number of short students is 2/5 of the total number of students, which is 2/5 * 400 = 160 students.\\nThe total number of students with average height is the difference between the total number of students and the number of tall students, which is 400 - 90 = 310 students. The answer is 310.<|im_end|>',\n",
       " 'A: James bought 20 coins at $15 each, so he spent 20 * $15 = $300. The value of the coins increased by 2/3, so the new value per coin is $15 + ($15 * 2/3) = $15 + $10 = $25. To recoup his original investment, he needs to sell coins worth $300. So, he sold $300 / $25 = 12 coins. The answer is 12.<|im_end|>',\n",
       " 'A: First, we need to find the number of each type of flower Lorelei picks.\\n\\nFor red roses, 50% of 12 is (50/100) * 12 = 6 red roses.\\nFor pink roses, 50% of 18 is (50/100) * 18 = 9 pink roses.\\nFor yellow roses, 25% of 20 is (25/100) * 20 = 5 yellow roses.\\nFor orange roses, 25% of 8 is (25/100) * 8 = 2 orange roses.\\n\\nNow, we add the number of roses of each color: 6 red + 9 pink + 5 yellow + 2 orange = 22 roses.\\n\\nThe answer is 22.<|im_end|>',\n",
       " 'A: Tonya made $50 in total. She sold $11 worth of small lemonades and $24 worth of medium lemonades. So, the revenue from small and medium lemonades is $11 + $24 = $35.\\n\\nSince large lemonades are sold at $3 each, to find out how many large lemonades she sold, we need to find out how much revenue she made from large lemonades. So, the revenue from large lemonades is $50 - $35 = $15.\\n\\nSince each large lemonade is sold at $3, to find out how many large lemonades she sold, we divide the revenue from large lemonades by the price of a large lemonade: $15 / $3 = 5.\\n\\nThe answer is 5 large lemonades.<|im_end|>',\n",
       " 'A: Leo finished the first part in 25 minutes. The second part took him twice as long, so it took 25 * 2 = 50 minutes. The total time for the first two parts is 25 + 50 = 75 minutes. Since he finished his assignment in 2 hours, which is 2 * 60 = 120 minutes, the time he took for the third part is 120 - 75 = 45 minutes. The answer is 45.<|im_end|>',\n",
       " 'A: The cost of the movie tickets is $10.00 each, so for two people, it would be 2 * $10.00 = $20.00. The large popcorn & 2 drink combo meal costs $11.00. Each person gets a box of candy for $2.50 each, so for two people, it would be 2 * $2.50 = $5.00. To find the total cost, add all the costs together: $20.00 (tickets) + $11.00 (popcorn & drinks) + $5.00 (candy) = $36.00. The answer is $36.00.<|im_end|>',\n",
       " 'A: First, find the number of red beans: 572 * (1/4) = 143 red beans.\\nThen, subtract the red beans from the total: 572 - 143 = 429 beans remaining.\\nNext, find the number of white beans: 429 * (1/3) = 143 white beans.\\nThen, subtract the red and white beans from the total: 429 - 143 - 143 = 143 beans remaining.\\nFinally, these 143 beans are the green beans. The answer is 143.<|im_end|>',\n",
       " \"A: Let's use variables to represent the number of each type of model car: B for Buicks, F for Fords, and C for Chevys.\\n\\nWe are given the following information:\\n1. B + F + C = 301 (total model cars)\\n2. B = 4F (Jim has 4 times as many Buicks as Fords)\\n3. B = 3(2C) + 3 (Jim has 3 more than twice the number of Fords than Chevies)\\n\\nNow, we can use the second and third equations to replace B in the first equation:\\n\\n4F + F + C = 3(2C) + 3\\n\\nCombine like terms:\\n\\n5F + C = 6C + 3\\n\\nSubtract C from both sides:\\n\\n5F = 5C + 3\\n\\nNow, we can divide both sides by 5:\\n\\nF = C + 3/5\\n\\nSince we don't have enough information to find the exact number of Chevys, we can't find the exact number of Buicks. However, we can express the number of Buicks in terms of the number of Chevies. Using the second equation, B = 4F, and the expression for F, we get:\\n\\nB = 4(C + 3/5)\\n\\nThe answer is B = 4(C + 3/5).<|im_end|>\",\n",
       " 'A: Peter needs $5,000 and currently has $2,900. He needs to save an additional $5,000 - $2,900 = $2,100. He saves $700 each month. To find out how many months he needs to wait, divide the remaining amount by the monthly savings: $2,100 / $700 = 3 months. The answer is 3.<|im_end|>',\n",
       " 'A: Wendy lived for 80 years. She spent 25 years as an accountant and 15 years as an accounting manager. So, she spent a total of 25 + 15 = 40 years in an accounting-related job. To find the percentage, we can divide the number of years she spent in an accounting-related job by her total lifespan and multiply by 100: (40 / 80) * 100 = 50%. The answer is 50%.<|im_end|>',\n",
       " 'A: Last year, 200 people signed up and 40 did not show up. So, 200 - 40 = 160 people actually ran. This year, there will be twice as many runners, so 160 * 2 = 320 people will run. The answer is 320.<|im_end|>',\n",
       " \"A: Dulce's father has 8 orange trees, and each tree has 200 fruits. So there are 8 * 200 = 1600 oranges in total. Dulce picks 2/5 of the oranges from each tree, which is 2/5 * 200 = 80 oranges per tree. Since there are 8 trees, Dulce picks a total of 80 * 8 = 640 oranges. To find the remaining oranges, subtract the picked oranges from the total: 1600 - 640 = 960. The answer is 960.<|im_end|>\",\n",
       " \"A: Padma traded 2 of her valuable cards for 10 of Robert's cards, so that's 2 + 10 = 12 cards traded. Robert traded 8 of his cards for 15 of Padma's cards, so that's 8 + 15 = 23 cards traded. In total, they traded 12 + 23 = 35 cards between both Padma and Robert. The answer is 35.<|im_end|>\",\n",
       " 'A: First, we need to determine how many floors have gates. Since there are 12 floors and every 3rd floor has a gate, there are 12 / 3 = 4 floors with gates.\\n\\nNext, we calculate the time it takes to go through each gate. Since it takes 2 minutes per gate and there are 4 gates, it takes 2 * 4 = 8 minutes to go through all the gates.\\n\\nNow, we need to convert the time spent at the gates to seconds. Since there are 60 seconds in a minute, it takes 8 * 60 = 480 seconds at the gates.\\n\\nNow, we need to calculate the time it takes to drive between floors. Since there are 12 floors and each floor is 800 feet away, the total distance to drive is 12 * 800 = 9600 feet. Since drivers drive at 10 feet per second, it takes 9600 / 10 = 960 seconds to drive between all floors.\\n\\nFinally, we add the time spent at the gates and the time spent driving between floors: 480 seconds (gates) + 960 seconds (driving) = 1440 seconds. The answer is 1440 seconds.<|im_end|>',\n",
       " 'A: First, find the number of deer with 8 antlers: 920 * 10% = 92 deer. Then, find the number of those deer that also have albino fur: 92 * 25% = 23 albino 8-antlered deer. The answer is 23.<|im_end|>',\n",
       " 'A: Allison bought 8 more glue sticks than Marie, so she bought 15 + 8 = 23 glue sticks. Marie bought 30 packs of construction paper. So, Allison bought 23 glue sticks + 30 packs of construction paper = 53 craft supply items. The answer is 53.<|im_end|>',\n",
       " 'A: The game promises 100 hours of gameplay, but 80% of that is boring grinding. So, 80% of 100 hours is 0.8 * 100 = 80 hours of boring grinding. The expansion adds another 30 hours of enjoyable gameplay. So, to find out how much enjoyable gameplay James gets, we subtract the boring grinding hours from the total gameplay hours: 100 - 80 = 20 hours. The answer is 20.<|im_end|>',\n",
       " \"First, let's find out how much money Monica and Michelle have together: Monica brings $15 and Michelle brings $12, so they have $15 + $12 = $27.\\n\\nNext, let's find out how many bottles of soda they can buy. Each bottle costs $3, so they can buy $27 / $3 = 9 bottles of soda.\\n\\nNow, let's find out how many servings of soda they have in total: each bottle has 12 servings, and they bought 9 bottles, so they have 9 * 12 = 108 servings of soda.\\n\\nFinally, let's find out how many servings each guest gets: there are 8 guests and 108 servings, so each guest gets 108 / 8 = 13.5 servings. Since we can't have half a serving, they will divide the remaining servings equally. So, each guest gets an additional 0.5 servings, making it 13.5 + 0.5 = 14 servings per guest. The answer is 14.<|im_end|>\",\n",
       " 'A: Peter spends $3 for each small glass and buys 8 small glasses, so he spends 8 * $3 = $24 on small glasses. He has $50 initially and receives $1 in change, so he spent $50 - $1 = $49. Since he spent $24 on small glasses, he spent the remaining $49 - $24 = $25 on large glasses. Each large glass costs $5, so he bought $25 / $5 = 5 large glasses. The answer is 5.<|im_end|>',\n",
       " 'A: Mara has 40 crayons and 10 percent of them are pink, so she has 40 * 0.10 = 4 pink crayons. Luna has 50 crayons and 20 percent of them are pink, so she has 50 * 0.20 = 10 pink crayons. In total, Mara and Luna have 4 + 10 = 14 pink crayons. The answer is 14.<|im_end|>',\n",
       " 'A: First, we need to find the monthly payments for both the house and the trailer.\\n\\nFor the house, the total cost is $480,000 and the loan term is 20 years (240 months). So, the monthly payment for the house is $480,000 / 240 = $2,000.\\n\\nFor the trailer, the total cost is $120,000 and the loan term is also 20 years (240 months). So, the monthly payment for the trailer is $120,000 / 240 = $500.\\n\\nNow, we can find the difference in monthly payments: $2,000 (house) - $500 (trailer) = $1,500.\\n\\nThe answer is $1,500 more in monthly payments for the house compared to the trailer.<|im_end|>',\n",
       " \"A: Mia spent $30 on each of her 3 siblings, which totals to 30 * 3 = $90. She spent a total of $150, so the amount she spent on her parents' gifts is 150 - 90 = $60. Since each of her parents received gifts of equal value, she spent 60 / 2 = $30 on each parent's gift. The answer is 30.<|im_end|>\",\n",
       " 'A: Clare had $47 initially. She bought 4 loaves of bread, which cost 4 * $2 = $8. She also bought 2 cartons of milk, which cost 2 * $2 = $4. So, the total cost of her purchases is $8 + $4 = $12. To find out how much money Clare has left, subtract the cost of her purchases from the initial amount: $47 - $12 = $35. The answer is $35.<|im_end|>',\n",
       " 'A: Oshea has 4 large planters, each holding 20 seeds, so the large planters can hold a total of 4 * 20 = 80 seeds. He has 200 - 80 = 120 seeds left to plant in small planters. Since each small planter can hold 4 seeds, he will need 120 / 4 = 30 small planters. The answer is 30.<|im_end|>',\n",
       " 'A: Eric originally had 150 colored pencils. Then 30 more pencils were added. So he has 150 + 30 = 180 pencils. He wants to evenly distribute these pencils into 5 containers. To find out how many pencils each container will have, we divide the total number of pencils by the number of containers: 180 / 5 = 36. The answer is 36.<|im_end|>',\n",
       " 'A: Oleg has 40 men, so half the number of men are women, which means there are 40 / 2 = 20 women. So far, there are 40 men + 20 women = 60 people accounted for. Since there are 80 guests in total, there must be 80 - 60 = 20 children. If Oleg adds 10 more children, there will be 20 + 10 = 30 children at the event. The answer is 30.<|im_end|>',\n",
       " 'A: Stella has 10 pages with 5 rows of 30 stamps each, which is 10 * 5 * 30 = 1500 stamps. The remaining pages (50 - 10 = 40) each have 50 stamps, which is 40 * 50 = 2000 stamps. In total, Stella has 1500 + 2000 = 3500 stamps. The answer is 3500.<|im_end|>',\n",
       " 'A: The first 100 channels cost $100. The next 100 channels cost half as much, which is $100/2 = $50. So, the total cost for all 200 channels is $100 + $50 = $150. Since James splits the cost evenly with his roommate, he pays half of the total cost, which is $150/2 = $75. The answer is $75.<|im_end|>',\n",
       " 'A: The sprinkler system uses 4 liters in the morning and 6 liters in the evening, for a total of 4 + 6 = 10 liters per day. To use 50 liters of water, it would take 50 / 10 = 5 days. The answer is 5.<|im_end|>',\n",
       " \"A: Mr. Langsley is picked up at 6:00 a.m. and it takes 40 minutes to reach the first station. So, he arrives at the first station at 6:00 a.m. + 40 minutes = 6:40 a.m.\\n\\nHe arrives at work at 9:00 a.m. The time difference between 6:40 a.m. and 9:00 a.m. is 2 hours and 20 minutes. Since there are 60 minutes in an hour, 2 hours is equal to 2 * 60 = 120 minutes. So, the total time taken from the first station to Mr. Langsley's workplace is 120 minutes (2 hours) + 20 minutes = 140 minutes. The answer is 140.<|im_end|>\",\n",
       " 'A: Janessa initially had 4 cards of her own and 13 from her father, which totals 4 + 13 = 17 cards. She then received 36 cards from eBay, making a total of 17 + 36 = 53 cards. After discarding the 4 bad cards, she had 53 - 4 = 49 cards. She gave Dexter 29 cards, so she kept 49 - 29 = 20 cards for herself. The answer is 20.<|im_end|>',\n",
       " 'A: John spends $850 and gets $40 in change. So, the total cost of the 3 barbells is $850 - $40 = $810. To find the cost of each barbell, divide the total cost by the number of barbells: $810 / 3 = $270. The answer is $270.<|im_end|>',\n",
       " 'A: Mike earned $160 in total wages. His second job pays $9 per hour and he works 12 hours, so he earned 9 * 12 = $108 from his second job. To find out how much he earned from his first job, subtract the earnings from his second job from the total wages: 160 - 108 = $52. The answer is 52.<|im_end|>',\n",
       " \"A: Wilson sleds down each of the 2 tall hills 4 times, so that's 2 * 4 = 8 times. He sleds down the tall hills 8 times, and he sleds down the small hills half as often, which is 8 / 2 = 4 times per small hill. There are 3 small hills, so he sleds down each small hill 4 times, which is 3 * 4 = 12 times. In total, he sleds down the hills 8 (tall hills) + 12 (small hills) = 20 times. The answer is 20.<|im_end|>\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_at_diff_batch_sizes[1]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 1\n",
      "Score - 0.64\n",
      "Latency - 2.7505770603815716 mins\n",
      "Optimal Batch Size - 1\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 2\n",
      "Score - 0.5\n",
      "Latency - 1.8359658439954123 mins\n",
      "Optimal Batch Size - 2\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 4\n",
      "Score - 0.32\n",
      "Latency - 1.1483861525853476 mins\n",
      "Optimal Batch Size - 4\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 8\n",
      "Score - 0.22\n",
      "Latency - 0.6280998746554057 mins\n",
      "Optimal Batch Size - 8\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 16\n",
      "Score - 0.1\n",
      "Latency - 0.45137432018915813 mins\n",
      "Optimal Batch Size - 16\n",
      "\n",
      "\n",
      "\n",
      "Batch Size - 32\n",
      "Score - 0.1\n",
      "Latency - 0.507991349697113 mins\n",
      "Optimal Batch Size - 32\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the results\n",
    "\n",
    "# proves that llmsearch should be run on a batch size of 1\n",
    "\n",
    "for batch_size, score_dict in score_at_diff_batch_sizes.items():\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "    print(f\"Score - {score_dict['score']}\")\n",
    "    print(f\"Latency - {score_dict['latency_mins']} mins\")\n",
    "    print(f\"Optimal Batch Size - {score_dict['optimal_batch_size']}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes[batch_size] = {\n",
    "        'score' : scores,\n",
    "        'outputs' : outputs,\n",
    "        'optimal_batch_size' : tuner_ob.estimator._optimal_batch_size,\n",
    "        'latency_mins' : (end  - start) / 60,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "cm()\n",
    "batch_size = 1\n",
    "\n",
    "tuner_ob = Tuner(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset = bm_samples,\n",
    "    device = 'cuda:0',\n",
    "    batch_size = batch_size,\n",
    "    tokenizer_encoding_kwargs={'padding': 'longest', 'add_special_tokens' : False},\n",
    "    tokenizer_decoding_kwargs={'spaces_between_special_tokens' : False},\n",
    "    scorer = get_score,\n",
    "    prompt_template = langchain.PromptTemplate.from_template(\"{X}\"),\n",
    "    is_encoder_decoder = False,\n",
    "    seed = seed,\n",
    "    column_mapping = {'input_cols' : [\"X\"],'eval_cols' : ['answer']},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob._optimal_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder parser is working as expected\n",
    "# TODO : check scores at different bs then llmsearch\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MultiTokenEOSCriteria(sequence_ids = [32000])])\n",
    "\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)\n",
    "\n",
    "scores_before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
