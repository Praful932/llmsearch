{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582b973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c80c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner.tuner import EstimatorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef83ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device - mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/praful932/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Autoreload\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "nltk.download('punkt')    \n",
    "\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "\n",
    "print(f\"Device - {device}\")\n",
    "\n",
    "def beep(duration, frequency=440, rhythm=1):\n",
    "    sample_rate = 44100  # Standard audio sample rate\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    audio_data = np.sin(2*np.pi*frequency*t)  # Generate a sine wave\n",
    "    audio_data *= np.where(np.arange(len(audio_data)) % rhythm == 0, 1, 0)  # Apply rhythm\n",
    "    display(Audio(audio_data, rate=sample_rate, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703176c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/praful932/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6a4d52cc4b4501b0c0490e5d1500d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"samsum\")\n",
    "\n",
    "sample_size = 100\n",
    "samples_to_tune_on = datasets.Dataset.from_dict(dataset[\"train\"][:sample_size])\n",
    "samples_to_tune_on = samples_to_tune_on.rename_columns(column_mapping = {'dialogue' : 'X', 'summary' : \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9916d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conversation: Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-) Summary:\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "X = samples_to_tune_on[0]['X']\n",
    "\n",
    "pt = langchain.PromptTemplate.from_template(\"Conversation: {X} Summary:\")\n",
    "\n",
    "pt.format_prompt(X = X).to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51795bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import get_total_available_ram, get_gpu_information\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "seed = 42\n",
    "\n",
    "\n",
    "tuner_ob = Tuner(model = None,tokenizer = tokenizer,dataset = samples_to_tune_on,device = \"cpu\", batch_size = 16, scorer = None, prompt_template = pt, is_encoder_decoder = False, seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29b0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "clone(tuner_ob.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68cb7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1afc7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Inspired from toma - https://github.com/BlackHC/toma\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def batch(func,*,batch_size = None,):\n",
    "    gc_cuda()\n",
    "\n",
    "    #\n",
    "#     if batch_size is None and 'batch_size' in kwargs:\n",
    "#         batch_size = kwargs['batch_size']\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Im in the decorator - {func.__name__}\")\n",
    "            return func(batch_size)\n",
    "        except RuntimeError as exception:\n",
    "            if batch_size > 1 and should_reduce_batch_size(exception):\n",
    "                batch_size //= 2\n",
    "                gc_cuda()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def gc_cuda():\n",
    "    \"\"\"Gargage collect Torch (CUDA) memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def should_reduce_batch_size(exception):\n",
    "    return is_cuda_out_of_memory(exception) or is_cudnn_snafu(exception) or is_out_of_cpu_memory(exception)\n",
    "\n",
    "def is_cuda_out_of_memory(exception):\n",
    "    return (\n",
    "        isinstance(exception, RuntimeError) and len(exception.args) == 1 and \"CUDA out of memory.\" in exception.args[0]\n",
    "    )\n",
    "\n",
    "def is_cudnn_snafu(exception):\n",
    "    # For/because of https://github.com/pytorch/pytorch/issues/4107\n",
    "    return (\n",
    "        isinstance(exception, RuntimeError)\n",
    "        and len(exception.args) == 1\n",
    "        and \"cuDNN error: CUDNN_STATUS_NOT_SUPPORTED.\" in exception.args[0]\n",
    "    )\n",
    "\n",
    "def is_out_of_cpu_memory(exception):\n",
    "    return (\n",
    "        isinstance(exception, RuntimeError)\n",
    "        and len(exception.args) == 1\n",
    "        and \"DefaultCPUAllocator: can't allocate memory\" in exception.args[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7082dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func - (2,), kwargs - {'b': 4}, batch_size - 4\n",
      "in function foo result - 6, batch_size - 4\n"
     ]
    }
   ],
   "source": [
    "def batch(func):\n",
    "    \"\"\"Run inference by deciding appropriate batch size on input\n",
    "    \"\"\"\n",
    "    def inner_wrapper(*args, batch_size, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"func - {args}, kwargs - {kwargs}, batch_size - {batch_size}\")\n",
    "                res = func(*args,batch_size = batch_size, **kwargs)\n",
    "                return res\n",
    "            except RuntimeError as exception:\n",
    "                raise\n",
    "                if batch_size > 1 and should_reduce_batch_size(exception):\n",
    "                    batch_size //= 2\n",
    "                    gc_cuda()\n",
    "                else:\n",
    "                    raise\n",
    "    \n",
    "    return inner_wrapper\n",
    "\n",
    "@batch\n",
    "def foo(a,b, batch_size = 8):\n",
    "    \"\"\"\n",
    "    a, b - batch of data\n",
    "    \n",
    "    default batch size - 8 will try to reduce by half if it doesn't fit in memory\n",
    "    \"\"\"\n",
    "    print(f\"in function foo result - {a+b}, batch_size - {batch_size}\")\n",
    "    \n",
    "foo(2, b = 4, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c1e29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im in the decorator - foo\n",
      "I'm in function foo - None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@batch\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfoo\u001b[39m(batch_size, a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm in function foo - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "@batch\n",
    "def foo(batch_size, a = 1, b = 2):\n",
    "    print(f\"I'm in function foo - {batch_size}\")\n",
    "    \n",
    "foo(batch_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69352f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
