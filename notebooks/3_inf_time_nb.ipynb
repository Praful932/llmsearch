{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', low_cpu_mem_usage: True, torch_dtype: 'torch.bfloat16'\n",
      "1.1706524221265413\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'auto'\n",
      "13.495654390917164\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.bfloat16'\n",
      "17.252136787429297\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'cuda:0'\n",
      "18.116759082387627\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.float16'\n",
      "20.071419678895033\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', revision: 'refs/pr/1'\n",
      "18.905335754174427\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True'\n",
      "15.528752866505773\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True', exllama_version: 'v2'\n",
      "16.090855255173437\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-8bit-32g-actorder_True'\n",
      "3.1711142028060886\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', low_cpu_mem_usage: True, use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "17.52417389720822\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "17.322899302948603\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'\n",
      "44.28048877512073\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: False, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "19.14424932230798\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0bpw-h6-exl2'\n",
      "37.620098225627714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('inference_time_tests.json', 'r') as file:\n",
    "    inf_time_tests = json.load(file)\n",
    "\n",
    "def dict_to_interpretable_string(d):\n",
    "    parts = []\n",
    "    for key, value in d.items():\n",
    "        # Convert the value to a string\n",
    "        value_str = str(value)\n",
    "        # Add quotes around string values for clarity\n",
    "        if isinstance(value, str):\n",
    "            value_str = f\"'{value_str}'\"\n",
    "        # Append the formatted key-value pair to the parts list\n",
    "        parts.append(f\"{key}: {value_str}\")\n",
    "    # Join all parts into a single string\n",
    "    return ', '.join(parts)\n",
    "\n",
    "df = {\n",
    "    'model_loading_params' : [],\n",
    "    'model_checkpoint' : [],\n",
    "    'gpu' : [],\n",
    "    'model_dtype' : [],\n",
    "    'model_device' : [],\n",
    "    'avg_token_per_sec' : [],\n",
    "    'model_loading_time' : [],\n",
    "    'total_gpu_mem_gb' : [],\n",
    "    'occupied_gpu_mem_gb' : [],\n",
    "    'total_mem_gb' : [],\n",
    "    'occupied_mem_gb' : [],\n",
    "}\n",
    "\n",
    "for item in inf_time_tests:\n",
    "    gpu = item['model_loading_params'].get('gpu', 't4_2x')\n",
    "    item['model_loading_params'].pop('gpu', None)\n",
    "    model_loading_params_str = dict_to_interpretable_string(item['model_loading_params'])\n",
    "\n",
    "    avg_tokens_per_sec = 0\n",
    "    for sample_metric in item['sample_metrics']:\n",
    "        avg_tokens_per_sec += sample_metric['output_tokens'] / (sample_metric['gen_latency'] / 1000)\n",
    "    avg_tokens_per_sec /= len(item['sample_metrics'])\n",
    "\n",
    "    print(model_loading_params_str)\n",
    "    print(avg_tokens_per_sec)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    df['model_loading_params'].append(model_loading_params_str)\n",
    "    df['model_checkpoint'].append(item['model_loading_params'].get('pretrained_model_name_or_path', item['model_loading_params'].get('model_name_or_path')))\n",
    "    df['gpu'].append(gpu)\n",
    "    df['avg_token_per_sec'].append(avg_tokens_per_sec)\n",
    "    avl_gpu_mem, occupied_gpu_mem = item['hardware_info']['post_loading']['gpu_info']['total_available_memory_gb'], item['hardware_info']['post_loading']['gpu_info']['total_occupied_memory_gb']\n",
    "    avl_mem, total_mem = item['hardware_info']['post_loading']['ram_info']['total_available_ram_gb'], item['hardware_info']['post_loading']['ram_info']['total_ram_gb']\n",
    "    df['total_gpu_mem_gb'].append(avl_gpu_mem + occupied_gpu_mem)\n",
    "    df['occupied_gpu_mem_gb'].append(occupied_gpu_mem)\n",
    "    df['total_mem_gb'].append(total_mem)\n",
    "    df['occupied_mem_gb'].append(total_mem - avl_mem)\n",
    "\n",
    "    df['model_device'].append(item.get('model_device'))\n",
    "    df['model_dtype'].append(item.get('model_dtype'))\n",
    "    df['model_loading_time'].append(item.get(\"model_loading_time\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "# dict_to_interpretable_string(inf_time_tests[0]['model_loading_params'].pop('gpu', N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_loading_params</th>\n",
       "      <th>model_checkpoint</th>\n",
       "      <th>gpu</th>\n",
       "      <th>model_dtype</th>\n",
       "      <th>model_device</th>\n",
       "      <th>avg_token_per_sec</th>\n",
       "      <th>model_loading_time</th>\n",
       "      <th>total_gpu_mem_gb</th>\n",
       "      <th>occupied_gpu_mem_gb</th>\n",
       "      <th>total_mem_gb</th>\n",
       "      <th>occupied_mem_gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pretrained_model_name_or_path: '/kaggle/input/...</td>\n",
       "      <td>/kaggle/input/thebloke-dolphin-2-2-1-mistral-7...</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>None</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>44.280489</td>\n",
       "      <td>3675.498247</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pretrained_model_name_or_path: '/kaggle/input/...</td>\n",
       "      <td>/kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0b...</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>None</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>37.620098</td>\n",
       "      <td>56271.106720</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.1-mistral-7B-AWQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>20.071420</td>\n",
       "      <td>38718.880415</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>19.144249</td>\n",
       "      <td>8424.471140</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.1-mistral-7B-AWQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>18.905336</td>\n",
       "      <td>38744.750738</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretrained_model_name_or_path: '/kaggle/input/...</td>\n",
       "      <td>/kaggle/input/ehartford-dolphin-2-1-mistral-7b</td>\n",
       "      <td>p100</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>18.116759</td>\n",
       "      <td>113963.797092</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>17.524174</td>\n",
       "      <td>7646.041632</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>17.322899</td>\n",
       "      <td>8991.247892</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.1-mistral-7B-AWQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>17.252137</td>\n",
       "      <td>56350.338459</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>16.090855</td>\n",
       "      <td>17687.150002</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>15.528753</td>\n",
       "      <td>5708.461523</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pretrained_model_name_or_path: '/kaggle/input/...</td>\n",
       "      <td>/kaggle/input/ehartford-dolphin-2-1-mistral-7b</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>13.495654</td>\n",
       "      <td>138732.031584</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pretrained_model_name_or_path: 'TheBloke/dolph...</td>\n",
       "      <td>TheBloke/dolphin-2.2.1-mistral-7B-GPTQ</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>3.171114</td>\n",
       "      <td>210793.818712</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretrained_model_name_or_path: '/kaggle/input/...</td>\n",
       "      <td>/kaggle/input/ehartford-dolphin-2-1-mistral-7b</td>\n",
       "      <td>t4_2x</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1.170652</td>\n",
       "      <td>177302.478075</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_loading_params  \\\n",
       "11  pretrained_model_name_or_path: '/kaggle/input/...   \n",
       "13  pretrained_model_name_or_path: '/kaggle/input/...   \n",
       "4   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "12  model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...   \n",
       "5   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "3   pretrained_model_name_or_path: '/kaggle/input/...   \n",
       "9   model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...   \n",
       "10  model_name_or_path: 'TheBloke/dolphin-2.2.1-mi...   \n",
       "2   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "7   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "6   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "1   pretrained_model_name_or_path: '/kaggle/input/...   \n",
       "8   pretrained_model_name_or_path: 'TheBloke/dolph...   \n",
       "0   pretrained_model_name_or_path: '/kaggle/input/...   \n",
       "\n",
       "                                     model_checkpoint    gpu     model_dtype  \\\n",
       "11  /kaggle/input/thebloke-dolphin-2-2-1-mistral-7...  t4_2x            None   \n",
       "13  /kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0b...  t4_2x            None   \n",
       "4                 TheBloke/dolphin-2.1-mistral-7B-AWQ  t4_2x   torch.float16   \n",
       "12             TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "5                 TheBloke/dolphin-2.1-mistral-7B-AWQ  t4_2x   torch.float16   \n",
       "3      /kaggle/input/ehartford-dolphin-2-1-mistral-7b   p100  torch.bfloat16   \n",
       "9              TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "10             TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "2                 TheBloke/dolphin-2.1-mistral-7B-AWQ  t4_2x  torch.bfloat16   \n",
       "7              TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "6              TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "1      /kaggle/input/ehartford-dolphin-2-1-mistral-7b  t4_2x  torch.bfloat16   \n",
       "8              TheBloke/dolphin-2.2.1-mistral-7B-GPTQ  t4_2x   torch.float16   \n",
       "0      /kaggle/input/ehartford-dolphin-2-1-mistral-7b  t4_2x  torch.bfloat16   \n",
       "\n",
       "   model_device  avg_token_per_sec  model_loading_time  total_gpu_mem_gb  \\\n",
       "11       cuda:0          44.280489         3675.498247                30   \n",
       "13       cuda:0          37.620098        56271.106720                30   \n",
       "4        cuda:0          20.071420        38718.880415                30   \n",
       "12       cuda:0          19.144249         8424.471140                30   \n",
       "5        cuda:0          18.905336        38744.750738                30   \n",
       "3        cuda:0          18.116759       113963.797092                16   \n",
       "9        cuda:0          17.524174         7646.041632                30   \n",
       "10       cuda:0          17.322899         8991.247892                30   \n",
       "2        cuda:0          17.252137        56350.338459                30   \n",
       "7        cuda:0          16.090855        17687.150002                30   \n",
       "6        cuda:0          15.528753         5708.461523                30   \n",
       "1        cuda:0          13.495654       138732.031584                30   \n",
       "8        cuda:0           3.171114       210793.818712                30   \n",
       "0           cpu           1.170652       177302.478075                30   \n",
       "\n",
       "    occupied_gpu_mem_gb  total_mem_gb  occupied_mem_gb  \n",
       "11                   10            32                4  \n",
       "13                    5            32                3  \n",
       "4                     6            32                5  \n",
       "12                   19            32               14  \n",
       "5                     6            32                4  \n",
       "3                    16            32                5  \n",
       "9                    16            32               10  \n",
       "10                   16            32               12  \n",
       "2                     6            32                5  \n",
       "7                     9            32                5  \n",
       "6                    13            32               10  \n",
       "1                    17            32                5  \n",
       "8                    11            32                6  \n",
       "0                     1            32               17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by = 'avg_token_per_sec', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by = 'avg_token_per_sec', ascending = False).iloc[0]['model_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsearch-VBcDHvMD-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
