{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', low_cpu_mem_usage: True, torch_dtype: 'torch.bfloat16'\n",
      "1.1706524221265413\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'auto'\n",
      "13.495654390917164\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.bfloat16'\n",
      "17.252136787429297\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'cuda:0'\n",
      "18.116759082387627\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.float16'\n",
      "20.071419678895033\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', revision: 'refs/pr/1'\n",
      "18.905335754174427\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True'\n",
      "15.528752866505773\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True', exllama_version: 'v2'\n",
      "16.090855255173437\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-8bit-32g-actorder_True'\n",
      "3.1711142028060886\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', low_cpu_mem_usage: True, use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "17.52417389720822\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "17.322899302948603\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'\n",
      "44.28048877512073\n",
      "\n",
      "\n",
      "model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: False, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "19.14424932230798\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0bpw-h6-exl2'\n",
      "37.620098225627714\n",
      "\n",
      "\n",
      "pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', quantization_config: 'BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "'\n",
      "8.95074193033884\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('inference_time_tests.json', 'r') as file:\n",
    "    inf_time_tests = json.load(file)\n",
    "\n",
    "def dict_to_interpretable_string(d):\n",
    "    parts = []\n",
    "    for key, value in d.items():\n",
    "        # Convert the value to a string\n",
    "        value_str = str(value)\n",
    "        # Add quotes around string values for clarity\n",
    "        if isinstance(value, str):\n",
    "            value_str = f\"'{value_str}'\"\n",
    "        # Append the formatted key-value pair to the parts list\n",
    "        parts.append(f\"{key}: {value_str}\")\n",
    "    # Join all parts into a single string\n",
    "    return ', '.join(parts)\n",
    "\n",
    "df = {\n",
    "    'model_loading_params' : [],\n",
    "    'model_checkpoint' : [],\n",
    "    'gpu' : [],\n",
    "    'model_dtype' : [],\n",
    "    'model_device' : [],\n",
    "    'avg_token_per_sec' : [],\n",
    "    'model_loading_time' : [],\n",
    "    'total_gpu_mem_gb' : [],\n",
    "    'occupied_gpu_mem_gb' : [],\n",
    "    'total_mem_gb' : [],\n",
    "    'occupied_mem_gb' : [],\n",
    "}\n",
    "\n",
    "for item in inf_time_tests:\n",
    "    gpu = item['model_loading_params'].get('gpu', 't4_2x')\n",
    "    item['model_loading_params'].pop('gpu', None)\n",
    "    model_loading_params_str = dict_to_interpretable_string(item['model_loading_params'])\n",
    "\n",
    "    avg_tokens_per_sec = 0\n",
    "    for sample_metric in item['sample_metrics']:\n",
    "        avg_tokens_per_sec += sample_metric['output_tokens'] / (sample_metric['gen_latency'] / 1000)\n",
    "    avg_tokens_per_sec /= len(item['sample_metrics'])\n",
    "\n",
    "    print(model_loading_params_str)\n",
    "    print(avg_tokens_per_sec)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    df['model_loading_params'].append(model_loading_params_str)\n",
    "    df['model_checkpoint'].append(item['model_loading_params'].get('pretrained_model_name_or_path', item['model_loading_params'].get('model_name_or_path')))\n",
    "    df['gpu'].append(gpu)\n",
    "    df['avg_token_per_sec'].append(avg_tokens_per_sec)\n",
    "    avl_gpu_mem, occupied_gpu_mem = item['hardware_info']['post_loading']['gpu_info']['total_available_memory_gb'], item['hardware_info']['post_loading']['gpu_info']['total_occupied_memory_gb']\n",
    "    avl_mem, total_mem = item['hardware_info']['post_loading']['ram_info']['total_available_ram_gb'], item['hardware_info']['post_loading']['ram_info']['total_ram_gb']\n",
    "    df['total_gpu_mem_gb'].append(avl_gpu_mem + occupied_gpu_mem)\n",
    "    df['occupied_gpu_mem_gb'].append(occupied_gpu_mem)\n",
    "    df['total_mem_gb'].append(total_mem)\n",
    "    df['occupied_mem_gb'].append(total_mem - avl_mem)\n",
    "\n",
    "    df['model_device'].append(item.get('model_device'))\n",
    "    df['model_dtype'].append(item.get('model_dtype'))\n",
    "    df['model_loading_time'].append(item.get(\"model_loading_time\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "# dict_to_interpretable_string(inf_time_tests[0]['model_loading_params'].pop('gpu', N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps - 44.28048877512073\n",
      "model checkpoint - /kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'\n",
      "model dtype - None\n",
      "\n",
      "\n",
      "\n",
      "tps - 37.620098225627714\n",
      "model checkpoint - /kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0bpw-h6-exl2\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/ls-dolphin-2-2-1-mistral-7b-4-0bpw-h6-exl2'\n",
      "model dtype - None\n",
      "\n",
      "\n",
      "\n",
      "tps - 20.071419678895033\n",
      "model checkpoint - TheBloke/dolphin-2.1-mistral-7B-AWQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.float16'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 19.14424932230798\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: False, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 18.905335754174427\n",
      "model checkpoint - TheBloke/dolphin-2.1-mistral-7B-AWQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', revision: 'refs/pr/1'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 18.116759082387627\n",
      "model checkpoint - /kaggle/input/ehartford-dolphin-2-1-mistral-7b\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'cuda:0'\n",
      "model dtype - torch.bfloat16\n",
      "\n",
      "\n",
      "\n",
      "tps - 17.52417389720822\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', low_cpu_mem_usage: True, use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 17.322899302948603\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', use_cuda_fp16: True, use_safetensors: True, inject_fused_attention: True, inject_fused_mlp: True, warmup_triton: False, quantize_config: None\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 17.252136787429297\n",
      "model checkpoint - TheBloke/dolphin-2.1-mistral-7B-AWQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.1-mistral-7B-AWQ', device_map: 'cuda:0', trust_remote_code: False, torch_dtype: 'torch.bfloat16'\n",
      "model dtype - torch.bfloat16\n",
      "\n",
      "\n",
      "\n",
      "tps - 16.090855255173437\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True', exllama_version: 'v2'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 15.528752866505773\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-4bit-32g-actorder_True'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 13.495654390917164\n",
      "model checkpoint - /kaggle/input/ehartford-dolphin-2-1-mistral-7b\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', torch_dtype: 'torch.bfloat16', device_map: 'auto'\n",
      "model dtype - torch.bfloat16\n",
      "\n",
      "\n",
      "\n",
      "tps - 8.95074193033884\n",
      "model checkpoint - /kaggle/input/ehartford-dolphin-2-1-mistral-7b\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', quantization_config: 'BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 3.1711142028060886\n",
      "model checkpoint - TheBloke/dolphin-2.2.1-mistral-7B-GPTQ\n",
      "model_loading_params - pretrained_model_name_or_path: 'TheBloke/dolphin-2.2.1-mistral-7B-GPTQ', device_map: 'auto', trust_remote_code: False, revision: 'gptq-8bit-32g-actorder_True'\n",
      "model dtype - torch.float16\n",
      "\n",
      "\n",
      "\n",
      "tps - 1.1706524221265413\n",
      "model checkpoint - /kaggle/input/ehartford-dolphin-2-1-mistral-7b\n",
      "model_loading_params - pretrained_model_name_or_path: '/kaggle/input/ehartford-dolphin-2-1-mistral-7b', low_cpu_mem_usage: True, torch_dtype: 'torch.bfloat16'\n",
      "model dtype - torch.bfloat16\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_df = df.sort_values(by = 'avg_token_per_sec', ascending = False)\n",
    "for idx in sorted_df.index:\n",
    "    sample = sorted_df.loc[idx]\n",
    "\n",
    "    print(f\"tps - {sample.avg_token_per_sec}\")\n",
    "    print(f\"model checkpoint - {sample.model_checkpoint}\")\n",
    "    print(f\"model_loading_params - {sample.model_loading_params}\")\n",
    "    print(f\"model dtype - {sample.model_dtype}\")\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by = 'avg_token_per_sec', ascending = False).iloc[0]['model_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pretrained_model_name_or_path: '/kaggle/input/thebloke-dolphin-2-2-1-mistral-7b-gptq'\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by = 'avg_token_per_sec', ascending = False).iloc[0]['model_loading_params']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsearch-VBcDHvMD-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
