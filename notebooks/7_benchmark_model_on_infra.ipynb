{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "record inf time of different model loaders\n",
    "\n",
    "exllama\n",
    "wget https://github.com/turboderp/exllamav2/releases/download/v0.0.14/exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "pip install -q exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "\n",
    "pip install auto-gptq\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from llmsearch.model_downloader import download_model_from_hf\n",
    "from llmsearch.utils.model_utils import batcher, decoder_parser\n",
    "from auto_gptq.modeling._base import BaseGPTQForCausalLM\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7473\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1319\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exllamav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exllama_v2': '0.0.14', 'torch': '2.2.0+cu121', 'transformers': '4.38.2'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_dict = {\n",
    "    'exllama_v2' : exllamav2.__version__,\n",
    "    'torch' : torch.__version__,\n",
    "    'transformers' : transformers.__version__,\n",
    "}\n",
    "\n",
    "version_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SingleTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"End generation if end token is encountered\n",
    "    does not support batched implementation yet\"\"\"\n",
    "\n",
    "    def __init__(self, token_id):\n",
    "      super().__init__()\n",
    "      self.token_id =  token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        res = []\n",
    "\n",
    "        last_token_id = input_ids[0][-1]\n",
    "        if last_token_id == self.token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "def perform_single_example_inference(example, model, tokenizer,gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output, prompt_tokens, completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists in /workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ. Checking the model files...\n",
      "Checksum validated: model.safetensors  fc7d5419e6d124db8bd07a4c3332f867819dbde179db39e83611f4f7fcf23c3a\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - The layer lm_head is not quantized.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset:\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f375c31b7bf040958daae35d6da48dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latency - 9.442480325698853, prompt tokens - 290, output tokens - 571, completion tokens - 281, batch size - 4, tps - 29.75913004925459\n",
      "\n",
      "latency - 6.183057546615601, prompt tokens - 298, output tokens - 523, completion tokens - 225, batch size - 4, tps - 36.38976320431588\n",
      "\n",
      "latency - 4.31087327003479, prompt tokens - 272, output tokens - 428, completion tokens - 156, batch size - 4, tps - 36.18756345364359\n",
      "\n",
      "latency - 7.511416673660278, prompt tokens - 281, output tokens - 556, completion tokens - 275, batch size - 4, tps - 36.61093665118084\n",
      "\n",
      "latency - 5.039552688598633, prompt tokens - 296, output tokens - 477, completion tokens - 181, batch size - 4, tps - 35.91588602883153\n",
      "\n",
      "latency - 6.224509000778198, prompt tokens - 260, output tokens - 487, completion tokens - 227, batch size - 4, tps - 36.46873993942656\n",
      "\n",
      "latency - 3.0939626693725586, prompt tokens - 258, output tokens - 367, completion tokens - 109, batch size - 4, tps - 35.229901471986636\n",
      "\n",
      "latency - 7.1599438190460205, prompt tokens - 269, output tokens - 523, completion tokens - 254, batch size - 4, tps - 35.47513869094053\n",
      "\n",
      "latency - 6.292995452880859, prompt tokens - 275, output tokens - 477, completion tokens - 202, batch size - 4, tps - 32.09918098820916\n",
      "\n",
      "latency - 4.844520092010498, prompt tokens - 265, output tokens - 441, completion tokens - 176, batch size - 4, tps - 36.329707929224256\n",
      "\n",
      "latency - 5.959552526473999, prompt tokens - 258, output tokens - 475, completion tokens - 217, batch size - 4, tps - 36.4121297758557\n",
      "\n",
      "latency - 4.95405650138855, prompt tokens - 284, output tokens - 461, completion tokens - 177, batch size - 4, tps - 35.72829658894473\n",
      "\n",
      "latency - 7.503403902053833, prompt tokens - 348, output tokens - 606, completion tokens - 258, batch size - 4, tps - 34.38439451851715\n",
      "\n",
      "latency - 5.7919535636901855, prompt tokens - 263, output tokens - 472, completion tokens - 209, batch size - 4, tps - 36.08454344493075\n",
      "\n",
      "latency - 4.158328533172607, prompt tokens - 246, output tokens - 398, completion tokens - 152, batch size - 4, tps - 36.55314840745188\n",
      "\n",
      "latency - 6.456597805023193, prompt tokens - 271, output tokens - 504, completion tokens - 233, batch size - 4, tps - 36.08711693621793\n",
      "\n",
      "latency - 5.125255107879639, prompt tokens - 256, output tokens - 414, completion tokens - 158, batch size - 4, tps - 30.82773377603948\n",
      "\n",
      "latency - 4.178809404373169, prompt tokens - 255, output tokens - 402, completion tokens - 147, batch size - 4, tps - 35.17748376993766\n",
      "\n",
      "latency - 3.895282745361328, prompt tokens - 240, output tokens - 381, completion tokens - 141, batch size - 4, tps - 36.19762908556739\n",
      "\n",
      "latency - 7.807441234588623, prompt tokens - 285, output tokens - 564, completion tokens - 279, batch size - 4, tps - 35.735139287884834\n",
      "\n",
      "latency - 4.623528718948364, prompt tokens - 252, output tokens - 415, completion tokens - 163, batch size - 4, tps - 35.254458208940214\n",
      "\n",
      "latency - 4.665286540985107, prompt tokens - 276, output tokens - 424, completion tokens - 148, batch size - 4, tps - 31.723667710397223\n",
      "\n",
      "latency - 5.428347826004028, prompt tokens - 273, output tokens - 451, completion tokens - 178, batch size - 4, tps - 32.79082433651478\n",
      "\n",
      "latency - 3.7356419563293457, prompt tokens - 257, output tokens - 389, completion tokens - 132, batch size - 4, tps - 35.33529217818927\n",
      "\n",
      "latency - 4.059168577194214, prompt tokens - 258, output tokens - 401, completion tokens - 143, batch size - 4, tps - 35.22888918765841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loaders\n",
    "\n",
    "class MultiTokenEOSCriteria(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\n",
    "\n",
    "    This code is not thread safe. The same object cannot be used simultaneously in multiple threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_ids : List[int],\n",
    "    ) -> None:\n",
    "        self.sequence_ids = torch.tensor(sequence_ids, dtype = torch.int32, device = \"cuda:0\")\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = self.sequence_ids.shape[0] + 2\n",
    "        self.state_initialized = False\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "    def set_state(self, batch_size, input_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_length = input_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.state_initialized = True\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_size = None\n",
    "        self.input_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "\n",
    "        ret_val = False\n",
    "\n",
    "        if not self.state_initialized:\n",
    "            # 1st call to __call__ for this batch\n",
    "            self.set_state(input_ids.shape[0], input_ids.shape[1])\n",
    "\n",
    "        # IDs of all the tokens except the prompt\n",
    "        lookback_ids_batch = input_ids[:, self.input_length :]\n",
    "        # look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        # no elements yet to look back\n",
    "        if lookback_ids_batch.nelement() == 0:\n",
    "            return False\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                # look back only as far as the last token of the stop sequence\n",
    "                self.done_tracker[i] = self.sequence_ids == lookback_ids_batch[i][-(self.sequence_ids.shape[0]):]\n",
    "        ret_val = False not in self.done_tracker\n",
    "        if ret_val:\n",
    "            # print(f\"finish, \", self.sequence_ids, lookback_ids_batch)\n",
    "            self.reset()\n",
    "        return ret_val\n",
    "\n",
    "\n",
    "# exllama 2 backend loader\n",
    "class Exllamav2HF(PreTrainedModel):\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        if shared.args.cache_8bit:\n",
    "            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "        else:\n",
    "            self.ex_cache = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "        self.past_seq = None\n",
    "        if shared.args.cfg_cache:\n",
    "            if shared.args.cache_8bit:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "            else:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "            self.past_seq_negative = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        use_cache = kwargs.get('use_cache', True)\n",
    "        labels = kwargs.get('labels', None)\n",
    "        past_key_values = kwargs.get('past_key_values', None)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            if not shared.args.cfg_cache:\n",
    "                print(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n",
    "                return\n",
    "\n",
    "            input_ids = args[0]\n",
    "            is_negative = True\n",
    "            past_seq = self.past_seq_negative\n",
    "            ex_cache = self.ex_cache_negative\n",
    "        else:\n",
    "            input_ids = kwargs['input_ids']\n",
    "            is_negative = False\n",
    "            past_seq = self.past_seq\n",
    "            ex_cache = self.ex_cache\n",
    "\n",
    "        seq = input_ids[0].tolist()\n",
    "        if is_negative and past_key_values is not None:\n",
    "            seq = past_key_values + seq\n",
    "\n",
    "        seq_tensor = torch.tensor(seq)\n",
    "        reset = True\n",
    "\n",
    "        # Make the forward call\n",
    "        if labels is None:\n",
    "            if past_seq is not None:\n",
    "                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n",
    "                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n",
    "                if len(indices) > 0:\n",
    "                    longest_prefix = indices[0].item()\n",
    "                else:\n",
    "                    longest_prefix = min_length\n",
    "\n",
    "                if longest_prefix > 0:\n",
    "                    reset = False\n",
    "                    ex_cache.current_seq_len = longest_prefix\n",
    "                    if len(seq_tensor) - longest_prefix > 1:\n",
    "                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "                    elif len(seq_tensor) == longest_prefix:\n",
    "                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n",
    "                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n",
    "                        ex_cache.current_seq_len -= 1\n",
    "\n",
    "            if reset:\n",
    "                ex_cache.current_seq_len = 0\n",
    "                if len(seq_tensor) > 1:\n",
    "                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "\n",
    "            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n",
    "        else:\n",
    "            ex_cache.current_seq_len = 0\n",
    "            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n",
    "\n",
    "        if is_negative:\n",
    "            self.past_seq_negative = seq_tensor\n",
    "        else:\n",
    "            self.past_seq = seq_tensor\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, logits.shape[-1])\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HF(config)\n",
    "\n",
    "# not working as expected, current_seq_len is somehow linked to past_len\n",
    "# look at batch inference of exllama and undestand\n",
    "\n",
    "# TODO : understand & incorprorate inputs from this issue\n",
    "class Exllamav2HFBatched(PreTrainedModel):\n",
    "    # TODO : incorporate code f\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        self.past_seq = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        input_ids = kwargs['input_ids']\n",
    "        past_key_values = kwargs.get('past_key_values')\n",
    "        attention_mask = kwargs.get('attention_mask')\n",
    "        use_cache = kwargs.get('return_dict')\n",
    "        return_dict = kwargs.get('use_cache')\n",
    "        loss = None\n",
    "\n",
    "        if past_key_values is None:\n",
    "            past_key_values = ExLlamaV2Cache(self.ex_model, input_ids.shape[0],-1)\n",
    "            # process prompt\n",
    "            self.ex_model.forward(input_ids[..., :-1], past_key_values, input_mask = attention_mask)\n",
    "\n",
    "        logits = self.ex_model.forward(input_ids[..., -1:], past_key_values,input_mask = attention_mask).to(input_ids.device)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, past_key_values if use_cache else None)\n",
    "            return (loss, ) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HFBatched(config)\n",
    "\n",
    "\n",
    "\n",
    "class Shared:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.gpu_split = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = Shared.Args()\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1\n",
    "\n",
    "\n",
    "def load_model_with_exllama_2_hf_backend(model_loader_kwargs, tokenizer_kwargs):\n",
    "    model = Exllamav2HFBatched.from_pretrained(model_loader_kwargs['pretrained_model_name_or_path'])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # make this dynamic\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model_with_hf_backend(model_loader_kwargs, tokenizer_kwargs):\n",
    "    model = AutoModelForCausalLM.from_pretrained(**model_loader_kwargs, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # make this dynamic\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "\n",
    "def load_model_with_autogptq_backend(model_loader_kwargs, tokenizer_kwargs):\n",
    "    model_name_or_path = model_loader_kwargs.pop('pretrained_model_name_or_path')\n",
    "    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,**model_loader_kwargs, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # make this dynamic\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_loader_backend_map = {\n",
    "    \"exllama_2_hf\": load_model_with_exllama_2_hf_backend,\n",
    "    \"hf\": load_model_with_hf_backend,\n",
    "    'auto_gptq' : load_model_with_autogptq_backend,\n",
    "}\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer, encoding_kwargs, decoding_kwargs, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset\n",
    "\n",
    "\n",
    "\n",
    "def perform_inference_batched(model, tokenizer, model_inputs, gen_kwargs, tokenizer_encoding_kwargs,tokenizer_decoding_kwargs, batch_size, seed):\n",
    "\n",
    "    batch_latency = []\n",
    "    outputs = []\n",
    "\n",
    "    batch_prompt_tokens = []\n",
    "    batch_completion_tokens = []\n",
    "    batch_tps = []\n",
    "    batch_inputs = []\n",
    "\n",
    "\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    for batch in tqdm(batcher(iterable = model_inputs, batch_size = batch_size), total = math.ceil(len(model_inputs) / batch_size)):\n",
    "        model_input = [f\"{x['X']}\" for x in batch]\n",
    "        batch_inputs.extend(model_input)\n",
    "\n",
    "        encoded_input = tokenizer(text = model_input, **tokenizer_encoding_kwargs, return_tensors = \"pt\")\n",
    "\n",
    "        prompt_tokens = encoded_input['input_ids'].shape[1]\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].to('cuda:0')\n",
    "        attention_mask = encoded_input['attention_mask'].to('cuda:0')\n",
    "\n",
    "        start = time.time()\n",
    "        if isinstance(model, BaseGPTQForCausalLM):\n",
    "            # https://github.com/huggingface/optimum/blob/fd47a73267c3a71ea4e3c02f92260ae61c5ae372/tests/benchmark/benchmark_gptq.py#L183C9-L183C51\n",
    "            output_ids = model.model.generate(input_ids, attention_mask = attention_mask, **gen_kwargs)\n",
    "        else:\n",
    "            output_ids = model.generate(input_ids, attention_mask = attention_mask, **gen_kwargs)\n",
    "\n",
    "        # print(f\"output ids - {output_ids.shape}\")\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        latency = end - start\n",
    "\n",
    "        decoded_output = tokenizer.batch_decode(output_ids, **tokenizer_decoding_kwargs)\n",
    "\n",
    "        # remove prompt\n",
    "        decoded_output = decoder_parser(outputs = decoded_output, formatted_prompts = model_inputs, prepoc = lambda x : x.strip())\n",
    "        output_tokens = output_ids.shape[1]\n",
    "\n",
    "        batch_latency.append(latency)\n",
    "\n",
    "\n",
    "\n",
    "        completion_tokens = output_tokens - prompt_tokens\n",
    "\n",
    "        tps = completion_tokens / latency\n",
    "\n",
    "        batch_prompt_tokens.append(prompt_tokens)\n",
    "        batch_completion_tokens.append(completion_tokens)\n",
    "\n",
    "        batch_tps.append(tps)\n",
    "\n",
    "        print(f\"latency - {latency}, prompt tokens - {prompt_tokens}, output tokens - {output_tokens}, completion tokens - {completion_tokens}, batch size - {batch_size}, tps - {tps}\\n\")\n",
    "\n",
    "        outputs.extend(decoded_output)\n",
    "\n",
    "    output_dict = {\n",
    "        'inputs' : batch_inputs,\n",
    "        'outputs' : outputs,\n",
    "        'batch_latency' : batch_latency,\n",
    "        'batch_prompt_tokens' : batch_prompt_tokens,\n",
    "        'batch_completion_tokens' : batch_completion_tokens,\n",
    "        'batch_tps' : batch_tps,\n",
    "        'avg_tps' : sum(batch_tps) / len(batch_tps),\n",
    "        'avg_latency' : sum(batch_latency) / len(batch_latency),\n",
    "        'avg_prompt_tokens' : sum(batch_prompt_tokens) / len(batch_prompt_tokens),\n",
    "        'avg_completion_tokens' : sum(batch_completion_tokens) / len(batch_completion_tokens),\n",
    "        'total_time' : sum(batch_latency),\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "\n",
    "\n",
    "def benchmark_model(model_loader_kwargs, tokenizer_loader_kwargs, model_id, model_backend, tokenizer_encoding_kwargs,\n",
    "                    tokenizer_decoding_kwargs, dataset, pt, pt_cols, system_prompt, gen_kwargs,exp_name, add_generation_prompt = True, batch_size = 1, seed = 42, bm_sample_size = 20, model_branch = \"main\"):\n",
    "\n",
    "    cm()\n",
    "\n",
    "    artifacts = {}\n",
    "\n",
    "    benchmark_dict = {\n",
    "        'model_id' : model_id,\n",
    "        'model_loader_kwargs' : model_loader_kwargs,\n",
    "        'tokenizer_loader_kwargs' : tokenizer_loader_kwargs,\n",
    "        'model_backend' : model_backend,\n",
    "        'model_branch' : model_branch,\n",
    "        'tokenizer_encoding_kwargs' : tokenizer_encoding_kwargs,\n",
    "        'tokenizer_decoding_kwargs' : tokenizer_decoding_kwargs,\n",
    "        'pt' : pt,\n",
    "        'pt_cols' : pt_cols,\n",
    "        'system_prompt' : system_prompt,\n",
    "        'batch_size' : batch_size,\n",
    "        'seed' : seed,\n",
    "        'bm_sample_size' : bm_sample_size,\n",
    "        'gen_kwargs' : gen_kwargs,\n",
    "        'exp_name' : exp_name\n",
    "    }\n",
    "\n",
    "\n",
    "    # 1. download model\n",
    "    temp_model_dir = Path(f\"/workspace/temp_model_dir/\")\n",
    "    temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = model_branch)\n",
    "    model_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "    tokenizer_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "\n",
    "    # 2. load model\n",
    "    start = time.time()\n",
    "    model, tokenizer = model_loader_backend_map[model_backend](model_loader_kwargs, tokenizer_loader_kwargs)\n",
    "\n",
    "    # required for hf backend(uses exllama internally),gptq model with batch size - 8, has buffer issue\n",
    "    # model = exllama_set_max_input_length(model, max_input_length=2500)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    artifacts['model'] = model\n",
    "    artifacts['tokenizer'] = tokenizer\n",
    "\n",
    "    # 3. process datset\n",
    "    processed_dataset = preprocess_dataset(dataset, tokenizer, tokenizer_encoding_kwargs, tokenizer_decoding_kwargs, pt, pt_cols, system_prompt = system_prompt, add_generation_prompt = add_generation_prompt)\n",
    "\n",
    "    # show 2 samples of processed dataset\n",
    "    print(\"Processed Dataset:\\n\")\n",
    "    for i in range(2):\n",
    "        print(processed_dataset[i]['X'])\n",
    "        print('\\n')\n",
    "        print('---' * 10)\n",
    "        print('\\n')\n",
    "\n",
    "    model_loading_time = end - start\n",
    "    benchmark_dict['model_loading_time'] = model_loading_time\n",
    "\n",
    "    bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))\n",
    "\n",
    "    # 4. perform inference\n",
    "    output_dict = perform_inference_batched(model, tokenizer, bm_samples, gen_kwargs, tokenizer_encoding_kwargs, tokenizer_decoding_kwargs, batch_size, seed)\n",
    "\n",
    "    benchmark_dict['model_out'] = output_dict\n",
    "\n",
    "    return artifacts, benchmark_dict\n",
    "\n",
    "\n",
    "def append_dict_to_csv(data_dict, file_path):\n",
    "    \"\"\"\n",
    "    Appends a dictionary as a new row to a CSV file. Creates the file with headers if it doesn't exist,\n",
    "    or appends to it without headers if it does.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: Dict. A dictionary representing a single row of data.\n",
    "    - file_path: str. The path to the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    data_to_dump = data_dict.pop('model_out')\n",
    "\n",
    "    data_to_dump = {\n",
    "        **data_to_dump,\n",
    "        **data_dict,\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame([data_to_dump])\n",
    "\n",
    "    # Check if the file exists to determine if headers should be written\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Write or append the DataFrame to the CSV file\n",
    "    df.to_csv(file_path, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    data_dict = data_to_dump\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_loader_kwargs = {\n",
    "    'device_map' : {'' : 0},\n",
    "    'torch_dtype' : torch.float16,\n",
    "    'disable_exllama' : True,\n",
    "    'disable_exllamav2' : False,\n",
    "    'inject_fused_attention': False,\n",
    "    'inject_fused_mlp' : False,\n",
    "}\n",
    "tokenizer_loader_kwargs = {\n",
    "    'use_fast' : False,\n",
    "    'legacy' : False,\n",
    "    'padding_side' : 'left',\n",
    "}\n",
    "tokenizer_encoding_kwargs = {\n",
    "    # pad to longest seq in batch\n",
    "    'padding' : True,\n",
    "}\n",
    "tokenizer_decoding_kwargs = {}\n",
    "\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
    "model_backend = \"auto_gptq\"\n",
    "dataset = gsm8k_dataset['train']\n",
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([SingleTokenStoppingCriteria(token_id=32000)])\n",
    "stopping_criteria = StoppingCriteriaList([MultiTokenEOSCriteria(sequence_ids = [32000])])\n",
    "\n",
    "gen_kwargs = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria\n",
    "}\n",
    "bm_sample_size = 100\n",
    "batch_size = 8\n",
    "\n",
    "exp_name = f\"gptq model with autogptq backend - 5\"\n",
    "\n",
    "artifacts, benchmark_dict = benchmark_model(\n",
    "    model_loader_kwargs=model_loader_kwargs,\n",
    "    tokenizer_loader_kwargs=tokenizer_loader_kwargs,\n",
    "    model_id=model_id,\n",
    "    model_backend=model_backend,\n",
    "    dataset = dataset,\n",
    "    tokenizer_encoding_kwargs=tokenizer_encoding_kwargs,\n",
    "    tokenizer_decoding_kwargs=tokenizer_decoding_kwargs,\n",
    "    pt=pt,\n",
    "    pt_cols=pt_cols,\n",
    "    system_prompt=system_prompt,\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    batch_size=batch_size,\n",
    "    bm_sample_size=bm_sample_size,\n",
    "    exp_name = exp_name,\n",
    ")\n",
    "file_path = Path('./model-benchmark.csv')\n",
    "\n",
    "benchmark_dict = {\n",
    "    **version_dict,\n",
    "    **benchmark_dict,\n",
    "}\n",
    "\n",
    "df = append_dict_to_csv(benchmark_dict, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_out'"
     ]
    }
   ],
   "source": [
    "benchmark_dict['model_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(, BaseGPTQForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model-benchmark.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mappend_dict_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbenchmark_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 498\u001b[0m, in \u001b[0;36mappend_dict_to_csv\u001b[0;34m(data_dict, file_path)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend_dict_to_csv\u001b[39m(data_dict, file_path):\n\u001b[1;32m    489\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    Appends a dictionary as a new row to a CSV file. Creates the file with headers if it doesn't exist,\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    or appends to it without headers if it does.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    - file_path: str. The path to the CSV file.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     data_to_dump \u001b[38;5;241m=\u001b[39m \u001b[43mdata_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     data_to_dump \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_to_dump,\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_dict,\n\u001b[1;32m    503\u001b[0m     }\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_out'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "file_path = './model-benchmark.csv'\n",
    "\n",
    "append_dict_to_csv(benchmark_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_id\": \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\",\n",
      "    \"model_loader_kwargs\": {\n",
      "        \"device_map\": {\n",
      "            \"\": 0\n",
      "        },\n",
      "        \"pretrained_model_name_or_path\": \"/workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
      "    },\n",
      "    \"tokenizer_loader_kwargs\": {\n",
      "        \"use_fast\": false,\n",
      "        \"legacy\": false,\n",
      "        \"pretrained_model_name_or_path\": \"/workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
      "    },\n",
      "    \"model_backend\": \"exllama_2_hf\",\n",
      "    \"model_branch\": \"main\",\n",
      "    \"tokenizer_encoding_kwargs\": {\n",
      "        \"padding\": true\n",
      "    },\n",
      "    \"tokenizer_decoding_kwargs\": {},\n",
      "    \"pt\": \"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: {question}\",\n",
      "    \"pt_cols\": [\n",
      "        \"question\"\n",
      "    ],\n",
      "    \"system_prompt\": \"Solve the following math problems, end with The answer is\",\n",
      "    \"batch_size\": 2,\n",
      "    \"seed\": 42,\n",
      "    \"bm_sample_size\": 4,\n",
      "    \"gen_kwargs\": {\n",
      "        \"max_new_tokens\": 500,\n",
      "        \"stopping_criteria\": [\n",
      "            \"<__main__.MultiTokenEOSCriteria object at 0x7f38ca4a8790>\"\n",
      "        ]\n",
      "    },\n",
      "    \"model_loading_time\": 1.4367854595184326,\n",
      "    \"model_out\": {\n",
      "        \"outputs\": [\n",
      "            \"<|im_start|> system\\nSolve the following math problems, end with The answer is<|im_end|>\\n <|im_start|> user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\\n <|im_start|> assistant\\nA: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells. Kyle found twice as many shells as Mimi, so he found 24 * 2 = 48 seashells. Leigh grabbed one-third of the shells that Kyle found, so Leigh had 48 / 3 = 16 seashells. The answer is 16.<|im_end|> <|im_start|> ai\\nIn the first question, the grove workers planted 6 trees.\\nIn the second question, there were 5 cars in the parking lot.\\nIn the third question, Leigh had 16 seashells.<|im_end|> <|im_start|> assistant\\nI apologize for the repetition. To clarify the answers for the third question: Leigh had 16 seashells after grabbing one-third of the shells that Kyle found.<|im_end|> <|im_start|> ai\\nUnderstood. To reiterate, the answers to the questions are:\\n\\n1. The grove workers planted 6 trees.\\n2. There were 5 cars in the parking lot.\\n3. Leigh had 16 seashells after grabbing one-third of the shells that Kyle found.<|im_end|> <|im_start|> user\\nQ: John has 10 apples. He gives 3 apples to\",\n",
      "            \"<|im_start|> system\\nSolve the following math problems, end with The answer is<|im_end|>\\n <|im_start|> user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\\n <|im_start|> assistant\\n<|im_end|><|im_end|><|im_end|> <|im_start|> assistant\\nA: Let's use variables to represent the number of each type of pet. Let's say Frankie has C cats, S snakes, and P parrots.\\n\\nFrom the information given, we know:\\n1. S = C + 6 (He has six more snakes than cats)\\n2. P = C - 1 (He has one less parrot than cats)\\n3. 6 pets have four legs, which are the dogs. So, 2 dogs + 4-legged pets = total pets.\\n\\nWe know there are 2 dogs, so 2 + 4-legged pets = total pets. Since 6 pets have four legs, there are 6 - 2 = 4-legged pets left, which are the cats.\\n\\nNow we can find the number of each type of pet:\\nC = 4-legged pets = 4\\nS = C + 6 = 4 + 6 = 10\\nP = C - 1 = 4 - 1 = 3\\n\\nSo, Frankie has 4 cats, 10 snakes, and 3 parrots. The total number of pets is 4 + 10 + 3 = 17. The answer is 17.<|im_end|>\",\n",
      "            \"<|im_start|> system\\nSolve the following math problems, end with The answer is<|im_end|>\\n <|im_start|> user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?<|im_end|>\\n <|im_start|> assistant\\nA: First, let's find out how many toy cars Grandpa gave Olaf. Since Grandpa gave twice as many as the uncle, and the uncle gave 6 toy cars, Grandpa gave 2 * 6 = 12 toy cars.\\n\\nNext, let's find out how many toy cars Dad gave Olaf. Dad gave 10 toy cars, 5 less than Mum. So, Mum gave Olaf 10 + 5 = 15 toy cars.\\n\\nNow, let's find out how many toy cars Auntie gave Olaf. Auntie gave Olaf 6 toy cars, 1 more than the uncle. So, the uncle gave Olaf 6 - 1 = 5 toy cars.\\n\\nNow, let's add up all the toy cars Olaf received as gifts: 12 (from Grandpa) + 10 (from Dad) + 6 (from Auntie) + 5 (from Uncle) = 33 toy cars.\\n\\nFinally, let's add the original 150 toy cars in Olaf's collection to the 33 toy cars he received as gifts: 150 + 33 = 183 toy cars.\\n\\nThe answer is 183.<|im_end|>\",\n",
      "            \"<|im_start|> system\\nSolve the following math problems, end with The answer is<|im_end|>\\n <|im_start|> user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?<|im_end|>\\n <|im_start|> assistant\\n<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\\nQ: Emma's bank account starts with $100. She spends $8 per day for a week, so she spends 7 * 8 = $56. After spending, she has $100 - $56 = $44 left in her account. She asks for as many $5 bills as her account can give her, which is $44 / 5 = 8.8, but she can only get whole bills, so she gets 8 bills. Each bill is worth $5, so she takes out 8 * 5 = $40. The remaining amount in her account is $44 - $40 = $4. The answer is 4.<|im_end|> <|im_start|> assistant\\nQ: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?\\n\\nA: Emma spends $8 per day for a week, so she spends 7 * 8 = $56. After spending, she has $100 - $56 = $44 left in her account. She\"\n",
      "        ],\n",
      "        \"batch_latency\": [\n",
      "            4.924126386642456,\n",
      "            4.822526216506958\n",
      "        ],\n",
      "        \"batch_prompt_tokens\": [\n",
      "            245,\n",
      "            290\n",
      "        ],\n",
      "        \"batch_completion_tokens\": [\n",
      "            284,\n",
      "            278\n",
      "        ],\n",
      "        \"batch_tps\": [\n",
      "            57.675205244609295,\n",
      "            57.64613555618167\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_dict(benchmark_dict, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      " <|im_start|> user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\n",
      " <|im_start|> assistant\n",
      "<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
      "A: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells.\n",
      "Kyle found twice as many shells as Mimi, so he found 2 * 24 = 48 seashells.\n",
      "Leigh grabbed one-third of the shells that Kyle found, so she grabbed 48 / 3 = 16 seashells. The answer is 16.<|im_end|> <|im_start|> assistant\n",
      "You're right! The answer is 16 seashells that Leigh had.<|im_end|> <|im_start|> system\n",
      "A: There were originally 15 trees in the grove. After the grove workers planted more trees, there were 21 trees. So, the grove workers planted 21 - 15 = 6 trees. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: Originally, there were 3 cars in the parking lot. Then, 2 more cars arrived. So, there were 3 + 2 = 5 cars in the parking lot. The answer is 5.\n",
      "\n",
      "Q: Mimi picked up 2 dozen seashells on the beach. Kyle found twice\n",
      "\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n",
      "<|im_start|> system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      " <|im_start|> user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      " <|im_start|> assistant\n",
      "<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
      "Q: Frankie has six more snakes than cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. To find the total number of pets, we need to determine the number of cats, snakes, and parrots.\n",
      "\n",
      "Let's assume the number of cats is C.\n",
      "Then the number of snakes is C + 6.\n",
      "And the number of parrots is C - 1.\n",
      "\n",
      "Since six pets have four legs, which means 6 pets = 24 legs (6 pets * 4 legs/pet).\n",
      "And since there are 2 dogs, which means 2 dogs = 8 legs (2 dogs * 4 legs/dog).\n",
      "\n",
      "So, the total number of legs from pets is 24 legs (6 pets * 4 legs/pet) - 8 legs (2 dogs * 4 legs/dog) = 16 legs (6 pets * 4 legs/pet - 2 dogs * 4 legs/dog).\n",
      "\n",
      "Now, we know that cats have 4 legs per cat, so the total number of legs from cats is C * 4 (C cats * 4 legs/cat).\n",
      "\n",
      "Since the total number of legs from pets and cats is the same (1\n",
      "\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n",
      "<|im_start|> system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      " <|im_start|> user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?<|im_end|>\n",
      " <|im_start|> assistant\n",
      "A: First, let's find out how many toy cars Grandpa gave Olaf. Since Grandpa gave twice as many as the uncle, and the uncle gave 6 toy cars, Grandpa gave 2 * 6 = 12 toy cars.\n",
      "\n",
      "Next, let's find out how many toy cars Dad gave Olaf. Dad gave 10 toy cars, 5 less than Mum. So, Mum gave Olaf 10 + 5 = 15 toy cars.\n",
      "\n",
      "Now, let's find out how many toy cars Auntie gave Olaf. Auntie gave Olaf 6 toy cars, 1 more than the uncle. So, the uncle gave Olaf 6 - 1 = 5 toy cars.\n",
      "\n",
      "Now, let's add up all the toy cars Olaf received as gifts: 12 (from Grandpa) + 10 (from Dad) + 6 (from Auntie) + 5 (from Uncle) = 33 toy cars.\n",
      "\n",
      "Finally, let's add the original number of toy cars in Olaf's collection (150) to the number of toy cars he received as gifts (33): 150 + 33 = 183.\n",
      "\n",
      "The answer is 183.<|im_end|>\n",
      "\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n",
      "<|im_start|> system\n",
      "Solve the following math problems, end with The answer is<|im_end|>\n",
      " <|im_start|> user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?<|im_end|>\n",
      " <|im_start|> assistant\n",
      "<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
      "Q: Emma's bank account starts with $100. She spends $8 per day for a week, so she spends 7 * 8 = $56. After spending, she has $100 - $56 = $44 left in her account. She asks for as many $5 bills as her account can give her, which is $44 / 5 = 8.8, but she can only get whole bills, so she gets 8 bills. Each bill is worth $5, so she takes out 8 * 5 = $40. The remaining amount in her account is $44 - $40 = $4. The answer is 4.<|im_end|> <|im_start|> assistant\n",
      "Q: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?\n",
      "\n",
      "A: Emma spends $8 per day for a week, so she spends 7 * 8 = $56. After spending, she has $100 - $56 = $44 left in her account. She asks for as\n",
      "\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for out in res[1]['model_out']['outputs']:\n",
    "    print(out)\n",
    "    print('\\n\\n', '---' * 10, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gsm8k_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgsm8k_dataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gsm8k_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(samples = 10)\n",
    "    sampled_dataset = gsm8k_dataset['train'].shuffle(seed=42).select(range(samples))\n",
    "    return sampled_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_model_with_hf_backend(model_id, loading_args, tokenizer_args):\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, **loading_args)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id, *tokenizer_args)\n",
    "\n",
    "    model = Exllamav2HF.from_pretrained(pretrained_model_name_or_path = model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, legacy=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_input(tokenizer,question):\n",
    "    text = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "\n",
    "    pt = langchain.PromptTemplate.from_template(text)\n",
    "    formatted_pt = pt.format(question=question)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly assistant who can solve math problems\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": formatted_pt},\n",
    "    ]\n",
    "\n",
    "    ct_sample = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n",
    "    return ct_sample\n",
    "\n",
    "\n",
    "def driver(model_id, loading_args, tokenizer_args, gen_params1):\n",
    "\n",
    "    cm()\n",
    "\n",
    "    print(model_id)\n",
    "    print(f\"Model loading args - {loading_args}\")\n",
    "    print(f\"tokenizer args - {tokenizer_args}\\n\")\n",
    "\n",
    "    start = time.time()\n",
    "    model, tokenizer = load_model_with_hf_backend(model_id, loading_args, tokenizer_args)\n",
    "    loading_time = (time.time() - start)\n",
    "\n",
    "\n",
    "    avg_tps = []\n",
    "\n",
    "    seed = 42\n",
    "    seed_everything(seed)\n",
    "\n",
    "    detail_dict = {\n",
    "        'model_loading_time' : loading_time,\n",
    "        'input_args' : {\n",
    "            'model_id' : model_id,\n",
    "            'loading_args' : loading_args,\n",
    "            'tokenizer_args' : tokenizer_args,\n",
    "            'seed' : seed,\n",
    "        },\n",
    "        'out' : [],\n",
    "        'inf_latency' : [],\n",
    "        'p_tokens' : [],\n",
    "        'c_tokens' : [],\n",
    "        'tps' : [],\n",
    "    }\n",
    "\n",
    "\n",
    "    for idx, item in tqdm(enumerate(sampled_dataset)):\n",
    "        prepared_example = prepare_input(tokenizer,item['question'])\n",
    "        start = time.time()\n",
    "        out, prompt_tokens, c_tokens = perform_single_example_inference(prepared_example, model,tokenizer, gen_params1)\n",
    "        inf_latency = (time.time() - start)\n",
    "\n",
    "        detail_dict['out'].append(out)\n",
    "        detail_dict['inf_latency'].append(inf_latency)\n",
    "        detail_dict['p_tokens'].append(prompt_tokens)\n",
    "        detail_dict['c_tokens'].append(c_tokens)\n",
    "\n",
    "        tps = c_tokens/inf_latency\n",
    "        detail_dict['tps'].append(tps)\n",
    "\n",
    "        # record post warmup (3 eg)\n",
    "        if idx > 2:\n",
    "            avg_tps.append(tps)\n",
    "            print(f\"idx  - {idx} , tps - {tps}\\n\")\n",
    "\n",
    "            print('\\n\\n')\n",
    "            print('----' * 10)\n",
    "            print('\\n\\n')\n",
    "\n",
    "\n",
    "    a_tps = sum(avg_tps)/len(avg_tps)\n",
    "    print(f\"Average tps - {a_tps}\")\n",
    "\n",
    "    print('\\n', '----' * 15, '\\n\\n')\n",
    "\n",
    "    detail_dict['average_tps'] = a_tps\n",
    "\n",
    "    return detail_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/capybarahermes-2.5-gptq/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ\n",
      "Model loading args - {'device_map': {'': 0}}\n",
      "tokenizer args - {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cbe56a41fa4ecda70282deb4500b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens - 242\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells.\n",
      "Kyle found twice as many shells as Mimi, so he found 24 * 2 = 48 seashells.\n",
      "Leigh grabbed one-third of the shells that Kyle found, so she grabbed 48 / 3 = 16 seashells.\n",
      "The answer is 16.<|im_end|>\n",
      "Completion Tokens - 100\n",
      "Prompt tokens - 239\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\n",
      "\n",
      "We know that s = x + 6 (Frankie has six more snakes than cats).\n",
      "We also know that p = x - 1 (Frankie has one less parrot than cats).\n",
      "\n",
      "We know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\n",
      "\n",
      "Now, we can substitute the expressions for x, s, and p from the previous equations into this equation:\n",
      "\n",
      "x + (x + 6) + 2 = 6\n",
      "2x + 6 + 2 = 6\n",
      "2x + 8 = 6\n",
      "\n",
      "Now, we can solve for x:\n",
      "\n",
      "2x = 6 - 8\n",
      "2x = -2\n",
      "x = -1\n",
      "\n",
      "Since we can't have a negative number of cats, there must be an error in the problem statement. Please check the problem statement and provide the correct information.<|im_end|>\n",
      "Completion Tokens - 259\n",
      "Prompt tokens - 287\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Let's first find out how many toy cars Grandpa gave Olaf. Since Grandpa gave Olaf twice as many as Uncle, and Uncle gave Olaf 6 toy cars, Grandpa gave Olaf 2 * 6 = 12 toy cars.\n",
      "\n",
      "Now let's find out how many toy cars Dad gave Olaf. Dad gave Olaf 10 toy cars, 5 less than Mum. So Mum gave Olaf 10 + 5 = 15 toy cars.\n",
      "\n",
      "Now we know the number of toy cars each family member gave Olaf: Grandpa (12), Dad (10), Mum (15), and Auntie (6).\n",
      "\n",
      "To find the total number of toy cars Olaf received, we add all these numbers together: 12 + 10 + 15 + 6 = 43.\n",
      "\n",
      "Since Olaf initially had 150 toy cars in his collection, after receiving all these gifts, he has a total of 150 + 43 = 193 toy cars. The answer is 193.<|im_end|>\n",
      "Completion Tokens - 240\n",
      "Prompt tokens - 253\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Emma spends $8 per day for a week, which is 7 days. So, she spends 8 * 7 = $56 during the week. Her account balance is $100, so she has $100 - $56 = $44 left in her account. She can get 44 / 5 = 8 $5 bills and will have $4 left in her account. The answer is $4.<|im_end|>\n",
      "Completion Tokens - 98\n",
      "idx  - 3 , tps - 69.58707267977795\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 273\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: Ezekiel hiked 10 kilometers on the first day. On the second day, he covered half the full hike distance, which is 50 kilometers / 2 = 25 kilometers. So, on the first and second days combined, he hiked 10 + 25 = 35 kilometers. To finish the hike, he needed to cover the remaining distance, which is 50 kilometers - 35 kilometers = 15 kilometers. The answer is 15.<|im_end|>\n",
      "Completion Tokens - 119\n",
      "idx  - 4 , tps - 70.73661821425965\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 295\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: James decides to build a tin house by collecting 500 tins in a week. On the first day, he collects 50 tins. On the second day, he manages to collect 3 times that number. On the third day, he collects 50 tins fewer than the number he collected on the second day. If he collects an equal number of tins on the remaining days of the week, what's the number of tins he collected each day for the rest of the week?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: On the second day, James collects 3 times the number of tins he collected on the first day, which is 3 * 50 = 150 tins. On the third day, he collects 50 tins fewer than the second day, so he collects 150 - 50 = 100 tins.\n",
      "\n",
      "In total, for the first three days, James collected 50 (first day) + 150 (second day) + 100 (third day) = 300 tins.\n",
      "\n",
      "He has 500 tins to collect in total, so he has 500 - 300 = 200 tins left to collect for the rest of the week.\n",
      "\n",
      "Since he collects an equal number of tins each day for the remaining days, he will collect 200 / 4 = 50 tins per day for the rest of the week. The answer is 50.<|im_end|>\n",
      "Completion Tokens - 222\n",
      "idx  - 5 , tps - 71.72886459483864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 242\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Don throws 3 darts.  One is a bullseye worth 50 points.  One completely missed the target, so received no points. The third was worth half the points of the bullseye.  What is the final score from these 3 throws?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: The bullseye is worth 50 points. The third dart is worth half the points of the bullseye, so it's worth 50 / 2 = 25 points. The second dart received no points. To find the final score, add the points from all three darts: 50 (bullseye) + 0 (missed) + 25 (third dart) = 75 points. The answer is 75.<|im_end|>\n",
      "Completion Tokens - 107\n",
      "idx  - 6 , tps - 71.22450619093364\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 291\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: TreQuan is throwing rocks in the river and he notices that the bigger the rock, the wider the splash. Pebbles make a splash that is a 1/4 meter wide. Rocks make a splash that is 1/2 a meter wide, and boulders create a splash that is 2 meters wide. If he tosses 6 pebbles, 3 rocks, and 2 boulders, what is the total width of the splashes he makes?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: First, we need to find the total width of the splashes made by each type of rock. For pebbles, it's 1/4 meter per pebble, and he throws 6 pebbles, so the total width for pebbles is 1/4 * 6 = 1.5 meters.\n",
      "\n",
      "For rocks, it's 1/2 a meter per rock, and he throws 3 rocks, so the total width for rocks is 1/2 * 3 = 1.5 meters.\n",
      "\n",
      "For boulders, it's 2 meters per boulder, and he throws 2 boulders, so the total width for boulders is 2 * 2 = 4 meters.\n",
      "\n",
      "Now, we add the total widths of the splashes for each type of rock: 1.5 (pebbles) + 1.5 (rocks) + 4 (boulders) = 7 meters. The answer is 7.<|im_end|>\n",
      "Completion Tokens - 221\n",
      "idx  - 7 , tps - 71.79894523987714\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 240\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Pauly is making omelets for his family. There are three dozen eggs, and he plans to use them all. Each omelet requires 4 eggs. Including himself, there are 3 people. How many omelets does each person get?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: There are 3 dozen eggs, which means there are 3 * 12 = 36 eggs. Each omelet requires 4 eggs, so with all 36 eggs, Pauly can make 36 / 4 = 9 omelets. Since there are 3 people, each person gets 9 / 3 = 3 omelets. The answer is 3.<|im_end|>\n",
      "Completion Tokens - 89\n",
      "idx  - 8 , tps - 70.14478054309568\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Prompt tokens - 269\n",
      "<|im_start|>system\n",
      "You are a friendly assistant who can solve math problems<|im_end|>\n",
      "<|im_start|>user\n",
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
      "\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
      "\n",
      "Q: Thomas made 4 stacks of wooden blocks. The first stack was 7 blocks tall. The second stack was 3 blocks taller than the first. The third stack was 6 blocks shorter than the second stack, and the fourth stack was 10 blocks taller than the third stack. If the fifth stack has twice as many blocks as the second stack, how many blocks did Thomas use in all?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: The first stack has 7 blocks. The second stack is 3 blocks taller, so it has 7 + 3 = 10 blocks. The third stack is 6 blocks shorter, so it has 10 - 6 = 4 blocks. The fourth stack is 10 blocks taller, so it has 4 + 10 = 14 blocks. The fifth stack has twice as many blocks as the second stack, so it has 10 * 2 = 20 blocks. To find the total number of blocks, add all the blocks together: 7 + 10 + 4 + 14 + 20 = 55. The answer is 55.<|im_end|>\n",
      "Completion Tokens - 155\n",
      "idx  - 9 , tps - 71.73252604276718\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Average tps - 70.99333050079284\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([SingleTokenStoppingCriteria(token_id=32000)])\n",
    "model_id = \"/workspace/capybarahermes-2.5-gptq/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
    "loading_args = {\n",
    "    'device_map' : {\"\" : 0},\n",
    "}\n",
    "tokenizer_args = {\n",
    "\n",
    "}\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'do_sample' : False,\n",
    "}\n",
    "detail_dict = driver(model_id, loading_args, tokenizer_args, gen_params1)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('****' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_loading_time': 1.8519587516784668,\n",
       " 'input_args': {'model_id': '/workspace/capybarahermes-2.5-gptq/TheBloke_CapybaraHermes-2.5-Mistral-7B-GPTQ',\n",
       "  'loading_args': {'device_map': {'': 0}},\n",
       "  'tokenizer_args': {},\n",
       "  'seed': 42},\n",
       " 'out': ['<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?<|im_end|>\\n<|im_start|>assistant\\nA: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells.\\nKyle found twice as many shells as Mimi, so he found 24 * 2 = 48 seashells.\\nLeigh grabbed one-third of the shells that Kyle found, so she grabbed 48 / 3 = 16 seashells.\\nThe answer is 16.<|im_end|>',\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Frankie's parents let him have many pets. He has six more snakes than he has cats. He has one less parrot than cats. Six of his pets have four legs. He has 2 dogs. How many pets does he have in total?<|im_end|>\\n<|im_start|>assistant\\nA: Let's use variables to represent the number of each type of pet. Let x be the number of cats, s be the number of snakes, and p be the number of parrots.\\n\\nWe know that s = x + 6 (Frankie has six more snakes than cats).\\nWe also know that p = x - 1 (Frankie has one less parrot than cats).\\n\\nWe know that 6 pets have four legs, and these are the cats, snakes, and dogs. So, x + s + 2 (the number of dogs) = 6.\\n\\nNow, we can substitute the expressions for x, s, and p from the previous equations into this equation:\\n\\nx + (x + 6) + 2 = 6\\n2x + 6 + 2 = 6\\n2x + 8 = 6\\n\\nNow, we can solve for x:\\n\\n2x = 6 - 8\\n2x = -2\\nx = -1\\n\\nSince we can't have a negative number of cats, there must be an error in the problem statement. Please check the problem statement and provide the correct information.<|im_end|>\",\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts?<|im_end|>\\n<|im_start|>assistant\\nA: Let's first find out how many toy cars Grandpa gave Olaf. Since Grandpa gave Olaf twice as many as Uncle, and Uncle gave Olaf 6 toy cars, Grandpa gave Olaf 2 * 6 = 12 toy cars.\\n\\nNow let's find out how many toy cars Dad gave Olaf. Dad gave Olaf 10 toy cars, 5 less than Mum. So Mum gave Olaf 10 + 5 = 15 toy cars.\\n\\nNow we know the number of toy cars each family member gave Olaf: Grandpa (12), Dad (10), Mum (15), and Auntie (6).\\n\\nTo find the total number of toy cars Olaf received, we add all these numbers together: 12 + 10 + 15 + 6 = 43.\\n\\nSince Olaf initially had 150 toy cars in his collection, after receiving all these gifts, he has a total of 150 + 43 = 193 toy cars. The answer is 193.<|im_end|>\",\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Emma's bank account has $100 in it. Each day of the week, she spends $8. At the end of the week, she goes to the bank and asks for as many $5 bills as her account can give her. She leaves the rest in the account. How many dollars remain in the account?<|im_end|>\\n<|im_start|>assistant\\nA: Emma spends $8 per day for a week, which is 7 days. So, she spends 8 * 7 = $56 during the week. Her account balance is $100, so she has $100 - $56 = $44 left in her account. She can get 44 / 5 = 8 $5 bills and will have $4 left in her account. The answer is $4.<|im_end|>\",\n",
       "  '<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Ezekiel hikes as a hobby. This past summer, he did a challenging three-day hike across 50 kilometers of wilderness. The first day, he covered 10 kilometers of steep mountainside. The second day was flatter and he was able to cover half the full hike distance. How many kilometers did he have to hike on the third day to finish the hike?<|im_end|>\\n<|im_start|>assistant\\nA: Ezekiel hiked 10 kilometers on the first day. On the second day, he covered half the full hike distance, which is 50 kilometers / 2 = 25 kilometers. So, on the first and second days combined, he hiked 10 + 25 = 35 kilometers. To finish the hike, he needed to cover the remaining distance, which is 50 kilometers - 35 kilometers = 15 kilometers. The answer is 15.<|im_end|>',\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: James decides to build a tin house by collecting 500 tins in a week. On the first day, he collects 50 tins. On the second day, he manages to collect 3 times that number. On the third day, he collects 50 tins fewer than the number he collected on the second day. If he collects an equal number of tins on the remaining days of the week, what's the number of tins he collected each day for the rest of the week?<|im_end|>\\n<|im_start|>assistant\\nA: On the second day, James collects 3 times the number of tins he collected on the first day, which is 3 * 50 = 150 tins. On the third day, he collects 50 tins fewer than the second day, so he collects 150 - 50 = 100 tins.\\n\\nIn total, for the first three days, James collected 50 (first day) + 150 (second day) + 100 (third day) = 300 tins.\\n\\nHe has 500 tins to collect in total, so he has 500 - 300 = 200 tins left to collect for the rest of the week.\\n\\nSince he collects an equal number of tins each day for the remaining days, he will collect 200 / 4 = 50 tins per day for the rest of the week. The answer is 50.<|im_end|>\",\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Don throws 3 darts.  One is a bullseye worth 50 points.  One completely missed the target, so received no points. The third was worth half the points of the bullseye.  What is the final score from these 3 throws?<|im_end|>\\n<|im_start|>assistant\\nA: The bullseye is worth 50 points. The third dart is worth half the points of the bullseye, so it's worth 50 / 2 = 25 points. The second dart received no points. To find the final score, add the points from all three darts: 50 (bullseye) + 0 (missed) + 25 (third dart) = 75 points. The answer is 75.<|im_end|>\",\n",
       "  \"<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: TreQuan is throwing rocks in the river and he notices that the bigger the rock, the wider the splash. Pebbles make a splash that is a 1/4 meter wide. Rocks make a splash that is 1/2 a meter wide, and boulders create a splash that is 2 meters wide. If he tosses 6 pebbles, 3 rocks, and 2 boulders, what is the total width of the splashes he makes?<|im_end|>\\n<|im_start|>assistant\\nA: First, we need to find the total width of the splashes made by each type of rock. For pebbles, it's 1/4 meter per pebble, and he throws 6 pebbles, so the total width for pebbles is 1/4 * 6 = 1.5 meters.\\n\\nFor rocks, it's 1/2 a meter per rock, and he throws 3 rocks, so the total width for rocks is 1/2 * 3 = 1.5 meters.\\n\\nFor boulders, it's 2 meters per boulder, and he throws 2 boulders, so the total width for boulders is 2 * 2 = 4 meters.\\n\\nNow, we add the total widths of the splashes for each type of rock: 1.5 (pebbles) + 1.5 (rocks) + 4 (boulders) = 7 meters. The answer is 7.<|im_end|>\",\n",
       "  '<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Pauly is making omelets for his family. There are three dozen eggs, and he plans to use them all. Each omelet requires 4 eggs. Including himself, there are 3 people. How many omelets does each person get?<|im_end|>\\n<|im_start|>assistant\\nA: There are 3 dozen eggs, which means there are 3 * 12 = 36 eggs. Each omelet requires 4 eggs, so with all 36 eggs, Pauly can make 36 / 4 = 9 omelets. Since there are 3 people, each person gets 9 / 3 = 3 omelets. The answer is 3.<|im_end|>',\n",
       "  '<|im_start|>system\\nYou are a friendly assistant who can solve math problems<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: Thomas made 4 stacks of wooden blocks. The first stack was 7 blocks tall. The second stack was 3 blocks taller than the first. The third stack was 6 blocks shorter than the second stack, and the fourth stack was 10 blocks taller than the third stack. If the fifth stack has twice as many blocks as the second stack, how many blocks did Thomas use in all?<|im_end|>\\n<|im_start|>assistant\\nA: The first stack has 7 blocks. The second stack is 3 blocks taller, so it has 7 + 3 = 10 blocks. The third stack is 6 blocks shorter, so it has 10 - 6 = 4 blocks. The fourth stack is 10 blocks taller, so it has 4 + 10 = 14 blocks. The fifth stack has twice as many blocks as the second stack, so it has 10 * 2 = 20 blocks. To find the total number of blocks, add all the blocks together: 7 + 10 + 4 + 14 + 20 = 55. The answer is 55.<|im_end|>'],\n",
       " 'inf_latency': [1.7479984760284424,\n",
       "  3.583725929260254,\n",
       "  3.3465399742126465,\n",
       "  1.4083075523376465,\n",
       "  1.6822969913482666,\n",
       "  3.0949883460998535,\n",
       "  1.5022919178009033,\n",
       "  3.0780396461486816,\n",
       "  1.2688043117523193,\n",
       "  2.1608049869537354],\n",
       " 'p_tokens': [242, 239, 287, 253, 273, 295, 242, 291, 240, 269],\n",
       " 'c_tokens': [100, 259, 240, 98, 119, 222, 107, 221, 89, 155],\n",
       " 'tps': [57.20828786258785,\n",
       "  72.27115162053207,\n",
       "  71.71586230834303,\n",
       "  69.58707267977795,\n",
       "  70.73661821425965,\n",
       "  71.72886459483864,\n",
       "  71.22450619093364,\n",
       "  71.79894523987714,\n",
       "  70.14478054309568,\n",
       "  71.73252604276718],\n",
       " 'average_tps': 70.99333050079284}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_dump(ob : dict, file_path: Path):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ob, json_file, indent=4)\n",
    "\n",
    "idx = 1\n",
    "file_path = f\"./{model_id.split('/')[-1]}-{idx}-inf-time.json\"\n",
    "json_dump(detail_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
