{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "llmsearch on gsm8k awq with batch size 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import langchain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPTQConfig, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from llmsearch.model_downloader import download_model_from_hf\n",
    "from llmsearch.utils.model_utils import batcher, decoder_parser\n",
    "\n",
    "import awq\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0+cu121', '0.2.4')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')\n",
    "\n",
    "torch.__version__, awq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# loaders\n",
    "\n",
    "def load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_kwargs,temp_model_dir, model_branch = \"main\"):\n",
    "    output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = model_branch)\n",
    "\n",
    "    model_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "    tokenizer_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "\n",
    "    model_name_or_path = model_loader_kwargs.pop('pretrained_model_name_or_path')\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=model_name_or_path,\n",
    "        **model_loader_kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    # pad token is null in config -https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ/blob/eb64c310c44905321d012962db9ac0d47c3a64fa/tokenizer_config.json#L53\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_loader_backend_map = {\n",
    "    # \"exllama_2_hf\": load_model_with_exllama_2_hf_backend,\n",
    "    # \"hf\": load_model_with_hf_backend,\n",
    "    # 'auto_gptq' : load_model_with_autogptq_backend,\n",
    "    'awq' : load_model_with_awq_backend,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists in /workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ. Checking the model files...\n",
      "Checksum validated: model.safetensors  645dfc7f09074aaf25e642f3c6a4f7ea399a0ff2605fa650e4e74078832546de\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  6.39it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 51.82it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "\n",
    "temp_model_dir = Path(f\"/workspace/temp_model_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "model_loader_kwargs = {\n",
    "    'device_map' : {'' : 0},\n",
    "    'fuse_layers' : True,\n",
    "}\n",
    "\n",
    "tokenizer_loader_kwargs = {\n",
    "    'use_fast' : False,\n",
    "    'legacy' : False,\n",
    "    'padding_side' : 'left',\n",
    "}\n",
    "\n",
    "model, tokenizer = load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_loader_kwargs,temp_model_dir, model_branch = \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "# Add prompt template\n",
    "processed_dataset = preprocess_dataset(gsm8k_dataset['train'], tokenizer,pt = pt, pt_cols = pt_cols, system_prompt = system_prompt, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer_from_out(s):\n",
    "    pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "    match = pattern.search(s)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t['answer'].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug, set_verbosity_warning\n",
    "set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "bm_sample_size = 150\n",
    "bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "# TODO : bs = 16 error, figure this out\n",
    "# TODO : run llmsearch\n",
    "\n",
    "class MultiTokenEOSCriteria2(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\n",
    "\n",
    "    This code is not thread safe. The same object cannot be used simultaneously in multiple threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_ids : List[int],\n",
    "    ) -> None:\n",
    "        self.sequence_ids = torch.tensor(sequence_ids, dtype = torch.int32, device = \"cuda:0\")\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = self.sequence_ids.shape[0] + 2\n",
    "        self.state_initialized = False\n",
    "        self.prompt_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "    def set_state(self, batch_size, prompt_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.prompt_length = prompt_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.state_initialized = True\n",
    "\n",
    "    def reset(self):\n",
    "        # print(\"Resetting\")\n",
    "        self.batch_size = None\n",
    "        self.prompt_length = None\n",
    "        self.state_initialized = False\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        \"\"\"\n",
    "        This is called after a new token is generated\n",
    "        \"\"\"\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "\n",
    "        ret_val = False\n",
    "\n",
    "        if not self.state_initialized:\n",
    "            # Every batch should set this state\n",
    "            # print(f\"Setting state, batch_size - {input_ids.shape[0]}, batch prompt length - {input_ids.shape[1] - 1}\")\n",
    "            self.set_state(input_ids.shape[0], input_ids.shape[1] - 1)\n",
    "\n",
    "        # IDs of all the tokens except the prompt\n",
    "        lookback_ids_batch = input_ids[:, self.prompt_length :]\n",
    "        # look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        # print(f\"Current input length - {input_ids.shape[1]}, completion length - {abs(self.prompt_length - input_ids.shape[1])}\")\n",
    "        # print(f\"Current input - {tokenizer.batch_decode(input_ids, **{'spaces_between_special_tokens' : False})}\")\n",
    "\n",
    "        # no elements yet to look back\n",
    "        if lookback_ids_batch.nelement() == 0:\n",
    "            return False\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                # look back only as far as the last token of the stop sequence\n",
    "                # print(len(self.done_tracker), lookback_ids_batch.shape, self.batch_size, self.prompt_length)\n",
    "                self.done_tracker[i] = self.sequence_ids == lookback_ids_batch[i][-(self.sequence_ids.shape[0]):]\n",
    "        ret_val = False not in self.done_tracker\n",
    "        if ret_val:\n",
    "            # ASSUMPTION: Relies on the assumption that generation will only stop when the stop token is generated\n",
    "            self.reset()\n",
    "        return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f5c72ea8bb4f348b378fcb2f3c7ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_token_stop_criteria_ob = MultiTokenEOSCriteria2(sequence_ids = [32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "\n",
    "batch_size = 1\n",
    "tuner_ob = Tuner(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset = bm_samples,\n",
    "    device = 'cuda:0',\n",
    "    batch_size = batch_size,\n",
    "    tokenizer_encoding_kwargs={'padding': 'longest', 'add_special_tokens' : False},\n",
    "    tokenizer_decoding_kwargs={'spaces_between_special_tokens' : False},\n",
    "    scorer = get_score,\n",
    "    prompt_template = langchain.PromptTemplate.from_template(\"{X}\"),\n",
    "    is_encoder_decoder = False,\n",
    "    seed = seed,\n",
    "    column_mapping = {'input_cols' : [\"X\"],'eval_cols' : ['answer']},\n",
    "    callbacks_after_inference = [multi_token_stop_criteria_ob.reset],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45e1864ddaf4dfb9288795aec22690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    # max_new_tokens take precendece over stopping criteria\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7133333333333334\n"
     ]
    }
   ],
   "source": [
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "hyp_space = {\n",
    "    'max_new_tokens' : [500],\n",
    "    'stopping_criteria' : [stopping_criteria],\n",
    "    'generation_seed' : [42],\n",
    "    'do_sample' : [True],\n",
    "\n",
    "    # 'temperature': [0.8,0.9,1.0],  # Continuous distribution from 0.1 to 1.0\n",
    "    'top_k': [10,50,60,70,80],  # Discrete uniform distribution from 1 to 100\n",
    "    'top_p' : [0.7,0.75,0.8,0.95],\n",
    "    'no_repeat_ngram_size': [0],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "scorer = make_scorer(score_func=get_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "clf = GridSearchCV(\n",
    "    estimator = tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring = scorer,\n",
    "    cv = 2,\n",
    "    n_jobs = None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fafd7be5ca644378b7b72c18ccf9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.7;, score=0.707 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98547cd46ef449d9192606741b8518e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.7;, score=0.693 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0816db7e09714737b7f03efd695ec358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.75;, score=0.667 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4adfcb27c3443bb05f86cc7d6efda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.75;, score=0.747 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909fbad0f6084523962231fdf90482d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.8;, score=0.733 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3478eec335c48e897f32d1ca7ebab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.8;, score=0.707 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962b23d2c3b84f7eb74a1e9999b63b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.95;, score=0.747 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4460f6e597046d5b13be08347dd8317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=10, top_p=0.95;, score=0.680 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c32285440d4ec0a7769159f063c512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.7;, score=0.720 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd81497fdf21415fba95784f50052c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.7;, score=0.693 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34bc94c0ed14dba832aac815a0eb1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.75;, score=0.640 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9850a5bae93248a89b5d9f454a261b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.75;, score=0.720 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e951ba2f064680ae496bd88669d550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.8;, score=0.680 total time= 3.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b58d26580a2451eaf8600864c00cec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.8;, score=0.733 total time= 3.5min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5f662b9e1a43d1881878660713ed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.95;, score=0.720 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b48f8f82f54cc2bb84597d74747e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=50, top_p=0.95;, score=0.653 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf75df6f746845028b218f36ba5ade31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.7;, score=0.720 total time= 3.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1594042d642e4a659f558cf4a926d9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.7;, score=0.693 total time= 3.5min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68c43ab3880468ea49db86148287df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.75;, score=0.640 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcd26f660834617bd8517d58fb5b41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.75;, score=0.720 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51ba6370fdd4731b3d8d079c290a3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.8;, score=0.680 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ab53f9ec5c405fb6ff3f7782ce305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.8;, score=0.733 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2593f72361049919ec133b790528556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.95;, score=0.720 total time= 4.2min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c468b2d006b8475bae0193fe82bd1373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=60, top_p=0.95;, score=0.653 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4c8265627a4c35864da501b320ce66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.7;, score=0.720 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344bcc8fb07b4618aed0d994a463f251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.7;, score=0.693 total time= 3.5min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa672a9611e543bdbcee6c4a6c6dc5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.75;, score=0.640 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c55e6ea21f74af7bff2924c7a20189b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.75;, score=0.720 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da75e159fb141478cab2959bbbc5fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.8;, score=0.680 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e75a01bd4a4ab3b80bf595ff9e36d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.8;, score=0.733 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3d0c583d754c6d95678b6270c17fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.95;, score=0.720 total time= 4.1min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d97f952fafe4c17b39969d74f1f71af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=70, top_p=0.95;, score=0.653 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddd1688e41e42e1ac7f1a8274a3a17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.7;, score=0.720 total time= 3.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7599bcb0b204c0495be18b57d7c6e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.7;, score=0.693 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa57034a8ae145be9e587cbe8a4cca22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.75;, score=0.640 total time= 3.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804c9f6de2d44788313e5ef71716284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.75;, score=0.720 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ee409dc7c2486daae8233fbdae83d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.8;, score=0.680 total time= 3.9min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ae3e320495495a978be29178d414b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.8;, score=0.733 total time= 3.5min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a6e65e653b4ce680add8ad01391174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.95;, score=0.720 total time= 4.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cfe5a0185b470db3631d7634e5cf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], top_k=80, top_p=0.95;, score=0.653 total time= 3.6min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenEOSCriteria2.reset of &lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;,\n",
       "                                           disable_batch_size_cache=False,\n",
       "                                           do_sample=True, generation_seed=42,\n",
       "                                           is_encoder_decoder=False,\n",
       "                                           is_fitted_=True, max_new_tokens=500,\n",
       "                                           model=MistralAWQForCa...\n",
       "                                           tokenizer_encoding_kwargs={&#x27;add_special_tokens&#x27;: False,\n",
       "                                                                      &#x27;padding&#x27;: &#x27;longest&#x27;},\n",
       "                                           top_k=10, top_p=0.8),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [500], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;]],\n",
       "                         &#x27;top_k&#x27;: [10, 50, 60, 70, 80],\n",
       "                         &#x27;top_p&#x27;: [0.7, 0.75, 0.8, 0.95]},\n",
       "             scoring=make_scorer(get_score, response_method=&#x27;predict&#x27;),\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenEOSCriteria2.reset of &lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;,\n",
       "                                           disable_batch_size_cache=False,\n",
       "                                           do_sample=True, generation_seed=42,\n",
       "                                           is_encoder_decoder=False,\n",
       "                                           is_fitted_=True, max_new_tokens=500,\n",
       "                                           model=MistralAWQForCa...\n",
       "                                           tokenizer_encoding_kwargs={&#x27;add_special_tokens&#x27;: False,\n",
       "                                                                      &#x27;padding&#x27;: &#x27;longest&#x27;},\n",
       "                                           top_k=10, top_p=0.8),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [500], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;]],\n",
       "                         &#x27;top_k&#x27;: [10, 50, 60, 70, 80],\n",
       "                         &#x27;top_p&#x27;: [0.7, 0.75, 0.8, 0.95]},\n",
       "             scoring=make_scorer(get_score, response_method=&#x27;predict&#x27;),\n",
       "             verbose=3)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content fitted\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenEOSCriteria2.reset of &lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, disable_batch_size_cache=False,\n",
       "                    do_sample=True, generation_seed=42,\n",
       "                    is_encoder_decoder=False, is_fitted_=True,\n",
       "                    max_new_tokens=500,\n",
       "                    model=MistralAWQForCausalLM(\n",
       "  (model): MistralForCau...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "},\n",
       "                    tokenizer_decoding_kwargs={&#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encoding_kwargs={&#x27;add_special_tokens&#x27;: False,\n",
       "                                               &#x27;padding&#x27;: &#x27;longest&#x27;},\n",
       "                    top_k=10, top_p=0.8)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content fitted\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenEOSCriteria2.reset of &lt;__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, disable_batch_size_cache=False,\n",
       "                    do_sample=True, generation_seed=42,\n",
       "                    is_encoder_decoder=False, is_fitted_=True,\n",
       "                    max_new_tokens=500,\n",
       "                    model=MistralAWQForCausalLM(\n",
       "  (model): MistralForCau...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "},\n",
       "                    tokenizer_decoding_kwargs={&#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encoding_kwargs={&#x27;add_special_tokens&#x27;: False,\n",
       "                                               &#x27;padding&#x27;: &#x27;longest&#x27;},\n",
       "                    top_k=10, top_p=0.8)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[<bound method MultiTokenEOSCriteria2.reset of <__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>>],\n",
       "                                           device='cuda:0',\n",
       "                                           disable_batch_size_cache=False,\n",
       "                                           do_sample=True, generation_seed=42,\n",
       "                                           is_encoder_decoder=False,\n",
       "                                           is_fitted_=True, max_new_tokens=500,\n",
       "                                           model=MistralAWQForCa...\n",
       "                                           tokenizer_encoding_kwargs={'add_special_tokens': False,\n",
       "                                                                      'padding': 'longest'},\n",
       "                                           top_k=10, top_p=0.8),\n",
       "             param_grid={'do_sample': [True], 'generation_seed': [42],\n",
       "                         'max_new_tokens': [500], 'no_repeat_ngram_size': [0],\n",
       "                         'stopping_criteria': [[<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>]],\n",
       "                         'top_k': [10, 50, 60, 70, 80],\n",
       "                         'top_p': [0.7, 0.75, 0.8, 0.95]},\n",
       "             scoring=make_scorer(get_score, response_method='predict'),\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"X\"], y=tuner_ob.dataset['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8489b2e67e6244a99322bf7facd1d099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_after, outputs_after = tuner_ob.get_score(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7133333333333334 0.7266666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(scores_before, scores_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107.0, 109.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bm_samples) * scores_before, len(bm_samples) * scores_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bm_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 500, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<__main__.MultiTokenEOSCriteria2 object at 0x7f599c74b520>], 'top_k': 10, 'top_p': 0.8}\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : evaluate on a bigger sample, currently evaluated on 150 size sample\n",
    "str(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_dump(ob : dict, file_path: Path):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ob, json_file, indent=4)\n",
    "\n",
    "d = {\n",
    "    'scores_before' : scores_before,\n",
    "    'scores_after' : scores_after,\n",
    "    'outputs_before' : outputs_before,\n",
    "    'outputs_after' : outputs_after,\n",
    "    'best_params' : str(clf.best_params_),\n",
    "}\n",
    "\n",
    "f = \"./results-gsm8k-capybara-bs-1.json\"\n",
    "json_dump(d, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
