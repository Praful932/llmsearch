{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n",
      "Model already exists in temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ. Checking the model files...\n",
      "Checksum validated: model.safetensors  645dfc7f09074aaf25e642f3c6a4f7ea399a0ff2605fa650e4e74078832546de\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.28it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.99it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During init - [<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to test functionality of llmsearch\n",
    "Works with /archive/runpod_dev_env_setup.sh\n",
    "\n",
    "Does not do any kind of exhaustive search\n",
    "\n",
    "Requires - pip install autoawq==0.2.4 autoawq_kernels==0.0.6\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspace/llmsearch/\")\n",
    "\n",
    "import datasets\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import StoppingCriteriaList, AutoTokenizer\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from llmsearch.utils.logging_utils import set_verbosity_debug, set_verbosity_info\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria\n",
    "\n",
    "set_verbosity_debug()\n",
    "\n",
    "def print_lines(n = 2):\n",
    "    print(\"\\n\" * n)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "batch_size = 1\n",
    "bm_sample_size = 5\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def load_model_and_tokenizer(model_id, temp_model_dir):\n",
    "    temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n",
    "\n",
    "    gc_cuda()\n",
    "\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=output_folder, fuse_layers=True, device_map={\"\": device}, local_files_only=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        output_folder, local_files_only=True, legacy=False, use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    def preprocess_dataset(\n",
    "        dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt=True\n",
    "    ):\n",
    "\n",
    "        def wrapper(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            messages = (\n",
    "                []\n",
    "                if system_prompt is None\n",
    "                else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            )\n",
    "            formatted_pt = pt.format(**{pt_col: sample[pt_col] for pt_col in pt_cols})\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": formatted_pt,\n",
    "                }\n",
    "            )\n",
    "            formatted_pt_with_ct = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "            return formatted_pt_with_ct\n",
    "\n",
    "        def actual_input(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            return sample[pt_cols[0]]\n",
    "\n",
    "        pt_dataset = dataset.map(\n",
    "            lambda sample: {\n",
    "                \"X\": wrapper(sample),\n",
    "                \"actual input\": actual_input(sample),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return pt_dataset\n",
    "\n",
    "\n",
    "    # 2-shot prompt template - https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n",
    "    pt = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\"\n",
    "    )\n",
    "    pt_cols = [\"question\"]\n",
    "    system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "    gsm8k_dataset = datasets.load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "\n",
    "    processed_dataset = preprocess_dataset(\n",
    "        gsm8k_dataset[\"train\"],\n",
    "        tokenizer,\n",
    "        pt=pt,\n",
    "        pt_cols=pt_cols,\n",
    "        system_prompt=system_prompt,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    bm_samples = processed_dataset.shuffle(seed=seed).select(range(bm_sample_size))\n",
    "    return bm_samples\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    def extract_answer_from_out(s):\n",
    "        pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "        match = pattern.search(s)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t[\"answer\"].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "        print(\"y_pred - \", y_p_answer)\n",
    "        print(\"y_true - \", y_t_answer)\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Load Model, Tokenizer, Dataset\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n",
    "\n",
    "bm_samples = load_dataset()\n",
    "\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "print(f\"During init - {callbacks_after_inference}\") # 0x7fa49e1d9c90, 0x7fa49049f0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:42:10.061 - llmsearch.tuner.tuner:89 - DEBUG - Initializing new estimator with generation parameters - {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During estimator init - [<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]\n"
     ]
    }
   ],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=bm_samples,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\", \"add_special_tokens\": False},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False},\n",
    "    scorer=get_score,\n",
    "    prompt_template=\"{X}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"X\"], \"eval_cols\": [\"answer\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0x7f7c826d2d40\n",
    "\n",
    "tuner_ob.estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone as sklearn_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get_params\n",
      "<module> - gen_params = tuner_ob.estimator.get_params()\n"
     ]
    }
   ],
   "source": [
    "gen_params = tuner_ob.estimator.get_params()\n",
    "\n",
    "filtered_gen_params = {k: v for k, v in gen_params.items() if k in tuner_ob.estimator._model_generation_param_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'stopping_criteria' : stopping_criteria}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c826d2d40>]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c70312f50>]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different 0x7f7c70312f50 !! than 0x7f7c826d2d40\n",
    "# params are deepcopied - https://github.com/scikit-learn/scikit-learn/blob/10b5c6628630d57e3025a17381f71172d2649d8d/sklearn/base.py#L104\n",
    "# so a gen param that maintains state will not work as expected\n",
    "# solution TODO\n",
    "sklearn_clone(params, safe = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_space = {\n",
    "    \"max_new_tokens\": [500],\n",
    "    \"stopping_criteria\": [stopping_criteria],\n",
    "    \"generation_seed\": [42],\n",
    "    \"do_sample\": [True],\n",
    "    'temperature' : [0.9],\n",
    "    'top_k' : [50],\n",
    "    'top_p' : [0.95],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring=tuner_ob.scorer,\n",
    "    cv=2,\n",
    "    n_jobs=None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_new_tokens': [500],\n",
       " 'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c826d2d40>]],\n",
       " 'generation_seed': [42],\n",
       " 'do_sample': [True],\n",
       " 'temperature': [0.9],\n",
       " 'top_k': [50],\n",
       " 'top_p': [0.95]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_params['callbacks_after_inference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:33:35.736 - llmsearch.tuner.tuner:206 - DEBUG - Attributes after setting new parameters - {'model': MistralAWQForCausalLM(\n",
      "  (model): MistralForCausalLM(\n",
      "    (model): LlamaLikeModel(\n",
      "      (embedding): Embedding(32002, 4096, padding_idx=0)\n",
      "      (blocks): ModuleList(\n",
      "        (0-31): 32 x LlamaLikeBlock(\n",
      "          (norm_1): FasterTransformerRMSNorm()\n",
      "          (attn): QuantAttentionFused(\n",
      "            (qkv_proj): WQLinear_GEMM(in_features=4096, out_features=6144, bias=False, w_bit=4, group_size=128)\n",
      "            (o_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
      "            (rope): RoPE()\n",
      "          )\n",
      "          (norm_2): FasterTransformerRMSNorm()\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
      "            (up_proj): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
      "            (down_proj): WQLinear_GEMM(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  )\n",
      "), 'tokenizer': LlamaTokenizer(name_or_path='temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'scorer': make_scorer(get_score, response_method='predict'), 'device': 'cuda:0', 'tokenizer_encode_args': {'padding': 'longest', 'add_special_tokens': False}, 'tokenizer_decode_args': {'spaces_between_special_tokens': False}, 'batch_size': 1, 'callbacks_after_inference': [<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>], 'disable_batch_size_cache': False, 'pred_function': None, 'is_encoder_decoder': False, 'disable_generation_param_checks': False, 'output_preproc': <function Tuner.<lambda> at 0x7fa49e1cadd0>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_estimator = tuner_ob.estimator.set_params(**sklearn_clone(tuner_ob.estimator.get_params(), safe=False))\n",
    "new_estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
