{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.0  /51.0   230kiB/s\n",
      "100%|██████████| 911   /911    270kiB/s\n",
      "100%|██████████| 17.9k /17.9k  7.94MiB/s\n",
      "100%|██████████| 115   /115    499kiB/s\n",
      "100%|██████████| 420   /420    264kiB/s\n",
      "  1%|          | 21.0M /4.15G  191MiB/s\n",
      "100%|██████████| 126   /126    678kiB/s\n",
      "100%|██████████| 1.80M /1.80M  3.67MiB/s\n",
      "100%|██████████| 1.60k /1.60k  5.37MiB/s\n",
      "100%|██████████| 493k  /493k   11.7MiB/s\n",
      "100%|██████████| 4.15G /4.15G  260MiB/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m temp_model_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./temp_dir/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m temp_model_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 154\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_model_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m bm_samples \u001b[38;5;241m=\u001b[39m load_dataset()\n\u001b[1;32m    158\u001b[0m multi_token_stop_criteria_ob \u001b[38;5;241m=\u001b[39m MultiTokenStoppingCriteria(sequence_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32000\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_id, temp_model_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_and_tokenizer\u001b[39m(model_id, temp_model_dir):\n\u001b[1;32m     43\u001b[0m     temp_model_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m     output_folder \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_model_from_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     gc_cuda()\n\u001b[1;32m     48\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoAWQForCausalLM\u001b[38;5;241m.\u001b[39mfrom_quantized(\n\u001b[1;32m     49\u001b[0m         quant_path\u001b[38;5;241m=\u001b[39moutput_folder, fuse_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: device}, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/llmsearch/llmsearch/utils/model_downloader.py:329\u001b[0m, in \u001b[0;36mdownload_model_from_hf\u001b[0;34m(model_id, save_dir, branch, threads, max_retries)\u001b[0m\n\u001b[1;32m    326\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mcheck_model_files(model_id, branch, links, sha256, output_folder)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_folder\n\u001b[0;32m--> 329\u001b[0m \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msha256\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecific_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_llamacpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_llamacpp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_folder\n",
      "File \u001b[0;32m/workspace/llmsearch/llmsearch/utils/model_downloader.py:274\u001b[0m, in \u001b[0;36mModelDownloader.download_model_files\u001b[0;34m(self, model, branch, links, sha256, output_folder, progress_bar, start_from_scratch, threads, specific_file, is_llamacpp)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading the model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_download_threads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/llmsearch/llmsearch/utils/model_downloader.py:228\u001b[0m, in \u001b[0;36mModelDownloader.start_download_threads\u001b[0;34m(self, file_list, output_folder, start_from_scratch, threads)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_download_threads\u001b[39m(\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m, file_list, output_folder, start_from_scratch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    227\u001b[0m ):\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to test functionality of llmsearch\n",
    "Works with /archive/runpod_dev_env_setup.sh\n",
    "\n",
    "Does not do any kind of exhaustive search\n",
    "\n",
    "Requires - pip install autoawq==0.2.4 autoawq_kernels==0.0.6\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspace/llmsearch/\")\n",
    "\n",
    "import datasets\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import StoppingCriteriaList, AutoTokenizer\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from llmsearch.utils.logging_utils import set_verbosity_debug, set_verbosity_info\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria\n",
    "\n",
    "set_verbosity_debug()\n",
    "\n",
    "def print_lines(n = 2):\n",
    "    print(\"\\n\" * n)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "batch_size = 1\n",
    "bm_sample_size = 5\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def load_model_and_tokenizer(model_id, temp_model_dir):\n",
    "    temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n",
    "\n",
    "    gc_cuda()\n",
    "\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=output_folder, fuse_layers=True, device_map={\"\": device}, local_files_only=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        output_folder, local_files_only=True, legacy=False, use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    def preprocess_dataset(\n",
    "        dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt=True\n",
    "    ):\n",
    "\n",
    "        def wrapper(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            messages = (\n",
    "                []\n",
    "                if system_prompt is None\n",
    "                else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            )\n",
    "            formatted_pt = pt.format(**{pt_col: sample[pt_col] for pt_col in pt_cols})\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": formatted_pt,\n",
    "                }\n",
    "            )\n",
    "            formatted_pt_with_ct = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "            return formatted_pt_with_ct\n",
    "\n",
    "        def actual_input(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            return sample[pt_cols[0]]\n",
    "\n",
    "        pt_dataset = dataset.map(\n",
    "            lambda sample: {\n",
    "                \"X\": wrapper(sample),\n",
    "                \"actual input\": actual_input(sample),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return pt_dataset\n",
    "\n",
    "\n",
    "    # 2-shot prompt template - https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n",
    "    pt = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\"\n",
    "    )\n",
    "    pt_cols = [\"question\"]\n",
    "    system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "    gsm8k_dataset = datasets.load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "\n",
    "    processed_dataset = preprocess_dataset(\n",
    "        gsm8k_dataset[\"train\"],\n",
    "        tokenizer,\n",
    "        pt=pt,\n",
    "        pt_cols=pt_cols,\n",
    "        system_prompt=system_prompt,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    bm_samples = processed_dataset.shuffle(seed=seed).select(range(bm_sample_size))\n",
    "    return bm_samples\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    def extract_answer_from_out(s):\n",
    "        pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "        match = pattern.search(s)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t[\"answer\"].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "        print(\"y_pred - \", y_p_answer)\n",
    "        print(\"y_true - \", y_t_answer)\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Load Model, Tokenizer, Dataset\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n",
    "\n",
    "bm_samples = load_dataset()\n",
    "\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "print(f\"During init - {callbacks_after_inference}\") # 0x7fa49e1d9c90, 0x7fa49049f0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:42:10.061 - llmsearch.tuner.tuner:89 - DEBUG - Initializing new estimator with generation parameters - {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During estimator init - [<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]\n"
     ]
    }
   ],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=bm_samples,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\", \"add_special_tokens\": False},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False},\n",
    "    scorer=get_score,\n",
    "    prompt_template=\"{X}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"X\"], \"eval_cols\": [\"answer\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0x7f7c826d2d40\n",
    "\n",
    "tuner_ob.estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone as sklearn_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get_params\n",
      "<module> - gen_params = tuner_ob.estimator.get_params()\n"
     ]
    }
   ],
   "source": [
    "gen_params = tuner_ob.estimator.get_params()\n",
    "\n",
    "filtered_gen_params = {k: v for k, v in gen_params.items() if k in tuner_ob.estimator._model_generation_param_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.stopping_criteria.StoppingCriteriaList"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopping_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'stopping_criteria' : stopping_criteria}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c826d2d40>]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c70312f50>]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different 0x7f7c70312f50 !! than 0x7f7c826d2d40\n",
    "# params are deepcopied - https://github.com/scikit-learn/scikit-learn/blob/10b5c6628630d57e3025a17381f71172d2649d8d/sklearn/base.py#L104\n",
    "# so a gen param that maintains state will not work as expected\n",
    "# solution TODO\n",
    "sklearn_clone(params, safe = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_space = {\n",
    "    \"max_new_tokens\": [500],\n",
    "    \"stopping_criteria\": [stopping_criteria],\n",
    "    \"generation_seed\": [42],\n",
    "    \"do_sample\": [True],\n",
    "    'temperature' : [0.9],\n",
    "    'top_k' : [50],\n",
    "    'top_p' : [0.95],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring=tuner_ob.scorer,\n",
    "    cv=2,\n",
    "    n_jobs=None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_new_tokens': [500],\n",
       " 'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria at 0x7f7c826d2d40>]],\n",
       " 'generation_seed': [42],\n",
       " 'do_sample': [True],\n",
       " 'temperature': [0.9],\n",
       " 'top_k': [50],\n",
       " 'top_p': [0.95]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f7c826d2d40>>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_params['callbacks_after_inference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:33:35.736 - llmsearch.tuner.tuner:206 - DEBUG - Attributes after setting new parameters - {'model': MistralAWQForCausalLM(\n",
      "  (model): MistralForCausalLM(\n",
      "    (model): LlamaLikeModel(\n",
      "      (embedding): Embedding(32002, 4096, padding_idx=0)\n",
      "      (blocks): ModuleList(\n",
      "        (0-31): 32 x LlamaLikeBlock(\n",
      "          (norm_1): FasterTransformerRMSNorm()\n",
      "          (attn): QuantAttentionFused(\n",
      "            (qkv_proj): WQLinear_GEMM(in_features=4096, out_features=6144, bias=False, w_bit=4, group_size=128)\n",
      "            (o_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
      "            (rope): RoPE()\n",
      "          )\n",
      "          (norm_2): FasterTransformerRMSNorm()\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
      "            (up_proj): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)\n",
      "            (down_proj): WQLinear_GEMM(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  )\n",
      "), 'tokenizer': LlamaTokenizer(name_or_path='temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'scorer': make_scorer(get_score, response_method='predict'), 'device': 'cuda:0', 'tokenizer_encode_args': {'padding': 'longest', 'add_special_tokens': False}, 'tokenizer_decode_args': {'spaces_between_special_tokens': False}, 'batch_size': 1, 'callbacks_after_inference': [<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>], 'disable_batch_size_cache': False, 'pred_function': None, 'is_encoder_decoder': False, 'disable_generation_param_checks': False, 'output_preproc': <function Tuner.<lambda> at 0x7fa49e1cadd0>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_estimator = tuner_ob.estimator.set_params(**sklearn_clone(tuner_ob.estimator.get_params(), safe=False))\n",
    "new_estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fa49049f0a0>>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_estimator.callbacks_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
