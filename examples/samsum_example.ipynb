{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n",
      "0.0.14 2.2.0+cu121 4.38.2 0.1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model - https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b fintuned using LORA on samsum validation set and then quantized to GPTQ to be used via exllamav2\n",
    "dataset finetuned on - samsum validation set\n",
    "\n",
    "llmsearch example shown on - samsum train set and evaluated on samsum test set\n",
    "\n",
    "Requires:\n",
    "nltk==3.8.1\n",
    "rouge_score==0.1.2\n",
    "py7zr=0.21.0\n",
    "exllamav2@https://github.com/turboderp/exllamav2/releases/download/v0.0.14/exllamav2-0.0.14+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import llmsearch\n",
    "import exllamav2\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "print(exllamav2.__version__, torch.__version__, transformers.__version__, llmsearch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Union, List\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria\n",
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug, set_verbosity_warning\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, AutoTokenizer, StoppingCriteriaList, AutoModelForCausalLM\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1\n",
    "num_tune_samples = 1200\n",
    "num_test_samples = 800\n",
    "\n",
    "model_id = \"Praful932/dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Model related code ------\n",
    "class Exllamav2HF(PreTrainedModel):\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        if shared.args.cache_8bit:\n",
    "            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "        else:\n",
    "            self.ex_cache = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "        self.past_seq = None\n",
    "        if shared.args.cfg_cache:\n",
    "            if shared.args.cache_8bit:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "            else:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "            self.past_seq_negative = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        use_cache = kwargs.get('use_cache', True)\n",
    "        labels = kwargs.get('labels', None)\n",
    "        past_key_values = kwargs.get('past_key_values', None)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            if not shared.args.cfg_cache:\n",
    "                print(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n",
    "                return\n",
    "\n",
    "            input_ids = args[0]\n",
    "            is_negative = True\n",
    "            past_seq = self.past_seq_negative\n",
    "            ex_cache = self.ex_cache_negative\n",
    "        else:\n",
    "            input_ids = kwargs['input_ids']\n",
    "            is_negative = False\n",
    "            past_seq = self.past_seq\n",
    "            ex_cache = self.ex_cache\n",
    "\n",
    "        seq = input_ids[0].tolist()\n",
    "        if is_negative and past_key_values is not None:\n",
    "            seq = past_key_values + seq\n",
    "\n",
    "        seq_tensor = torch.tensor(seq)\n",
    "        reset = True\n",
    "\n",
    "        # Make the forward call\n",
    "        if labels is None:\n",
    "            if past_seq is not None:\n",
    "                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n",
    "                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n",
    "                if len(indices) > 0:\n",
    "                    longest_prefix = indices[0].item()\n",
    "                else:\n",
    "                    longest_prefix = min_length\n",
    "\n",
    "                if longest_prefix > 0:\n",
    "                    reset = False\n",
    "                    ex_cache.current_seq_len = longest_prefix\n",
    "                    if len(seq_tensor) - longest_prefix > 1:\n",
    "                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "                    elif len(seq_tensor) == longest_prefix:\n",
    "                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n",
    "                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n",
    "                        ex_cache.current_seq_len -= 1\n",
    "\n",
    "            if reset:\n",
    "                ex_cache.current_seq_len = 0\n",
    "                if len(seq_tensor) > 1:\n",
    "                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "\n",
    "            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n",
    "        else:\n",
    "            ex_cache.current_seq_len = 0\n",
    "            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n",
    "\n",
    "        if is_negative:\n",
    "            self.past_seq_negative = seq_tensor\n",
    "        else:\n",
    "            self.past_seq = seq_tensor\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, logits.shape[-1])\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HF(config)\n",
    "\n",
    "class Shared:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.gpu_split = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = Shared.Args()\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1\n",
    "\n",
    "\n",
    "# ------ Dataset related code ------\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=[item['summary'] for item in y_true])\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True, use_aggregator=False)\n",
    "    return np.mean(result['rouge2'])\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, hf_dataset, tokenizer, prompt_template = \"Summarize : {dialogue}\", input_key = \"\", output_key = \"\", system_prompt = \"\", add_output = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x : {\"chat_format\" : ([{'role' : \"system\", \"content\" : system_prompt}] if system_prompt else []) + [\n",
    "            {\n",
    "                'role' : \"user\", \"content\" : prompt_template.format(**{input_key : x[input_key]})\n",
    "            }\n",
    "        ] + ([{'role' : 'assistant', \"content\" : x[output_key]}] if add_output else [])})\n",
    "\n",
    "    def apply_chat_template(self, add_gen_prompt = True):\n",
    "        \"\"\"Converts the dataset to a chat based format\"\"\"\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x: {\"formatted_chat\": self.tokenizer.apply_chat_template(x[\"chat_format\"], tokenize=False, add_generation_prompt=add_gen_prompt)})\n",
    "\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_id, temp_model_dir):\n",
    "    temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n",
    "\n",
    "    gc_cuda()\n",
    "\n",
    "    model = Exllamav2HF.from_pretrained(output_folder)\n",
    "    # model = AutoGPTQForCausalLM.from_quantized(\n",
    "    #     model_name_or_path = output_folder,\n",
    "    #     device = device,\n",
    "    # )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        output_folder, local_files_only=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    # model was finetuned on val set\n",
    "\n",
    "    train_dataset = datasets.load_dataset(\"samsum\")['train']\n",
    "    train_dataset = DatasetWrapper(train_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    train_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "    train_dataset.hf_dataset = train_dataset.hf_dataset.shuffle(seed=seed)\n",
    "\n",
    "    samples_to_tune_on = train_dataset.hf_dataset.select(range(num_tune_samples))\n",
    "\n",
    "    test_dataset = datasets.load_dataset(\"samsum\")['test']\n",
    "    test_dataset = DatasetWrapper(test_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    test_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "    test_dataset.hf_dataset = test_dataset.hf_dataset.shuffle(seed=seed)\n",
    "    test_samples = test_dataset.hf_dataset.select(range(num_test_samples))\n",
    "\n",
    "    return samples_to_tune_on, test_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists in temp_dir/Praful932_dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ. Checking the model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum validated: model.safetensors  817eec4c0f73483e67516e28b499ab75f11a4639aad5ffa04c389f8f25ce2cf8\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Model, Tokenizer, Dataset\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n",
    "\n",
    "# Dataset we will use to find the best generation parameters and test samples\n",
    "samples_to_tune_on,test_dataset = load_dataset()\n",
    "\n",
    "# create stop token criteria\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=samples_to_tune_on,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\",'truncation' : True, \"add_special_tokens\": False},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n",
    "    scorer=get_rouge_score,\n",
    "    prompt_template=\"{formatted_chat}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"formatted_chat\"], \"eval_cols\": [\"summary\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner_ob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtuner_ob\u001b[49m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_X\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner_ob' is not defined"
     ]
    }
   ],
   "source": [
    "print(tuner_ob.dataset['_X'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [17:00<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "    'temperature' : 0.1,\n",
    "    'top_k' : 70,\n",
    "    'no_repeat_ngram_size' : 0,\n",
    "    'do_sample' : True,\n",
    "}\n",
    "\n",
    "s, o = tuner_ob.get_score(gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2475812543892885\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params1 = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24729290809820706\n"
     ]
    }
   ],
   "source": [
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_space = {\n",
    "    'max_new_tokens' : [70],\n",
    "    'stopping_criteria' : [stopping_criteria],\n",
    "    'generation_seed' : [42],\n",
    "    'do_sample' : [True],\n",
    "\n",
    "    'temperature': [0.1,0.3,0.5,0.7,0.9,1.0],\n",
    "    'top_k': [50,60,70,80],\n",
    "    'no_repeat_ngram_size': [0],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator = tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring = tuner_ob.scorer,\n",
    "    cv = 2,\n",
    "    n_jobs = None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 45/600 [00:35<06:16,  1.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=50;, score=0.253 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:55<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=50;, score=0.242 total time= 8.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:51<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=60;, score=0.252 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:51<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=60;, score=0.240 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:46<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=70;, score=0.247 total time= 7.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:49<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=70;, score=0.239 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:53<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=80;, score=0.255 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:47<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.1, top_k=80;, score=0.233 total time= 7.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:58<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=50;, score=0.242 total time= 8.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:54<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=50;, score=0.234 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:10<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=60;, score=0.251 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:51<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=60;, score=0.236 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:52<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=70;, score=0.243 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:38<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=70;, score=0.228 total time= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:55<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=80;, score=0.238 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:38<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.3, top_k=80;, score=0.236 total time= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:58<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=50;, score=0.221 total time= 8.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:45<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=50;, score=0.209 total time= 7.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:09<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=60;, score=0.227 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:52<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=60;, score=0.217 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:13<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=70;, score=0.221 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:56<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=70;, score=0.215 total time= 8.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:50<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=80;, score=0.226 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:05<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.5, top_k=80;, score=0.218 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:22<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=50;, score=0.206 total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:51<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=50;, score=0.184 total time= 7.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:06<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=60;, score=0.199 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:10<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=60;, score=0.190 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:32<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=70;, score=0.200 total time= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:03<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=70;, score=0.192 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:22<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=80;, score=0.200 total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.7, top_k=80;, score=0.190 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:32<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=50;, score=0.168 total time= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:18<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=50;, score=0.164 total time= 8.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:27<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=60;, score=0.163 total time= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:14<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=60;, score=0.157 total time= 8.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:21<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=70;, score=0.152 total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:33<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=70;, score=0.150 total time= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:36<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=80;, score=0.152 total time= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:27<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=0.9, top_k=80;, score=0.162 total time= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:31<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=50;, score=0.138 total time= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:10<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=50;, score=0.141 total time= 8.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:45<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=60;, score=0.143 total time= 8.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:25<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=60;, score=0.136 total time= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:41<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=70;, score=0.138 total time= 8.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:31<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=70;, score=0.143 total time= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:33<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=80;, score=0.138 total time= 8.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [08:38<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], temperature=1.0, top_k=80;, score=0.127 total time= 8.7min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=50),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         &#x27;top_k&#x27;: [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=50),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         &#x27;top_k&#x27;: [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70, model=Exllamav2HF(),\n",
       "                    no_repeat_ngram_size=0,\n",
       "                    output_preproc=&lt;function Tune...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=50)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70, model=Exllamav2HF(),\n",
       "                    no_repeat_ngram_size=0,\n",
       "                    output_preproc=&lt;function Tune...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=50)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>>],\n",
       "                                           device='cuda:0', do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  'truncation': True},\n",
       "                                           top_k=50),\n",
       "             param_grid={'do_sample': [True], 'generation_seed': [42],\n",
       "                         'max_new_tokens': [70], 'no_repeat_ngram_size': [0],\n",
       "                         'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>]],\n",
       "                         'temperature': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         'top_k': [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"_X\"], y=tuner_ob.dataset['_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1200 [00:19<15:47,  1.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [15:39<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_after, outputs_after = tuner_ob.get_score(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_before, scores_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 70, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], 'temperature': 0.1, 'top_k': 50}\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.common_utils import json_load, json_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = {\n",
    "    'scores_before' : scores_before,\n",
    "    'scores_after' : scores_after,\n",
    "    'outputs_before' : outputs_before,\n",
    "    'outputs_after' : outputs_after,\n",
    "    'best_params' : str(clf.best_params_),\n",
    "}\n",
    "\n",
    "f = \"./samsum-best-params-1200s-capybara-7b.json\"\n",
    "json_dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json_load(\"./samsum-best-params-1200s-capybara-7b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 70, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f26eb8db160>], 'temperature': 0.1, 'top_k': 50}\n"
     ]
    }
   ],
   "source": [
    "print(d['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# harcoding from above file here due to notebook re-run\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# eval on test samples\n",
    "\n",
    "gen_params1 = {\n",
    "    \"max_new_tokens\": 70,\n",
    "    \"stopping_criteria\": stopping_criteria,\n",
    "    \"generation_seed\": 42,\n",
    "}\n",
    "\n",
    "oos_scores_before, oos_outputs_before = tuner_ob.get_score(gen_params1,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13681165-1',\n",
       " 'dialogue': \"Alyssa: Have you seen Fergie’s national anthem? Illuminati does a great job.\\r\\nDerek: This is not normal. I saw it last week…\\r\\nAlyssa: What do you think about it?\\r\\nDerek: I can fart bright stripes and bright stars better then she sings.\\r\\nAlyssa: The best part is that she acts like she nailed it. But at least it's funny in a good way.\\r\\nDerek: It is 😂\",\n",
       " 'summary': \"Derek and Alyssa make fun of Fergie's performance of the national anthem.\",\n",
       " 'chat_format': [{'content': \"Summarize : Alyssa: Have you seen Fergie’s national anthem? Illuminati does a great job.\\r\\nDerek: This is not normal. I saw it last week…\\r\\nAlyssa: What do you think about it?\\r\\nDerek: I can fart bright stripes and bright stars better then she sings.\\r\\nAlyssa: The best part is that she acts like she nailed it. But at least it's funny in a good way.\\r\\nDerek: It is 😂\",\n",
       "   'role': 'user'}],\n",
       " 'formatted_chat': \"<|im_start|>user\\nSummarize : Alyssa: Have you seen Fergie’s national anthem? Illuminati does a great job.\\r\\nDerek: This is not normal. I saw it last week…\\r\\nAlyssa: What do you think about it?\\r\\nDerek: I can fart bright stripes and bright stars better then she sings.\\r\\nAlyssa: The best part is that she acts like she nailed it. But at least it's funny in a good way.\\r\\nDerek: It is 😂<|im_end|>\\n<|im_start|>assistant\\n\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  0%|          | 1/200 [00:05<17:12,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  1%|          | 2/200 [00:12<21:07,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  2%|▏         | 3/200 [00:31<39:25, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  2%|▏         | 4/200 [00:51<49:45, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  2%|▎         | 5/200 [00:59<41:20, 12.72s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  3%|▎         | 6/200 [01:02<30:21,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  4%|▎         | 7/200 [01:06<24:24,  7.59s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  4%|▍         | 8/200 [01:26<36:44, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  4%|▍         | 9/200 [01:35<34:46, 10.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  5%|▌         | 10/200 [01:48<36:32, 11.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  6%|▌         | 11/200 [02:07<42:57, 13.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  6%|▌         | 12/200 [02:10<33:17, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  6%|▋         | 13/200 [02:13<25:27,  8.17s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  7%|▋         | 14/200 [02:31<34:34, 11.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  8%|▊         | 15/200 [02:59<50:17, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  8%|▊         | 16/200 [03:03<38:06, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  8%|▊         | 17/200 [03:31<52:48, 17.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "  9%|▉         | 18/200 [03:44<48:06, 15.86s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 10%|▉         | 19/200 [03:55<43:55, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 10%|█         | 20/200 [04:05<39:36, 13.20s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 10%|█         | 21/200 [04:09<30:31, 10.23s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 11%|█         | 22/200 [04:13<25:21,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 12%|█▏        | 23/200 [04:24<27:26,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 12%|█▏        | 24/200 [04:36<29:30, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 12%|█▎        | 25/200 [05:09<49:10, 16.86s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 13%|█▎        | 26/200 [05:18<42:04, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 14%|█▎        | 27/200 [05:37<45:27, 15.77s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 14%|█▍        | 28/200 [05:45<38:59, 13.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 14%|█▍        | 29/200 [05:51<32:30, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 15%|█▌        | 30/200 [06:03<32:38, 11.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 16%|█▌        | 31/200 [06:20<36:40, 13.02s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 16%|█▌        | 32/200 [06:26<30:51, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 16%|█▋        | 33/200 [06:55<45:17, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 17%|█▋        | 34/200 [07:03<38:31, 13.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 18%|█▊        | 35/200 [07:19<39:59, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 18%|█▊        | 36/200 [07:32<38:04, 13.93s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 18%|█▊        | 37/200 [07:39<32:39, 12.02s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 19%|█▉        | 38/200 [07:44<26:53,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 20%|█▉        | 39/200 [07:52<24:57,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 20%|██        | 40/200 [08:01<24:13,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 20%|██        | 41/200 [08:20<32:04, 12.10s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 21%|██        | 42/200 [08:24<25:55,  9.84s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 22%|██▏       | 43/200 [08:33<24:41,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 22%|██▏       | 44/200 [08:46<27:39, 10.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 22%|██▎       | 45/200 [08:54<25:30,  9.88s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 23%|██▎       | 46/200 [09:07<27:44, 10.81s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 24%|██▎       | 47/200 [09:14<24:23,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 24%|██▍       | 48/200 [09:22<23:12,  9.16s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 24%|██▍       | 49/200 [09:31<22:49,  9.07s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 25%|██▌       | 50/200 [09:34<18:22,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 26%|██▌       | 51/200 [09:44<20:07,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 26%|██▌       | 52/200 [09:51<19:00,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 26%|██▋       | 53/200 [09:55<15:48,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 27%|██▋       | 54/200 [09:58<13:12,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 28%|██▊       | 55/200 [10:10<17:51,  7.39s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 28%|██▊       | 56/200 [10:20<20:04,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 28%|██▊       | 57/200 [10:46<32:21, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 29%|██▉       | 58/200 [11:05<35:51, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 30%|██▉       | 59/200 [11:17<33:24, 14.22s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 30%|███       | 60/200 [11:26<29:23, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 30%|███       | 61/200 [11:50<37:23, 16.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 31%|███       | 62/200 [11:55<29:21, 12.76s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 32%|███▏      | 63/200 [12:05<27:00, 11.83s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 32%|███▏      | 64/200 [12:19<28:43, 12.67s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 32%|███▎      | 65/200 [12:25<23:59, 10.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 33%|███▎      | 66/200 [12:46<30:54, 13.84s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 34%|███▎      | 67/200 [12:51<24:25, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 34%|███▍      | 68/200 [13:07<27:29, 12.49s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 34%|███▍      | 69/200 [13:17<25:33, 11.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 35%|███▌      | 70/200 [13:24<22:34, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 36%|███▌      | 71/200 [13:50<32:33, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 36%|███▌      | 72/200 [14:21<42:27, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 36%|███▋      | 73/200 [14:45<44:40, 21.10s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 37%|███▋      | 74/200 [15:08<45:03, 21.45s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 38%|███▊      | 75/200 [15:13<35:00, 16.80s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 38%|███▊      | 76/200 [15:17<26:28, 12.81s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 38%|███▊      | 77/200 [15:34<29:07, 14.20s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 39%|███▉      | 78/200 [15:45<26:42, 13.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 40%|███▉      | 79/200 [16:11<33:56, 16.83s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 40%|████      | 80/200 [16:16<26:37, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 40%|████      | 81/200 [16:22<22:17, 11.24s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 41%|████      | 82/200 [16:36<23:56, 12.17s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 42%|████▏     | 83/200 [16:46<21:57, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 42%|████▏     | 84/200 [16:56<21:07, 10.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 42%|████▎     | 85/200 [17:01<17:39,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 43%|████▎     | 86/200 [17:11<18:13,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 44%|████▎     | 87/200 [17:23<19:22, 10.29s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 44%|████▍     | 88/200 [17:35<20:03, 10.74s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 44%|████▍     | 89/200 [17:38<15:46,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 45%|████▌     | 90/200 [18:06<26:07, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 46%|████▌     | 91/200 [18:31<31:57, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 46%|████▌     | 92/200 [18:34<23:19, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 46%|████▋     | 93/200 [19:03<31:52, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 47%|████▋     | 94/200 [19:07<24:04, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 48%|████▊     | 95/200 [19:13<20:04, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 48%|████▊     | 96/200 [19:24<19:51, 11.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 48%|████▊     | 97/200 [19:57<30:24, 17.72s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 49%|████▉     | 98/200 [20:01<23:24, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 50%|████▉     | 99/200 [20:09<20:17, 12.05s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 50%|█████     | 100/200 [20:17<17:48, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 50%|█████     | 101/200 [20:26<16:44, 10.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 51%|█████     | 102/200 [20:45<20:52, 12.78s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 52%|█████▏    | 103/200 [20:50<16:47, 10.39s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 52%|█████▏    | 104/200 [21:10<21:18, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 52%|█████▎    | 105/200 [21:14<16:52, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 53%|█████▎    | 106/200 [21:34<20:50, 13.30s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 54%|█████▎    | 107/200 [21:53<23:18, 15.04s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 54%|█████▍    | 108/200 [22:07<22:39, 14.78s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 55%|█████▍    | 109/200 [22:24<23:33, 15.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 55%|█████▌    | 110/200 [22:30<19:02, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 56%|█████▌    | 111/200 [22:37<16:02, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 56%|█████▌    | 112/200 [22:39<12:13,  8.33s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 56%|█████▋    | 113/200 [22:46<11:23,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 57%|█████▋    | 114/200 [23:10<18:02, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 57%|█████▊    | 115/200 [23:16<15:00, 10.59s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 58%|█████▊    | 116/200 [23:34<18:10, 12.98s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 58%|█████▊    | 117/200 [23:47<17:48, 12.87s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 59%|█████▉    | 118/200 [23:52<14:37, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 60%|█████▉    | 119/200 [24:03<14:20, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 60%|██████    | 120/200 [24:18<15:53, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 60%|██████    | 121/200 [24:22<12:31,  9.51s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 61%|██████    | 122/200 [24:32<12:52,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 62%|██████▏   | 123/200 [24:52<16:21, 12.75s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 62%|██████▏   | 124/200 [25:01<14:58, 11.83s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 62%|██████▎   | 125/200 [25:11<13:51, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 63%|██████▎   | 126/200 [25:26<15:05, 12.23s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 64%|██████▎   | 127/200 [25:50<19:23, 15.94s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 64%|██████▍   | 128/200 [26:02<17:25, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 64%|██████▍   | 129/200 [26:18<17:48, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 65%|██████▌   | 130/200 [26:53<24:39, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 66%|██████▌   | 131/200 [27:10<22:55, 19.94s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 66%|██████▌   | 132/200 [27:18<18:33, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 66%|██████▋   | 133/200 [27:25<15:08, 13.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 67%|██████▋   | 134/200 [27:31<12:15, 11.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 68%|██████▊   | 135/200 [27:37<10:19,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 68%|██████▊   | 136/200 [28:08<17:06, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 68%|██████▊   | 137/200 [28:27<17:50, 17.00s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 69%|██████▉   | 138/200 [28:46<18:13, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 70%|██████▉   | 139/200 [28:54<14:48, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 70%|███████   | 140/200 [28:57<11:11, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 70%|███████   | 141/200 [28:59<08:17,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 71%|███████   | 142/200 [29:08<08:17,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 72%|███████▏  | 143/200 [29:12<06:50,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 72%|███████▏  | 144/200 [29:19<06:43,  7.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 72%|███████▎  | 145/200 [29:24<05:56,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 73%|███████▎  | 146/200 [29:31<05:59,  6.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 74%|███████▎  | 147/200 [29:51<09:24, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 74%|███████▍  | 148/200 [29:54<07:17,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 74%|███████▍  | 149/200 [30:00<06:33,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 75%|███████▌  | 150/200 [30:03<05:13,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 76%|███████▌  | 151/200 [30:11<05:24,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 76%|███████▌  | 152/200 [30:29<08:03, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 76%|███████▋  | 153/200 [30:40<08:16, 10.57s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 77%|███████▋  | 154/200 [30:51<08:05, 10.56s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 78%|███████▊  | 155/200 [30:56<06:46,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 78%|███████▊  | 156/200 [31:18<09:27, 12.89s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 78%|███████▊  | 157/200 [31:24<07:39, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 79%|███████▉  | 158/200 [31:27<05:55,  8.46s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 80%|███████▉  | 159/200 [31:31<04:52,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 80%|████████  | 160/200 [31:51<07:16, 10.92s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 80%|████████  | 161/200 [32:00<06:46, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 81%|████████  | 162/200 [32:04<05:19,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 82%|████████▏ | 163/200 [32:07<04:10,  6.78s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 82%|████████▏ | 164/200 [32:18<04:52,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 82%|████████▎ | 165/200 [32:21<03:45,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 83%|████████▎ | 166/200 [32:41<06:05, 10.75s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 84%|████████▎ | 167/200 [32:54<06:08, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 84%|████████▍ | 168/200 [32:57<04:43,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 84%|████████▍ | 169/200 [33:05<04:25,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 85%|████████▌ | 170/200 [33:11<03:55,  7.85s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 86%|████████▌ | 171/200 [33:33<05:54, 12.21s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 86%|████████▌ | 172/200 [33:36<04:21,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 86%|████████▋ | 173/200 [33:47<04:27,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 87%|████████▋ | 174/200 [34:07<05:33, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 88%|████████▊ | 175/200 [34:14<04:37, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 88%|████████▊ | 176/200 [34:17<03:28,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 88%|████████▊ | 177/200 [34:23<02:59,  7.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 89%|████████▉ | 178/200 [34:36<03:29,  9.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 90%|████████▉ | 179/200 [34:41<02:47,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 90%|█████████ | 180/200 [34:48<02:34,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 90%|█████████ | 181/200 [34:51<01:59,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 91%|█████████ | 182/200 [35:09<02:58,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 92%|█████████▏| 183/200 [35:16<02:29,  8.82s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 92%|█████████▏| 184/200 [35:19<01:54,  7.15s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 92%|█████████▎| 185/200 [35:29<02:00,  8.02s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 93%|█████████▎| 186/200 [35:49<02:42, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 94%|█████████▎| 187/200 [35:55<02:10, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 94%|█████████▍| 188/200 [36:03<01:53,  9.42s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 94%|█████████▍| 189/200 [36:13<01:45,  9.55s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 95%|█████████▌| 190/200 [36:23<01:36,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 96%|█████████▌| 191/200 [36:31<01:23,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 96%|█████████▌| 192/200 [36:37<01:05,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 96%|█████████▋| 193/200 [36:43<00:52,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 97%|█████████▋| 194/200 [36:51<00:45,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 98%|█████████▊| 195/200 [37:10<00:54, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 98%|█████████▊| 196/200 [37:19<00:42, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 98%|█████████▊| 197/200 [37:26<00:28,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      " 99%|█████████▉| 198/200 [37:34<00:17,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "100%|█████████▉| 199/200 [37:38<00:07,  7.38s/it]Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "100%|██████████| 200/200 [37:52<00:00, 11.36s/it]\n"
     ]
    }
   ],
   "source": [
    "oos_scores_after, oos_outputs_after = tuner_ob.get_score(clf.best_params_,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2540499441403945 0.24443739584141397\n"
     ]
    }
   ],
   "source": [
    "print(oos_scores_before, oos_scores_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'scores_before' : scores_before,\n",
    "    'scores_after' : scores_after,\n",
    "    'outputs_before' : outputs_before,\n",
    "    'outputs_after' : outputs_after,\n",
    "\n",
    "    'oos_scores_before' : oos_scores_before,\n",
    "    'oos_scores_after' : oos_scores_after,\n",
    "    'oos_outputs_before' : oos_outputs_before,\n",
    "    'oos_outputs_after' : oos_outputs_after,\n",
    "    'best_params' : str(clf.best_params_),\n",
    "}\n",
    "\n",
    "f = \"./samsum-best-params-1200s-tune-capybara-7b.json\"\n",
    "json_dump(d, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsearch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
