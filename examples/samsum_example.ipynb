{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n",
      "0.0.14 2.2.0+cu121 4.38.2 0.1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Requires nltk==3.8.1\n",
    "rouge_score==0.1.2\n",
    "py7zr=0.21.0\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import llmsearch\n",
    "import exllamav2\n",
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Cache_8bit,\n",
    "    ExLlamaV2Config\n",
    ")\n",
    "\n",
    "print(exllamav2.__version__, torch.__version__, transformers.__version__, llmsearch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Union, List\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria\n",
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug, set_verbosity_warning\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, AutoTokenizer, StoppingCriteriaList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1\n",
    "num_tune_samples = 500\n",
    "num_test_samples = 200\n",
    "\n",
    "model_id = \"Praful932/dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Model related code ------\n",
    "class Exllamav2HF(PreTrainedModel):\n",
    "    def __init__(self, config: ExLlamaV2Config):\n",
    "        super().__init__(PretrainedConfig())\n",
    "        self.ex_config = config\n",
    "        self.ex_model = ExLlamaV2(config)\n",
    "        split = None\n",
    "        if shared.args.gpu_split:\n",
    "            split = [float(alloc) for alloc in shared.args.gpu_split.split(\",\")]\n",
    "\n",
    "        self.ex_model.load(split)\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.loras = None\n",
    "\n",
    "        if shared.args.cache_8bit:\n",
    "            self.ex_cache = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "        else:\n",
    "            self.ex_cache = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "        self.past_seq = None\n",
    "        if shared.args.cfg_cache:\n",
    "            if shared.args.cache_8bit:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache_8bit(self.ex_model)\n",
    "            else:\n",
    "                self.ex_cache_negative = ExLlamaV2Cache(self.ex_model)\n",
    "\n",
    "            self.past_seq_negative = None\n",
    "\n",
    "    def _validate_model_class(self):\n",
    "        pass\n",
    "\n",
    "    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n",
    "        pass\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_ids': input_ids, **kwargs}\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(0)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        use_cache = kwargs.get('use_cache', True)\n",
    "        labels = kwargs.get('labels', None)\n",
    "        past_key_values = kwargs.get('past_key_values', None)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            if not shared.args.cfg_cache:\n",
    "                print(\"Please enable the cfg-cache option to use CFG with ExLlamav2_HF.\")\n",
    "                return\n",
    "\n",
    "            input_ids = args[0]\n",
    "            is_negative = True\n",
    "            past_seq = self.past_seq_negative\n",
    "            ex_cache = self.ex_cache_negative\n",
    "        else:\n",
    "            input_ids = kwargs['input_ids']\n",
    "            is_negative = False\n",
    "            past_seq = self.past_seq\n",
    "            ex_cache = self.ex_cache\n",
    "\n",
    "        seq = input_ids[0].tolist()\n",
    "        if is_negative and past_key_values is not None:\n",
    "            seq = past_key_values + seq\n",
    "\n",
    "        seq_tensor = torch.tensor(seq)\n",
    "        reset = True\n",
    "\n",
    "        # Make the forward call\n",
    "        if labels is None:\n",
    "            if past_seq is not None:\n",
    "                min_length = min(past_seq.shape[0], seq_tensor.shape[0])\n",
    "                indices = torch.nonzero(~torch.eq(past_seq[:min_length], seq_tensor[:min_length]))\n",
    "                if len(indices) > 0:\n",
    "                    longest_prefix = indices[0].item()\n",
    "                else:\n",
    "                    longest_prefix = min_length\n",
    "\n",
    "                if longest_prefix > 0:\n",
    "                    reset = False\n",
    "                    ex_cache.current_seq_len = longest_prefix\n",
    "                    if len(seq_tensor) - longest_prefix > 1:\n",
    "                        self.ex_model.forward(seq_tensor[longest_prefix:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "                    elif len(seq_tensor) == longest_prefix:\n",
    "                        # Very tricky: if the prefix we are reusing *is* the input_ids, then we have to back up the cache pointer by one,\n",
    "                        # because we feed input_ids[-1] to forward() below, but that last token is already in the cache!\n",
    "                        ex_cache.current_seq_len -= 1\n",
    "\n",
    "            if reset:\n",
    "                ex_cache.current_seq_len = 0\n",
    "                if len(seq_tensor) > 1:\n",
    "                    self.ex_model.forward(seq_tensor[:-1].view(1, -1), ex_cache, preprocess_only=True, loras=self.loras)\n",
    "\n",
    "            logits = self.ex_model.forward(seq_tensor[-1:].view(1, -1), ex_cache, loras=self.loras).to(input_ids.device).float()\n",
    "        else:\n",
    "            ex_cache.current_seq_len = 0\n",
    "            logits = self.ex_model.forward(seq_tensor.view(1, -1), ex_cache, last_id_only=False, loras=self.loras).float()\n",
    "\n",
    "        if is_negative:\n",
    "            self.past_seq_negative = seq_tensor\n",
    "        else:\n",
    "            self.past_seq = seq_tensor\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, logits.shape[-1])\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=seq if use_cache else None, loss=loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        assert len(model_args) == 0 and len(kwargs) == 0, \"extra args is currently not supported\"\n",
    "        if isinstance(pretrained_model_name_or_path, str):\n",
    "            pretrained_model_name_or_path = Path(pretrained_model_name_or_path)\n",
    "\n",
    "\n",
    "        config = ExLlamaV2Config()\n",
    "        config.model_dir = str(pretrained_model_name_or_path)\n",
    "        config.prepare()\n",
    "\n",
    "        config.max_seq_len = shared.args.max_seq_len\n",
    "        config.scale_pos_emb = shared.args.compress_pos_emb\n",
    "        config.scale_alpha_value = shared.args.alpha_value\n",
    "        config.no_flash_attn = shared.args.no_flash_attn\n",
    "\n",
    "        return Exllamav2HF(config)\n",
    "\n",
    "class Shared:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.gpu_split = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.args = Shared.Args()\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1\n",
    "\n",
    "shared = Shared()\n",
    "shared.args.gpu_split = None\n",
    "shared.args.cache_8bit = None\n",
    "shared.args.cfg_cache = None\n",
    "# shared.args.model_dir = \"/kaggle/input/\"\n",
    "shared.args.max_seq_len = 2048\n",
    "shared.args.compress_pos_emb = 1\n",
    "shared.args.alpha_value = 1\n",
    "shared.args.no_flash_attn = 1\n",
    "\n",
    "\n",
    "# ------ Dataset related code ------\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def get_rouge_score(y_true: List, y_pred: List):\n",
    "    preds, gts = postprocess_text(preds=y_pred, labels=[item['summary'] for item in y_true])\n",
    "\n",
    "    result = rouge_metric.compute(predictions=preds, references=gts, use_stemmer=True, use_aggregator=False)\n",
    "    return np.mean(result['rouge2'])\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, hf_dataset, tokenizer, prompt_template = \"Summarize : {dialogue}\", input_key = \"\", output_key = \"\", system_prompt = \"\", add_output = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x : {\"chat_format\" : ([{'role' : \"system\", \"content\" : system_prompt}] if system_prompt else []) + [\n",
    "            {\n",
    "                'role' : \"user\", \"content\" : prompt_template.format(**{input_key : x[input_key]})\n",
    "            }\n",
    "        ] + ([{'role' : 'assistant', \"content\" : x[output_key]}] if add_output else [])})\n",
    "\n",
    "    def apply_chat_template(self, add_gen_prompt = True):\n",
    "        \"\"\"Converts the dataset to a chat based format\"\"\"\n",
    "        self.hf_dataset = self.hf_dataset.map(lambda x: {\"formatted_chat\": self.tokenizer.apply_chat_template(x[\"chat_format\"], tokenize=False, add_generation_prompt=add_gen_prompt)})\n",
    "\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_id, temp_model_dir):\n",
    "    temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n",
    "\n",
    "    gc_cuda()\n",
    "\n",
    "    model = Exllamav2HF.from_pretrained(output_folder)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        output_folder, local_files_only=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    # since model was finetuned on val set\n",
    "\n",
    "    # valid_dataset = datasets.load_dataset(\"samsum\")['validation']\n",
    "\n",
    "    # valid_dataset = DatasetWrapper(valid_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    # valid_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "    # valid_dataset.hf_dataset = valid_dataset.hf_dataset.shuffle(seed=seed)\n",
    "    # samples_to_tune_on = valid_dataset.hf_dataset.select(range(num_tune_samples))\n",
    "\n",
    "\n",
    "    test_dataset = datasets.load_dataset(\"samsum\")['test']\n",
    "    test_dataset = DatasetWrapper(test_dataset, tokenizer, input_key = \"dialogue\", output_key = \"summary\", add_output = False)\n",
    "    test_dataset.apply_chat_template(add_gen_prompt=True)\n",
    "    test_dataset.hf_dataset = test_dataset.hf_dataset.shuffle(seed=seed)\n",
    "\n",
    "    samples_to_tune_on = test_dataset.hf_dataset.select(range(num_tune_samples))\n",
    "    remaining_indices = range(num_tune_samples, num_tune_samples + num_test_samples)\n",
    "    test_samples = test_dataset.hf_dataset.select(remaining_indices)\n",
    "\n",
    "    return samples_to_tune_on, test_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists in temp_dir/Praful932_dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ. Checking the model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum validated: model.safetensors  817eec4c0f73483e67516e28b499ab75f11a4639aad5ffa04c389f8f25ce2cf8\n",
      "Checksum validated: tokenizer.model  dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055\n",
      "[+] Validated checksums of all model files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "\n",
    "# Load Model, Tokenizer, Dataset\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n",
    "\n",
    "# Dataset we will use to find the best generation parameters\n",
    "samples_to_tune_on,test_samples = load_dataset()\n",
    "\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_verbosity_debug()\n",
    "set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=samples_to_tune_on,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\",'truncation' : True, \"add_special_tokens\": False},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n",
    "    scorer=get_rouge_score,\n",
    "    prompt_template=\"{formatted_chat}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"formatted_chat\"], \"eval_cols\": [\"summary\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Summarize : Claire: <file_photo>\n",
      "Kim: Looks delicious...\n",
      "Linda: No way... Look what I'm cooking right now:\n",
      "Linda: <file_photo>\n",
      "Claire: hahahaha \n",
      "Kim: Curry dream team\n",
      "Claire: Enjoy your dinner :*<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tuner_ob.dataset['_X'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:15<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 70\n",
    "\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.257852927825895\n"
     ]
    }
   ],
   "source": [
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_space = {\n",
    "    'max_new_tokens' : [70],\n",
    "    'stopping_criteria' : [stopping_criteria],\n",
    "    'generation_seed' : [42],\n",
    "    'do_sample' : [True],\n",
    "\n",
    "    'temperature': [0.1,0.3,0.5,0.7,0.9,1.0],\n",
    "    'top_k': [50,60,70,80],\n",
    "    'no_repeat_ngram_size': [0],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator = tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring = tuner_ob.scorer,\n",
    "    cv = 2,\n",
    "    n_jobs = None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:01<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=50;, score=0.256 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:49<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=50;, score=0.249 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:16<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=60;, score=0.259 total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:55<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=60;, score=0.254 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:11<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=70;, score=0.257 total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:01<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=70;, score=0.257 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:15<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=80;, score=0.258 total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:55<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.1, top_k=80;, score=0.254 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:14<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=50;, score=0.250 total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=50;, score=0.254 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:22<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=60;, score=0.250 total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:54<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=60;, score=0.237 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:58<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=70;, score=0.247 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:50<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=70;, score=0.251 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:05<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=80;, score=0.253 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:47<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.3, top_k=80;, score=0.242 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:02<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=50;, score=0.217 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:53<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=50;, score=0.224 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:06<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=60;, score=0.229 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:50<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=60;, score=0.224 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:56<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=70;, score=0.227 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:48<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=70;, score=0.230 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:57<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=80;, score=0.225 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:51<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.5, top_k=80;, score=0.236 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:02<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=50;, score=0.193 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:47<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=50;, score=0.218 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:08<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=60;, score=0.205 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:45<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=60;, score=0.197 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=70;, score=0.194 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:55<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=70;, score=0.207 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:59<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=80;, score=0.183 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:48<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.7, top_k=80;, score=0.209 total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:04<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=50;, score=0.169 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:02<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=50;, score=0.159 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:07<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=60;, score=0.148 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:54<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=60;, score=0.162 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:16<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=70;, score=0.148 total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=70;, score=0.168 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:11<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=80;, score=0.152 total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:52<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=0.9, top_k=80;, score=0.165 total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:18<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=50;, score=0.142 total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=50;, score=0.145 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:05<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=60;, score=0.135 total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:58<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=60;, score=0.130 total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:08<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=70;, score=0.132 total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:13<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=70;, score=0.131 total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:12<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=80;, score=0.135 total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:10<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], temperature=1.0, top_k=80;, score=0.146 total time= 4.2min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=70),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         &#x27;top_k&#x27;: [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=70),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         &#x27;top_k&#x27;: [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70, model=Exllamav2HF(),\n",
       "                    no_repeat_ngram_size=0,\n",
       "                    output_preproc=&lt;function Tune...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=70)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70, model=Exllamav2HF(),\n",
       "                    no_repeat_ngram_size=0,\n",
       "                    output_preproc=&lt;function Tune...\n",
       "\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=70)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=1,\n",
       "                                           callbacks_after_inference=[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>>],\n",
       "                                           device='cuda:0', do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=Exllamav2HF(),\n",
       "                                           no_repeat_ngram_size=0,...\n",
       "                                                                  'truncation': True},\n",
       "                                           top_k=70),\n",
       "             param_grid={'do_sample': [True], 'generation_seed': [42],\n",
       "                         'max_new_tokens': [70], 'no_repeat_ngram_size': [0],\n",
       "                         'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>]],\n",
       "                         'temperature': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
       "                         'top_k': [50, 60, 70, 80]},\n",
       "             scoring=make_scorer(get_rouge_score), verbose=3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=tuner_ob.dataset[\"_X\"], y=tuner_ob.dataset['_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:58<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_after, outputs_after = tuner_ob.get_score(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.257852927825895 0.2581935494075166\n"
     ]
    }
   ],
   "source": [
    "print(scores_before, scores_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 70, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7fe3468afa90>], 'temperature': 0.1, 'top_k': 70}\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_dump(ob : dict, file_path: Path):\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ob, json_file, indent=4)\n",
    "\n",
    "d = {\n",
    "    'scores_before' : scores_before,\n",
    "    'scores_after' : scores_after,\n",
    "    'outputs_before' : outputs_before,\n",
    "    'outputs_after' : outputs_after,\n",
    "    'best_params' : str(clf.best_params_),\n",
    "}\n",
    "\n",
    "f = \"./samsum-best-params-500-capybara-7b.json\"\n",
    "json_dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO eval on oos set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsearch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
