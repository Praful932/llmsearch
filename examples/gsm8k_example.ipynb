{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Monkey Patching .generate function of `transformers` library\n","0.2.4 2.2.0+cu121 4.38.2 0.1.0\n"]}],"source":["# uses autoawq==0.2.4 autoawq_kernels==0.0.6\n","\"\"\"\n","model - 7B model - https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\n","dataset - gsm8k train split, llmsearch is run on a subset and evaluated on another\n","\n","\n","works with this environ setup - /archive/runpod_dev_env_setup.sh\n","\"\"\"\n","\n","import awq\n","import torch\n","import transformers\n","\n","import llmsearch\n","\n","print(awq.__version__, torch.__version__, transformers.__version__, llmsearch.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import re\n","import textwrap\n","from pathlib import Path\n","\n","import datasets\n","\n","from awq import AutoAWQForCausalLM\n","from sklearn.model_selection import GridSearchCV\n","from transformers import StoppingCriteriaList, AutoTokenizer\n","\n","from llmsearch.tuner import Tuner\n","from llmsearch.utils.mem_utils import gc_cuda\n","from llmsearch.utils.common_utils import json_load, json_dump\n","from llmsearch.utils.model_downloader import download_model_from_hf\n","from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["seed = 42\n","batch_size = 1\n","num_tune_samples = 150\n","num_test_samples = 500\n","model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n","device = \"cuda:0\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def load_model_and_tokenizer(model_id, temp_model_dir):\n","    temp_model_dir.mkdir(exist_ok=True, parents=True)\n","    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n","\n","    gc_cuda()\n","\n","    model = AutoAWQForCausalLM.from_quantized(\n","        quant_path=output_folder, fuse_layers=True, device_map={\"\": device}, local_files_only=True\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        output_folder, local_files_only=True, legacy=False, use_fast=False\n","    )\n","    tokenizer.pad_token = tokenizer.unk_token\n","    tokenizer.padding_side = \"left\"\n","\n","    return model, tokenizer\n","\n","def load_dataset():\n","\n","    def preprocess_dataset(\n","        dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt=True\n","    ):\n","\n","        def wrapper(sample):\n","            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n","            messages = (\n","                []\n","                if system_prompt is None\n","                else [{\"role\": \"system\", \"content\": system_prompt}]\n","            )\n","            formatted_pt = pt.format(**{pt_col: sample[pt_col] for pt_col in pt_cols})\n","            messages.append(\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": formatted_pt,\n","                }\n","            )\n","            formatted_pt_with_ct = tokenizer.apply_chat_template(\n","                messages, tokenize=False, add_generation_prompt=add_generation_prompt\n","            )\n","            return formatted_pt_with_ct\n","\n","        def actual_input(sample):\n","            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n","            return sample[pt_cols[0]]\n","\n","        pt_dataset = dataset.map(\n","            lambda sample: {\n","                \"X\": wrapper(sample),\n","                \"actual input\": actual_input(sample),\n","            }\n","        )\n","\n","        return pt_dataset\n","\n","\n","    # 2-shot prompt template - https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n","    pt = textwrap.dedent(\n","    \"\"\"\\\n","    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n","    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n","\n","    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n","    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n","\n","    Q: {question}\"\"\"\n","    )\n","    pt_cols = [\"question\"]\n","    system_prompt = \"Solve the following math problems, end with The answer is\"\n","    gsm8k_dataset = datasets.load_dataset(\"gsm8k\", \"main\")\n","\n","\n","    processed_dataset = preprocess_dataset(\n","        gsm8k_dataset[\"train\"],\n","        tokenizer,\n","        pt=pt,\n","        pt_cols=pt_cols,\n","        system_prompt=system_prompt,\n","        add_generation_prompt=True,\n","    )\n","\n","    shuffled_dataset = processed_dataset.shuffle(seed=seed)\n","\n","    samples_to_tune_on = shuffled_dataset.select(range(num_tune_samples))\n","    remaining_indices = range(num_tune_samples, num_tune_samples + num_test_samples)\n","    test_dataset = shuffled_dataset.select(remaining_indices)\n","    return samples_to_tune_on, test_dataset\n","\n","def get_score(y_true, y_pred):\n","    def standardize(s):\n","        if s is None:\n","            return s\n","        s = s.replace(\",\", \"\")\n","        s = s.replace(\"$\", \"\")\n","        # remove trailing zeros\n","        s = s.replace(\".00\", \"\")\n","        if s.endswith(\".\"):\n","            s = s[:-1]\n","        return s.strip()\n","\n","    def extract_answer_from_out(s):\n","        pattern = re.compile(r\"The answer is ((\\d|\\-|\\$)((\\d|\\,|\\.)+)?\\d?)\")\n","        match = pattern.search(s)\n","        if match:\n","            return standardize(match.group(1).strip())\n","        else:\n","            return None\n","\n","    scores = []\n","\n","    for y_t, y_p in zip(y_true, y_pred):\n","        y_t_answer = y_t[\"answer\"].split(\"####\")[-1].strip()\n","        y_p_answer = extract_answer_from_out(y_p)\n","\n","        y_t_answer = standardize(y_t_answer)\n","        y_p_answer = standardize(y_p_answer)\n","\n","        # print(\"y_pred - \", y_p_answer)\n","        # print(\"y_true - \", y_t_answer)\n","\n","        if y_t_answer == y_p_answer:\n","            scores.append(1)\n","        else:\n","            scores.append(0)\n","    return sum(scores) / len(scores)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading the model to temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 51.0  /51.0   34.6kiB/s\n","100%|██████████| 911   /911    1.09MiB/s\n","100%|██████████| 115   /115    611kiB/s\n","100%|██████████| 17.9k /17.9k  20.1MiB/s\n","100%|██████████| 420   /420    2.40MiB/s\n","100%|██████████| 126   /126    323kiB/s\n","\n","100%|██████████| 1.60k /1.60k  5.13MiB/s\n","\n","100%|██████████| 493k  /493k   8.02MiB/s\n","100%|██████████| 1.80M /1.80M  3.30MiB/s\n","100%|██████████| 4.15G /4.15G  190MiB/s\n","Replacing layers...: 100%|██████████| 32/32 [00:03<00:00,  8.02it/s]\n","Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 21.56it/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Downloading readme: 100%|██████████| 7.94k/7.94k [00:00<00:00, 12.2MB/s]\n","Downloading data: 100%|██████████| 2.31M/2.31M [00:00<00:00, 6.24MB/s]\n","Downloading data: 100%|██████████| 419k/419k [00:01<00:00, 411kB/s]s]\n","Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n","Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 801.13it/s]\n","Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 450915.44 examples/s]\n","Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 487340.29 examples/s]\n","Map: 100%|██████████| 7473/7473 [00:00<00:00, 12602.74 examples/s]\n"]}],"source":["# Load Model, Tokenizer, Dataset\n","temp_model_dir = Path(f\"./temp_dir/\")\n","temp_model_dir.mkdir(exist_ok=True, parents=True)\n","\n","model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n","\n","# Dataset we will use to find the best generation parameters\n","samples_to_tune_on,test_dataset = load_dataset()\n","\n","multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n","stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n","callbacks_after_inference = [multi_token_stop_criteria_ob.reset]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 150/150 [00:00<00:00, 5908.91 examples/s]\n"]}],"source":["tuner_ob = Tuner(\n","    model=model,\n","    tokenizer=tokenizer,\n","    dataset=samples_to_tune_on,\n","    device=\"cuda:0\",\n","    batch_size=batch_size,\n","    tokenizer_encode_args={\"padding\": \"longest\", \"add_special_tokens\": False},\n","    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n","    scorer=get_score,\n","    prompt_template=\"{X}\",\n","    seed=seed,\n","    column_mapping={\"input_cols\": [\"X\"], \"eval_cols\": [\"answer\"]},\n","    callbacks_after_inference=callbacks_after_inference,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/150 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 150/150 [08:00<00:00,  3.20s/it]\n"]}],"source":["gen_params1 = {\n","    \"max_new_tokens\": 500,\n","    \"stopping_criteria\": stopping_criteria,\n","    \"generation_seed\": 42,\n","}\n","\n","scores_before, outputs_before = tuner_ob.get_score(gen_params1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8266666666666667\n"]}],"source":["print(scores_before)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["hyp_space = {\n","    'max_new_tokens' : [500],\n","    'stopping_criteria' : [stopping_criteria],\n","    'generation_seed' : [42],\n","    'do_sample' : [True],\n","\n","    'top_k': [10,50,60,70,80],\n","    'top_p' : [0.7,0.75,0.8,0.95],\n","    'no_repeat_ngram_size': [0],\n","\n","}\n","\n","clf = GridSearchCV(\n","    estimator = tuner_ob.estimator,\n","    param_grid=hyp_space,\n","    scoring = tuner_ob.scorer,\n","    cv = 2,\n","    n_jobs = None,\n","    verbose=3,\n",")\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/75 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:12<00:00,  2.56s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.7;, score=0.707 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.7;, score=0.693 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:09<00:00,  2.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.75;, score=0.667 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:50<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.75;, score=0.747 total time= 2.8min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:03<00:00,  2.45s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.8;, score=0.733 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.8;, score=0.707 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:10<00:00,  2.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.95;, score=0.747 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:54<00:00,  2.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=10, top_p=0.95;, score=0.680 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.7;, score=0.720 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:52<00:00,  2.30s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.7;, score=0.693 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:09<00:00,  2.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.75;, score=0.640 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:54<00:00,  2.33s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.75;, score=0.720 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:06<00:00,  2.48s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.8;, score=0.680 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:47<00:00,  2.23s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.8;, score=0.733 total time= 2.8min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:10<00:00,  2.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.95;, score=0.720 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=50, top_p=0.95;, score=0.653 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.7;, score=0.720 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.7;, score=0.693 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.75;, score=0.640 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:54<00:00,  2.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.75;, score=0.720 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:05<00:00,  2.48s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.8;, score=0.680 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:46<00:00,  2.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.8;, score=0.733 total time= 2.8min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:10<00:00,  2.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.95;, score=0.720 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.28s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=60, top_p=0.95;, score=0.653 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.7;, score=0.720 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.7;, score=0.693 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.75;, score=0.640 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:53<00:00,  2.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.75;, score=0.720 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:05<00:00,  2.47s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.8;, score=0.680 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:45<00:00,  2.21s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.8;, score=0.733 total time= 2.8min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:10<00:00,  2.54s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.95;, score=0.720 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:51<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=70, top_p=0.95;, score=0.653 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:08<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.7;, score=0.720 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:53<00:00,  2.31s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.7;, score=0.693 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:09<00:00,  2.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.75;, score=0.640 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:53<00:00,  2.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.75;, score=0.720 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:05<00:00,  2.48s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.8;, score=0.680 total time= 3.1min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:46<00:00,  2.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.8;, score=0.733 total time= 2.8min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [03:09<00:00,  2.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.95;, score=0.720 total time= 3.2min\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 75/75 [02:52<00:00,  2.30s/it]"]},{"name":"stdout","output_type":"stream","text":["[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=500, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], top_k=80, top_p=0.95;, score=0.653 total time= 2.9min\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n","             estimator=LLMEstimatorWrapper(batch_size=1,\n","                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;&gt;],\n","                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n","                                           generation_seed=42, is_fitted_=True,\n","                                           max_new_tokens=500,\n","                                           model=MistralAWQForCausalLM(\n","  (model): Mistra...\n","                                                                  &#x27;padding&#x27;: &#x27;longest&#x27;},\n","                                           top_k=10, top_p=0.8),\n","             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n","                         &#x27;max_new_tokens&#x27;: [500], &#x27;no_repeat_ngram_size&#x27;: [0],\n","                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;]],\n","                         &#x27;top_k&#x27;: [10, 50, 60, 70, 80],\n","                         &#x27;top_p&#x27;: [0.7, 0.75, 0.8, 0.95]},\n","             scoring=make_scorer(get_score), verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n","             estimator=LLMEstimatorWrapper(batch_size=1,\n","                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;&gt;],\n","                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n","                                           generation_seed=42, is_fitted_=True,\n","                                           max_new_tokens=500,\n","                                           model=MistralAWQForCausalLM(\n","  (model): Mistra...\n","                                                                  &#x27;padding&#x27;: &#x27;longest&#x27;},\n","                                           top_k=10, top_p=0.8),\n","             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n","                         &#x27;max_new_tokens&#x27;: [500], &#x27;no_repeat_ngram_size&#x27;: [0],\n","                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;]],\n","                         &#x27;top_k&#x27;: [10, 50, 60, 70, 80],\n","                         &#x27;top_p&#x27;: [0.7, 0.75, 0.8, 0.95]},\n","             scoring=make_scorer(get_score), verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n","                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;&gt;],\n","                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n","                    is_fitted_=True, max_new_tokens=500,\n","                    model=MistralAWQForCausalLM(\n","  (model): MistralForCausalLM(\n","    (model): LlamaLi...\n","\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","},\n","                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n","                                           &#x27;spaces_between_special_tokens&#x27;: False},\n","                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n","                                           &#x27;padding&#x27;: &#x27;longest&#x27;},\n","                    top_k=10, top_p=0.8)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content\"><pre>LLMEstimatorWrapper(batch_size=1,\n","                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40&gt;&gt;],\n","                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n","                    is_fitted_=True, max_new_tokens=500,\n","                    model=MistralAWQForCausalLM(\n","  (model): MistralForCausalLM(\n","    (model): LlamaLi...\n","\t32000: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32001: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","},\n","                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n","                                           &#x27;spaces_between_special_tokens&#x27;: False},\n","                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n","                                           &#x27;padding&#x27;: &#x27;longest&#x27;},\n","                    top_k=10, top_p=0.8)</pre></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["GridSearchCV(cv=2,\n","             estimator=LLMEstimatorWrapper(batch_size=1,\n","                                           callbacks_after_inference=[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>>],\n","                                           device='cuda:0', do_sample=True,\n","                                           generation_seed=42, is_fitted_=True,\n","                                           max_new_tokens=500,\n","                                           model=MistralAWQForCausalLM(\n","  (model): Mistra...\n","                                                                  'padding': 'longest'},\n","                                           top_k=10, top_p=0.8),\n","             param_grid={'do_sample': [True], 'generation_seed': [42],\n","                         'max_new_tokens': [500], 'no_repeat_ngram_size': [0],\n","                         'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>]],\n","                         'top_k': [10, 50, 60, 70, 80],\n","                         'top_p': [0.7, 0.75, 0.8, 0.95]},\n","             scoring=make_scorer(get_score), verbose=3)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["clf.fit(X=tuner_ob.dataset[\"_X\"], y=tuner_ob.dataset['_y'])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 150/150 [08:01<00:00,  3.21s/it]\n"]}],"source":["scores_after, outputs_after = tuner_ob.get_score(best_params)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8266666666666667 0.8066666666666666\n"]}],"source":["print(scores_before, scores_after)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["\"{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 500, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], 'top_k': 10, 'top_p': 0.8}\""]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["str(clf.best_params_)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["\n","d = {\n","    'scores_before' : scores_before,\n","    'scores_after' : scores_after,\n","    'outputs_before' : outputs_before,\n","    'outputs_after' : outputs_after,\n","    'best_params' : str(clf.best_params_),\n","}\n","\n","f = \"./gsm-8k-best-params-150s-capybara-7b.json\"\n","json_dump(d, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d = json_load(\"./gsm-8k-best-params-150s-capybara-7b.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 500, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7f8f9e357c40>], 'top_k': 10, 'top_p': 0.8}\n"]}],"source":["print(d['best_params'])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# harcoding from above file here due to notebook re-run\n","\n","best_params = {\n","    'do_sample' : True,\n","    'generation_seed' : 42,\n","    'max_new_tokens' : 500,\n","    'no_repeat_ngram_size' : 0,\n","    'stopping_criteria' : stopping_criteria,\n","    'top_k' : 10,\n","    'top_p' : 0.8\n","}"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 500/500 [00:00<00:00, 8301.84 examples/s]\n","  1%|          | 3/500 [00:13<37:57,  4.58s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# eval on test samples\u001b[39;00m\n\u001b[1;32m      3\u001b[0m gen_params1 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping_criteria\u001b[39m\u001b[38;5;124m\"\u001b[39m: stopping_criteria,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m oos_scores_before, oos_outputs_before \u001b[38;5;241m=\u001b[39m \u001b[43mtuner_ob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_params1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspace/llmsearch/llmsearch/tuner/tuner.py:423\u001b[0m, in \u001b[0;36mTuner.get_score\u001b[0;34m(self, generation_args, dataset)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     dataset_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_dataset(dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[0;32m--> 423\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_to_evaluate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_encode_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_encode_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_decode_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_decode_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_batch_size_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_batch_size_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_generation_param_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_generation_param_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_preproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_preproc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks_after_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_func(y_true\u001b[38;5;241m=\u001b[39mdataset_to_evaluate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_y\u001b[39m\u001b[38;5;124m\"\u001b[39m], y_pred\u001b[38;5;241m=\u001b[39my_pred)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score, y_pred\n","File \u001b[0;32m/workspace/llmsearch/llmsearch/utils/mem_utils.py:163\u001b[0m, in \u001b[0;36mbatch_without_oom_error.<locals>.inner_wrapper\u001b[0;34m(batch_size, disable_batch_size_cache, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Try running with specified batch size\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_batch_size_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_batch_size_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m         gc_cuda()\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m disable_batch_size_cache:\n","File \u001b[0;32m/workspace/llmsearch/llmsearch/utils/model_utils.py:122\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, tokenizer, is_encoder_decoder, batch_size, disable_batch_size_cache, device, model_inputs, tokenizer_encode_args, tokenizer_decode_args, generation_args, disable_generation_param_checks, return_optimal_batch_size, output_preproc, callbacks)\u001b[0m\n\u001b[1;32m    119\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoded_input\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    120\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoded_input\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 122\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_args\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/awq/models/base.py:111\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A generate function that mimics the HF generate function.\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/awq/modules/fused/model.py:119\u001b[0m, in \u001b[0;36mLlamaLikeModel.forward\u001b[0;34m(self, input_ids, attn_bias, attention_mask, is_causal, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    114\u001b[0m     h, mask \u001b[38;5;241m=\u001b[39m fused_utils\u001b[38;5;241m.\u001b[39mprepare_correct_devices(\n\u001b[1;32m    115\u001b[0m         layer,\n\u001b[1;32m    116\u001b[0m         h,\n\u001b[1;32m    117\u001b[0m         mask,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     h, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    125\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mh,\n\u001b[1;32m    126\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    128\u001b[0m     attentions\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    129\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/awq/modules/fused/block.py:128\u001b[0m, in \u001b[0;36mLlamaLikeBlock.forward\u001b[0;34m(self, hidden_states, past_key_value, attn_bias, attention_mask, is_causal)\u001b[0m\n\u001b[1;32m    121\u001b[0m attn_output, _, past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    122\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mnorm_out,\n\u001b[1;32m    123\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    124\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m h \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(attn_output\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m+\u001b[39m attn_output\n\u001b[0;32m--> 128\u001b[0m out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/awq/modules/linear/gemm.py:242\u001b[0m, in \u001b[0;36mWQLinear_GEMM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 242\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mWQLinearMMFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqzeros\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_dtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    254\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39minput_dtype)\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/awq/modules/linear/gemm.py:46\u001b[0m, in \u001b[0;36mWQLinearMMFunction.forward\u001b[0;34m(ctx, x, qweight, qzeros, scales, w_bit, group_size, bias, out_features)\u001b[0m\n\u001b[1;32m     44\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x, out)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mawq_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm_forward_cuda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqzeros\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     out \u001b[38;5;241m=\u001b[39m dequantize_gemm(qweight, qzeros, scales, w_bit, group_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# eval on test samples\n","\n","gen_params1 = {\n","    \"max_new_tokens\": 500,\n","    \"stopping_criteria\": stopping_criteria,\n","    \"generation_seed\": 42,\n","}\n","\n","oos_scores_before, oos_outputs_before = tuner_ob.get_score(gen_params1,test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0.564"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["oos_scores_before"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["{'question': 'John climbs up 9 flights of stairs.  Each flight is 10 feet.  If each step is 18 inches, how many steps does he climb up?',\n"," 'answer': 'He has to climb 9*10=<<9*10=90>>90 feet\\nThat means he needs to climb 90*12=<<90*12=1080>>1080 inches\\nThat means he needs to climb 1080/18=<<1080/18=60>>60 stairs\\n#### 60',\n"," 'X': '<|im_start|>system\\nSolve the following math problems, end with The answer is<|im_end|>\\n<|im_start|>user\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQ: John climbs up 9 flights of stairs.  Each flight is 10 feet.  If each step is 18 inches, how many steps does he climb up?<|im_end|>\\n<|im_start|>assistant\\n',\n"," 'actual input': 'John climbs up 9 flights of stairs.  Each flight is 10 feet.  If each step is 18 inches, how many steps does he climb up?'}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","oos_scores_after, oos_outputs_after = tuner_ob.get_score(best_params,test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(oos_scores_after)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 500/500 [00:00<00:00, 9790.44 examples/s]\n","100%|██████████| 500/500 [21:14<00:00,  2.55s/it]\n"]}],"source":["\n","oos_scores_after, oos_outputs_after = tuner_ob.get_score(clf.best_params_,test_dataset)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.564 0.584\n"]}],"source":["print(oos_scores_before, oos_scores_after)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["d = {\n","    'scores_before' : scores_before,\n","    'scores_after' : scores_after,\n","    'outputs_before' : outputs_before,\n","    'outputs_after' : outputs_after,\n","\n","    'oos_scores_before' : oos_scores_before,\n","    'oos_scores_after' : oos_scores_after,\n","    'oos_outputs_before' : oos_outputs_before,\n","    'oos_outputs_after' : oos_outputs_after,\n","    'best_params' : str(clf.best_params_),\n","}\n","\n","f = \"./gsm-8k-best-params-150s-capybara-7b.json\"\n","json_dump(d, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"llmsearch-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}
