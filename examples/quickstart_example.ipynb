{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "# Requires accelerate==0.27.2 py7zr==0.21.0 evaluate==0.4.0 rouge_score==0.1.2\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a generation param search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables that we will use later\n",
    "seed = 42\n",
    "batch_size = 2\n",
    "num_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16839c4e17a47d48dc882416b82e282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model & tokenizer\n",
    "model_id = \"cognitivecomputations/dolphin-2.9-llama3-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side = \"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.float16, device_map = \"auto\")\n",
    "\n",
    "# load dataset on which to run search on\n",
    "dataset = datasets.load_dataset(\"samsum\")['train']\n",
    "sample_dataset = dataset.shuffle(seed = seed).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional : Define stopping criteria for the generation, here we stop a generation of a sequence when `<|im_end|>` is reached\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[128256])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "# useful when batching to reset state variables for the stopping criteria\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "\n",
    "# create a function that can be useful for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "def get_rouge_score(y_true, y_pred):\n",
    "    return np.mean(rouge.compute(predictions=y_pred, references=[item['summary'] for item in y_true], use_stemmer=True, use_aggregator=False)['rouge2'])\n",
    "\n",
    "# Define a dataset preprocessor - Should take in tokenizer & kwargs and return a string that can be input directly to the model, here we apply chat template which most decoder models use\n",
    "def sample_to_chat_format(tokenizer, **kwargs):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : \"system\",\n",
    "            'content' : \"You are Dolphin, a helpful AI assistant.\"\n",
    "        },\n",
    "        {\n",
    "            'role' : \"user\",\n",
    "            'content' : f\"Summarize the following text: {kwargs['dialogue']}\"\n",
    "        }\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuner object, this preprocesses the dataset and creates an LLMEstimator to run with scikit-learn\n",
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=sample_dataset,\n",
    "    device=\"cuda:0\",\n",
    "    # the tuner module automatically reduces the batch size while running inference if it goes OOM\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\",'truncation' : True, \"add_special_tokens\": False, 'max_length' : 1024},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n",
    "    # pass in the scorer\n",
    "    scorer=get_rouge_score,\n",
    "    # pass in `dataset` preprocessor\n",
    "    sample_preprocessor=sample_to_chat_format,\n",
    "    seed=seed,\n",
    "    # column mapping used to identify input and evaluation columns (these columns are passed in to the evaluation function & the dataset preprocessor)\n",
    "    column_mapping={\"input_cols\": [\"dialogue\"], \"eval_cols\": [\"summary\"]},\n",
    "    # callbacks if any to run after each inference\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "Input: <|im_start|>system\n",
      "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Summarize the following text: Lucy: omg did you see JK this morning?\n",
      "Sue: I try to avoid it lol\n",
      "Lucy: you should have seen it it was disgusting\n",
      "Sue: I cant do it anymore i try to listen to the radio in the mornings.. jk makes you think the whole world is full of idiots lol\n",
      "Lucy: you may be right I dont know how some of them can go on there in public for the world to see\n",
      "Sue: I would die if I got a call to go on there lol\n",
      "Sue: could you imagine ha ha \n",
      "Lucy: I would piss myself If I saw you and Andy up there\n",
      "Sue: over my dead body !<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "Output: {'summary': \"Sue doesn't watch JK any more as it's disgusting.\"}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "Input: <|im_start|>system\n",
      "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Summarize the following text: Wendy: What's up?\n",
      "Simon: Nothing much. I'm painting my cupboards. \n",
      "Angela: Cool what colour?\n",
      "Simon: Green.\n",
      "Ben: I'm just chilling in the garden. \n",
      "Angela: Nice weekend! I'm about to meet Chris.\n",
      "Wendy: Say hello from me!\n",
      "Angela: Will do! And how is your weekend, Wendy?\n",
      "Wendy: Very lazy... The week was hard at work, I really needed some rest. \n",
      "Ben: We should all come and visit Simon in his new apartment!\n",
      "Simon: You are welcome, guys! Whenever you wish.\n",
      "Ben: I should be in Bournemouth next week. \n",
      "Simon: I'm not going anywhere :-)\n",
      "Ben: Cool, I'll call you next week. <|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "Output: {'summary': 'This weekend Wendy is very lazy because she worked hard at work, and Angela is meeting Chris. Simon is chilling in the garden and painting his cupboards green. Next week, Ben, Angela, Chris and Wendy will visit him in his new apartament.'}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "Input: <|im_start|>system\n",
      "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Summarize the following text: Petra: Hi Zack, I see you called. Sorry I can't pick up. In lectures all day.\n",
      "Zack: Ok, when will the break be I can try then.\n",
      "Petra: I will call you back in the lunch break because I am not sure if this lecturer will stick closely to the break times in the programme.\n",
      "Zack: OK. \n",
      "Petra: Or you can write to me what it's about. I can type  and read I just can't listen or talk.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "Output: {'summary': \"Zack called Petra, but she didn't answer because is in lectures all day. Petra will call Zack back during the break. Zack can write to Petra because she can read and write during the lecture. Petra can't talk or listen during the lecture. \"}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to see if dataset is processed as expected, Tuner populates `_X` with the processed input and `_y` with `column_mapping.eval_cols`\n",
    "print(f\"Inputs: \")\n",
    "for _x, _y in zip(tuner_ob.dataset['_X'][:3], tuner_ob.dataset['_y'][:3]):\n",
    "    print(f\"Input: {_x}\")\n",
    "    print('\\n')\n",
    "    print(f\"Output: {_y}\")\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print('---' * 15,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463546ad430a4da7b7c033c5c8ce4eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score - 0.09896745444141972\n"
     ]
    }
   ],
   "source": [
    "# 0.09896745444141972\n",
    "# Get score & outputs using some generation parameters\n",
    "gen_params = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "score, outputs = tuner_ob.get_score(gen_params)\n",
    "\n",
    "print(f\"Score - {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameter space here for the earch\n",
    "hyp_space = {\n",
    "    'max_new_tokens' : [70],\n",
    "    'stopping_criteria' : [stopping_criteria],\n",
    "    'generation_seed' : [42],\n",
    "    'do_sample' : [True],\n",
    "\n",
    "    'temperature': [0.1],\n",
    "    'top_k': [50],\n",
    "    'no_repeat_ngram_size': [0],\n",
    "}\n",
    "\n",
    "# Pass in estimator & scorer as you do with the scikit-learn API\n",
    "clf = GridSearchCV(\n",
    "    estimator = tuner_ob.estimator,\n",
    "    param_grid=hyp_space,\n",
    "    scoring = tuner_ob.scorer,\n",
    "    cv = 2,\n",
    "    n_jobs = None,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61f9469499a4c7c9c450a285cfa9b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>], temperature=0.1, top_k=50;, score=0.144 total time= 1.0min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb11bb803a944bf9c7653f76169211e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END do_sample=True, generation_seed=42, max_new_tokens=70, no_repeat_ngram_size=0, stopping_criteria=[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>], temperature=0.1, top_k=50;, score=0.128 total time=  59.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=2,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=50),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1], &#x27;top_k&#x27;: [50]},\n",
       "             scoring=make_scorer(get_rouge_score, response_method=&#x27;predict&#x27;),\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=2,\n",
       "                                           callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;&gt;],\n",
       "                                           device=&#x27;cuda:0&#x27;, do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (...\n",
       "                                                                  &#x27;truncation&#x27;: True},\n",
       "                                           top_k=50),\n",
       "             param_grid={&#x27;do_sample&#x27;: [True], &#x27;generation_seed&#x27;: [42],\n",
       "                         &#x27;max_new_tokens&#x27;: [70], &#x27;no_repeat_ngram_size&#x27;: [0],\n",
       "                         &#x27;stopping_criteria&#x27;: [[&lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;]],\n",
       "                         &#x27;temperature&#x27;: [0.1], &#x27;top_k&#x27;: [50]},\n",
       "             scoring=make_scorer(get_rouge_score, response_method=&#x27;predict&#x27;),\n",
       "             verbose=3)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: LLMEstimatorWrapper</label><div class=\"sk-toggleable__content fitted\"><pre>LLMEstimatorWrapper(batch_size=2,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70,\n",
       "                    model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(1282...\n",
       "\t128256: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128257: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;max_length&#x27;: 1024,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=50)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LLMEstimatorWrapper</label><div class=\"sk-toggleable__content fitted\"><pre>LLMEstimatorWrapper(batch_size=2,\n",
       "                    callbacks_after_inference=[&lt;bound method MultiTokenStoppingCriteria.reset of &lt;llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0&gt;&gt;],\n",
       "                    device=&#x27;cuda:0&#x27;, do_sample=True, generation_seed=42,\n",
       "                    is_fitted_=True, max_new_tokens=70,\n",
       "                    model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(1282...\n",
       "\t128256: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128257: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "},\n",
       "                    tokenizer_decode_args={&#x27;skip_special_tokens&#x27;: True,\n",
       "                                           &#x27;spaces_between_special_tokens&#x27;: False},\n",
       "                    tokenizer_encode_args={&#x27;add_special_tokens&#x27;: False,\n",
       "                                           &#x27;max_length&#x27;: 1024,\n",
       "                                           &#x27;padding&#x27;: &#x27;longest&#x27;,\n",
       "                                           &#x27;truncation&#x27;: True},\n",
       "                    top_k=50)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=LLMEstimatorWrapper(batch_size=2,\n",
       "                                           callbacks_after_inference=[<bound method MultiTokenStoppingCriteria.reset of <llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>>],\n",
       "                                           device='cuda:0', do_sample=True,\n",
       "                                           generation_seed=42, is_fitted_=True,\n",
       "                                           max_new_tokens=70,\n",
       "                                           model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (...\n",
       "                                                                  'truncation': True},\n",
       "                                           top_k=50),\n",
       "             param_grid={'do_sample': [True], 'generation_seed': [42],\n",
       "                         'max_new_tokens': [70], 'no_repeat_ngram_size': [0],\n",
       "                         'stopping_criteria': [[<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>]],\n",
       "                         'temperature': [0.1], 'top_k': [50]},\n",
       "             scoring=make_scorer(get_rouge_score, response_method='predict'),\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on the dataset\n",
    "clf.fit(X=tuner_ob.dataset[\"_X\"], y=tuner_ob.dataset['_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_sample': True, 'generation_seed': 42, 'max_new_tokens': 70, 'no_repeat_ngram_size': 0, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>], 'temperature': 0.1, 'top_k': 50}\n"
     ]
    }
   ],
   "source": [
    "# print out the best parameters\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf48320c1c044b2e92d730621f38a260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the tuned params\n",
    "# you can also get a score on another dataset by passing in the `dataset` to the `get_score` method as another param, note that it gets processed the same way the `dataset` passed in the `Tuner` class was processed\n",
    "scores, outputs = tuner_ob.get_score(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores - 0.12349276113207769\n"
     ]
    }
   ],
   "source": [
    "print(f\"Scores - {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_warning, set_verbosity_debug\n",
    "\n",
    "# set verbosity to debug, useful to debug model outputs\n",
    "set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function Tuner.preprocess_dataset.<locals>.<lambda> at 0x7feec1310550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function Tuner.preprocess_dataset.<locals>.<lambda> at 0x7feec1310550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078b11b7c5154a309f1be00ccb72099f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 19:55:39.202 - llmsearch.utils.mem_utils:154 - INFO - Starting inference with generation parameters - {'max_new_tokens': 70, 'stopping_criteria': [<llmsearch.scripts.stopping_criteria.MultiTokenStoppingCriteria object at 0x7feed80eb8b0>], 'generation_seed': 42}\n",
      "2024-05-23 19:55:39.203 - llmsearch.utils.mem_utils:158 - INFO - Performing inference with batch_size - 2\n",
      "2024-05-23 19:55:39.204 - llmsearch.utils.model_utils:98 - INFO - Detected generation type - Greedy Decoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42e3259b9b249ebbc8eac918eec0cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2024-05-23 19:56:40.565 - llmsearch.utils.model_utils:149 - DEBUG - Input - '<|im_start|>system\\nYou are Dolphin, a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text: Lucy: omg did you see JK this morning?\\r\\nSue: I try to avoid it lol\\r\\nLucy: you should have seen it it was disgusting\\r\\nSue: I cant do it anymore i try to listen to the radio in the mornings.. jk makes you think the whole world is full of idiots lol\\r\\nLucy: you may be right I dont know how some of them can go on there in public for the world to see\\r\\nSue: I would die if I got a call to go on there lol\\r\\nSue: could you imagine ha ha \\r\\nLucy: I would piss myself If I saw you and Andy up there\\r\\nSue: over my dead body !<|im_end|>\\n<|im_start|>assistant\\n'\n",
      "2024-05-23 19:56:40.567 - llmsearch.utils.model_utils:150 - DEBUG - Model Output - \"Lucy and Sue discuss JK's morning show, which Lucy finds disgusting. Sue prefers listening to the radio instead. They agree that some guests on the show make them feel like the world is full of idiots. The idea of Sue and Andy being on the show makes Lucy laugh, but Sue firmly rejects the idea.\\n\\n\\nSorry\"\n",
      "2024-05-23 19:56:40.568 - llmsearch.utils.model_utils:149 - DEBUG - Input - \"<|im_start|>system\\nYou are Dolphin, a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text: Wendy: What's up?\\r\\nSimon: Nothing much. I'm painting my cupboards. \\r\\nAngela: Cool what colour?\\r\\nSimon: Green.\\r\\nBen: I'm just chilling in the garden. \\r\\nAngela: Nice weekend! I'm about to meet Chris.\\r\\nWendy: Say hello from me!\\r\\nAngela: Will do! And how is your weekend, Wendy?\\r\\nWendy: Very lazy... The week was hard at work, I really needed some rest. \\r\\nBen: We should all come and visit Simon in his new apartment!\\r\\nSimon: You are welcome, guys! Whenever you wish.\\r\\nBen: I should be in Bournemouth next week. \\r\\nSimon: I'm not going anywhere :-)\\r\\nBen: Cool, I'll call you next week. <|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "2024-05-23 19:56:40.568 - llmsearch.utils.model_utils:150 - DEBUG - Model Output - 'Wendy, Simon, Angela, and Ben are having a conversation. Simon is painting his cupboards green. Angela is about to meet Chris and is spending her weekend doing activities outdoors. Wendy is enjoying a lazy weekend after a hard week at work. Ben suggests coming to visit Simon in his new apartment, and he plans to be in Bournemouth'\n",
      "2024-05-23 19:56:40.790 - llmsearch.utils.mem_utils:176 - DEBUG - Setting batch_size cache value - 2 for this particular configuration\n",
      "2024-05-23 19:56:40.791 - llmsearch.utils.mem_utils:188 - INFO - Finished running inference, took 61.586311 secs\n"
     ]
    }
   ],
   "source": [
    "# Calculate score on a different dataset\n",
    "scores, outputs = tuner_ob.get_score(gen_params, dataset = datasets.Dataset.from_dict(sample_dataset[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
