{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Check how the model pads input - done\n",
    "Check how different enecoding & decoding params affect the encoding & decoding - done\n",
    "check performance of model on a sample set at different batch sizes (inadvertently check how the model performs when padding is present) - till now 1 bs gives best performance\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import ctypes\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPTQConfig, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PreTrainedModel, PretrainedConfig, GenerationConfig, StoppingCriteria, AutoTokenizer, StoppingCriteriaList, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ctypes\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.utils.model_utils import batcher, decoder_parser\n",
    "\n",
    "import awq\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0+cu121', '0.2.4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_dataset = load_dataset(\"gsm8k\", 'main')\n",
    "\n",
    "torch.__version__, awq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def cm():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed for reproducibilty\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "def perform_single_example_inference(example, model, tokenizer,gen_kwargs):\n",
    "\n",
    "    tokenized_input = tokenizer(example, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    tokenized_input['input_ids'] = tokenized_input['input_ids'].to('cuda:0')\n",
    "\n",
    "    tokenized_input['attention_mask'] = tokenized_input['attention_mask'].to('cuda:0')\n",
    "    # tokenized_input.to(device)\n",
    "    # print(tokenized_input)\n",
    "\n",
    "    model_out = model.generate(**tokenized_input, **gen_kwargs)\n",
    "    prompt_tokens = len(tokenized_input['input_ids'][0])\n",
    "    print(f\"Prompt tokens - {prompt_tokens}\")\n",
    "    # print(model_out.tolist()[0])\n",
    "\n",
    "    output_token_ids = model_out.tolist()[0]\n",
    "    decoded_output = tokenizer.decode(output_token_ids, spaces_between_special_tokens = False)\n",
    "\n",
    "    print(decoded_output)\n",
    "    completion_tokens = len(output_token_ids) - prompt_tokens\n",
    "\n",
    "    print(f\"Completion Tokens - {completion_tokens}\")\n",
    "\n",
    "    return decoded_output, prompt_tokens, completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders\n",
    "def load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_kwargs,temp_model_dir, model_branch = \"main\"):\n",
    "    output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = model_branch)\n",
    "\n",
    "    model_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "    tokenizer_loader_kwargs['pretrained_model_name_or_path'] = output_folder\n",
    "\n",
    "    model_name_or_path = model_loader_kwargs.pop('pretrained_model_name_or_path')\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=model_name_or_path,\n",
    "        **model_loader_kwargs\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_kwargs, local_files_only=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model_loader_backend_map = {\n",
    "    # \"exllama_2_hf\": load_model_with_exllama_2_hf_backend,\n",
    "    # \"hf\": load_model_with_hf_backend,\n",
    "    # 'auto_gptq' : load_model_with_autogptq_backend,\n",
    "    'awq' : load_model_with_awq_backend,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to /workspace/temp_model_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.9k /17.9k  27.4MiB/s\n",
      "100%|██████████| 911   /911    3.49MiB/s\n",
      "100%|██████████| 51.0  /51.0   178kiB/s\n",
      "100%|██████████| 126   /126    641kiB/s\n",
      "100%|██████████| 420   /420    975kiB/ss\n",
      "100%|██████████| 115   /115    493kiB/s\n",
      "  0%|          | 15.7M /4.15G  46.3MiB/s\n",
      "100%|██████████| 493k  /493k   41.5MiB/s\n",
      "\n",
      "100%|██████████| 1.60k /1.60k  4.87MiB/s\n",
      "100%|██████████| 1.80M /1.80M  3.32MiB/s\n",
      "100%|██████████| 4.15G /4.15G  235MiB/s\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  5.94it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:02<00:00, 11.27it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "\n",
    "temp_model_dir = Path(f\"/workspace/temp_model_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "model_loader_kwargs = {\n",
    "    'device_map' : {'' : 0},\n",
    "    'fuse_layers' : True,\n",
    "}\n",
    "\n",
    "tokenizer_loader_kwargs = {\n",
    "    'use_fast' : False,\n",
    "    'legacy' : False,\n",
    "}\n",
    "\n",
    "model, tokenizer = load_model_with_awq_backend(model_id, model_loader_kwargs, tokenizer_loader_kwargs,temp_model_dir, model_branch = \"main\")\n",
    "\n",
    "# pad token is null in config -https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ/blob/eb64c310c44905321d012962db9ac0d47c3a64fa/tokenizer_config.json#L53\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10970.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "# Add prompt template\n",
    "processed_dataset = preprocess_dataset(gsm8k_dataset['train'], tokenizer,pt = pt, pt_cols = pt_cols, system_prompt = system_prompt, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    def extract_answer_from_out(s):\n",
    "        pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "        match = pattern.search(s)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t[\"answer\"].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "        # print(\"y_pred - \", y_p_answer)\n",
    "        # print(\"y_true - \", y_t_answer)\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.logging_utils import set_verbosity_info, set_verbosity_debug, set_verbosity_warning\n",
    "set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "bm_sample_size = 50\n",
    "bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size - 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'multi_token_stop_criteria_ob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m cm()\n\u001b[0;32m---> 11\u001b[0m stopping_criteria \u001b[38;5;241m=\u001b[39m StoppingCriteriaList([\u001b[43mmulti_token_stop_criteria_ob\u001b[49m])\n\u001b[1;32m     13\u001b[0m tuner_ob \u001b[38;5;241m=\u001b[39m Tuner(\n\u001b[1;32m     14\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     15\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     callbacks_after_inference \u001b[38;5;241m=\u001b[39m [multi_token_stop_criteria_ob\u001b[38;5;241m.\u001b[39mreset],\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# is stopping criteria cache not being reset properly?\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# check if reset condition is working properly\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_token_stop_criteria_ob' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "batch_size_list = [1, 2, 4, 8, 16, 32]\n",
    "score_at_diff_batch_sizes = {}\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "\n",
    "    cm()\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "\n",
    "    callbacks_after_inference = [multi_token_stop_criteria_ob.reset]\n",
    "\n",
    "    tuner_ob = Tuner(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=bm_samples,\n",
    "        device=\"cuda:0\",\n",
    "        batch_size=batch_size,\n",
    "        tokenizer_encode_args={\"padding\": \"longest\", \"add_special_tokens\": False},\n",
    "        tokenizer_decode_args={\"spaces_between_special_tokens\": False},\n",
    "        scorer=get_score,\n",
    "        prompt_template=\"{X}\",\n",
    "        seed=seed,\n",
    "        column_mapping={\"input_cols\": [\"X\"], \"eval_cols\": [\"answer\"]},\n",
    "        callbacks_after_inference=callbacks_after_inference,\n",
    "    )\n",
    "\n",
    "\n",
    "    # is stopping criteria cache not being reset properly?\n",
    "    # check if reset condition is working properly\n",
    "\n",
    "\n",
    "    gen_params1 = {\n",
    "        'max_new_tokens' : 500,\n",
    "        # max_new_tokens take precendece over stopping criteria\n",
    "        'stopping_criteria' : stopping_criteria,\n",
    "        'generation_seed' : 42,\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    scores, outputs = tuner_ob.get_score(gen_params1)\n",
    "    end = time.time()\n",
    "\n",
    "    score_at_diff_batch_sizes[batch_size] = {\n",
    "        'score' : scores,\n",
    "        'outputs' : outputs,\n",
    "        'optimal_batch_size' : tuner_ob.estimator._optimal_batch_size,\n",
    "        'latency_mins' : (end  - start) / 60,\n",
    "    }\n",
    "\n",
    "    print(f'score at batch size - {batch_size} - {scores}')\n",
    "\n",
    "# Done - Problem - Stopping criteria does not reset when generation reaches max new tokens so no stop token is generated\n",
    "# When does generation stop - max new tokens is reached or stop token is generated\n",
    "\n",
    "# 2m44s for 50 samples\n",
    "# 100 samples - 5m28s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side, tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in score_at_diff_batch_sizes.keys():\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "    print(f\"Score - {score_at_diff_batch_sizes[batch_size]['score']}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes[4]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token = eos token\n",
    "# padding side - left\n",
    "\n",
    "for batch_size in score_at_diff_batch_sizes.keys():\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "    print(f\"Score - {score_at_diff_batch_sizes[batch_size]['score']}\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(bm_samples[0]['X'], **{'padding': 'longest', 'add_special_tokens' : False})['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes[4]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the results\n",
    "\n",
    "# proves that llmsearch should be run on a batch size of 1\n",
    "\n",
    "for batch_size, score_dict in score_at_diff_batch_sizes.items():\n",
    "    print(f\"Batch Size - {batch_size}\")\n",
    "    print(f\"Score - {score_dict['score']}\")\n",
    "    print(f\"Latency - {score_dict['latency_mins']} mins\")\n",
    "    print(f\"Optimal Batch Size - {score_dict['optimal_batch_size']}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_at_diff_batch_sizes[batch_size] = {\n",
    "        'score' : scores,\n",
    "        'outputs' : outputs,\n",
    "        'optimal_batch_size' : tuner_ob.estimator._optimal_batch_size,\n",
    "        'latency_mins' : (end  - start) / 60,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.tuner import Tuner\n",
    "\n",
    "cm()\n",
    "batch_size = 1\n",
    "\n",
    "tuner_ob = Tuner(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset = bm_samples,\n",
    "    device = 'cuda:0',\n",
    "    batch_size = batch_size,\n",
    "    tokenizer_encode_args={'padding': 'longest', 'add_special_tokens' : False},\n",
    "    tokenizer_decode_args={'spaces_between_special_tokens' : False},\n",
    "    scorer = get_score,\n",
    "    prompt_template = langchain.PromptTemplate.from_template(\"{X}\"),\n",
    "    is_encoder_decoder = False,\n",
    "    seed = seed,\n",
    "    column_mapping = {'input_cols' : [\"X\"],'eval_cols' : ['answer']},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ob._optimal_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder parser is working as expected\n",
    "# TODO : check scores at different bs then llmsearch\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MultiTokenEOSCriteria(sequence_ids = [32000])])\n",
    "\n",
    "gen_params1 = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)\n",
    "\n",
    "scores_before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
