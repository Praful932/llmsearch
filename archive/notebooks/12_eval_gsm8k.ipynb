{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n",
      "0.2.4 2.2.0+cu121 4.38.2 0.1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "llmsearch on gsm8k awq with batch size 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch')\n",
    "\n",
    "import awq\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import llmsearch\n",
    "\n",
    "print(awq.__version__, torch.__version__, transformers.__version__, llmsearch.__version__)\n",
    "\n",
    "def pretty_print_dict(d, indent = 4):\n",
    "    print(json.dumps(d, indent = indent, default = str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import StoppingCriteriaList, AutoTokenizer\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1\n",
    "bm_sample_size = 500\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import StoppingCriteriaList, AutoTokenizer\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how params found on 200 performs on 500\n",
    "\n",
    "def load_model_and_tokenizer(model_id, temp_model_dir):\n",
    "    temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_folder = download_model_from_hf(model_id, save_dir=temp_model_dir, branch=\"main\")\n",
    "\n",
    "    gc_cuda()\n",
    "\n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=output_folder, fuse_layers=True, device_map={\"\": device}, local_files_only=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        output_folder, local_files_only=True, legacy=False, use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    def preprocess_dataset(\n",
    "        dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt=True\n",
    "    ):\n",
    "\n",
    "        def wrapper(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            messages = (\n",
    "                []\n",
    "                if system_prompt is None\n",
    "                else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            )\n",
    "            formatted_pt = pt.format(**{pt_col: sample[pt_col] for pt_col in pt_cols})\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": formatted_pt,\n",
    "                }\n",
    "            )\n",
    "            formatted_pt_with_ct = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=add_generation_prompt\n",
    "            )\n",
    "            return formatted_pt_with_ct\n",
    "\n",
    "        def actual_input(sample):\n",
    "            \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "            return sample[pt_cols[0]]\n",
    "\n",
    "        pt_dataset = dataset.map(\n",
    "            lambda sample: {\n",
    "                \"X\": wrapper(sample),\n",
    "                \"actual input\": actual_input(sample),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return pt_dataset\n",
    "\n",
    "\n",
    "    # 2-shot prompt template - https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n",
    "    pt = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\"\n",
    "    )\n",
    "    pt_cols = [\"question\"]\n",
    "    system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "    gsm8k_dataset = datasets.load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "\n",
    "    processed_dataset = preprocess_dataset(\n",
    "        gsm8k_dataset[\"train\"],\n",
    "        tokenizer,\n",
    "        pt=pt,\n",
    "        pt_cols=pt_cols,\n",
    "        system_prompt=system_prompt,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    bm_samples = processed_dataset.shuffle(seed=seed).select(range(bm_sample_size))\n",
    "    return bm_samples\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    def extract_answer_from_out(s):\n",
    "        pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "        match = pattern.search(s)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t[\"answer\"].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "        # print(\"y_pred - \", y_p_answer)\n",
    "        # print(\"y_true - \", y_t_answer)\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 911   /911    6.74MiB/s\n",
      "100%|██████████| 17.9k /17.9k  45.1MiB/s\n",
      "\n",
      "100%|██████████| 51.0  /51.0   17.1kiB/s\n",
      "100%|██████████| 115   /115    67.8kiB/s\n",
      "100%|██████████| 126   /126    534kiB/s\n",
      "100%|██████████| 420   /420    1.43MiB/s\n",
      "  0%|          | 0.00  /1.80M  ?iB/s \n",
      "100%|██████████| 1.60k /1.60k  5.53MiB/s\n",
      "100%|██████████| 1.80M /1.80M  8.67MiB/s\n",
      "100%|██████████| 493k  /493k   14.6MiB/s\n",
      "100%|██████████| 4.15G /4.15G  205MiB/s\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  6.98it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:02<00:00, 13.25it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a1e7f8a35c488cb9b0aee7b543b3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model, Tokenizer, Dataset\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, temp_model_dir)\n",
    "\n",
    "# Dataset we will use to find the best generation parameters\n",
    "bm_samples = load_dataset()\n",
    "\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ade4975ea94247a698548ed7cad52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=bm_samples,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\", \"add_special_tokens\": False},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False},\n",
    "    scorer=get_score,\n",
    "    prompt_template=\"{X}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"X\"], \"eval_cols\": [\"answer\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18b7da290ab4b0cab637ad0f6025717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores before tuning -  0.612\n"
     ]
    }
   ],
   "source": [
    "gen_params1 = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"stopping_criteria\": stopping_criteria,\n",
    "    \"generation_seed\": 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)\n",
    "\n",
    "print(\"Scores before tuning - \", scores_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c99e2feba64c87bbdf5d63c0d281cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_params2 = {\n",
    "    'do_sample' : True,\n",
    "    'temperature': 0.9,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "\n",
    "    'no_repeat_ngram_size' : 0,\n",
    "    'max_new_tokens' : 500,\n",
    "    # max_new_tokens take precendece over stopping criteria\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_after, outputs_after = tuner_ob.get_score(gen_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores after tuning -  0.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores after tuning - \", scores_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612 0.62\n"
     ]
    }
   ],
   "source": [
    "print(scores_before, scores_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
