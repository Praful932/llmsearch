{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "# Requires accelerate==0.27.2, py7zr==0.21.0 , evaluate==0.4.0, rouge_score==0.1.2\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.35s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"cognitivecomputations/dolphin-2.9-llama3-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side = \"left\", max_length = 1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.float16, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"samsum\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1\n",
    "sample_dataset = dataset.shuffle(seed = seed).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids=[128256])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n",
    "callbacks_after_inference = [multi_token_stop_criteria_ob.reset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rouge_score  = lambda y_pred, y_true : np.mean(rouge.compute(predictions=y_pred, references=y_true['summary'], use_stemmer=True, use_aggregator=False)['rouge2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_rouge_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m123\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m451\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_rouge_score  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true : np\u001b[38;5;241m.\u001b[39mmean(rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, references\u001b[38;5;241m=\u001b[39my_true, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_aggregator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "get_rouge_score([\"123\"], [\"451\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=sample_dataset,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\",'truncation' : True, \"add_special_tokens\": False, 'max_length' : 1024},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n",
    "    scorer=get_rouge_score,\n",
    "    prompt_template=\"Summarize : {dialogue}\",\n",
    "    seed=seed,\n",
    "    column_mapping={\"input_cols\": [\"dialogue\"], \"eval_cols\": [\"summary\"]},\n",
    "    callbacks_after_inference=callbacks_after_inference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:14<00:56, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 2/5 [01:08<01:52, 37.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 3/5 [01:28<00:59, 29.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 4/5 [02:21<00:38, 38.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 5/5 [03:16<00:00, 39.22s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m gen_params1 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m70\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopping_criteria\u001b[39m\u001b[38;5;124m'\u001b[39m : stopping_criteria,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration_seed\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m scores_before, outputs_before \u001b[38;5;241m=\u001b[39m \u001b[43mtuner_ob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_params1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/llmsearch/llmsearch/tuner/tuner.py:421\u001b[0m, in \u001b[0;36mTuner.get_score\u001b[0;34m(self, generation_args, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m     dataset_to_evaluate \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m sample: {\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_X\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m         }\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m run_inference(\n\u001b[1;32m    407\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    408\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mcallbacks_after_inference,\n\u001b[1;32m    420\u001b[0m )\n\u001b[0;32m--> 421\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_to_evaluate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score, y_pred\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_rouge_score  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true : np\u001b[38;5;241m.\u001b[39mmean(rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, references\u001b[38;5;241m=\u001b[39my_true, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_aggregator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "gen_params1 = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "}\n",
    "\n",
    "scores_before, outputs_before = tuner_ob.get_score(gen_params1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsearch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
