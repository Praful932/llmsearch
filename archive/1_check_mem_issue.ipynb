{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/llmsearch/')\n",
    "\n",
    "import awq\n",
    "import torch\n",
    "import transformers\n",
    "import llmsearch\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList\n",
    "\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 2\n",
    "num_samples = 10\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at Praful932/dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Praful932/dolphin-2.2.1-mistral-7b-samsum-ft-v1-GPTQ\"\n",
    "revision = \"main\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,revision = revision)\n",
    "tokenizer.padding_side = \"left\"\n",
    "# model = AutoAWQForCausalLM.from_quantized(\n",
    "#         model_id, fuse_layers=True, device_map={\"\": device}, revision = revision\n",
    "#     )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": device})\n",
    "\n",
    "dataset = datasets.load_dataset(\"samsum\")['train']\n",
    "sample_dataset = dataset.shuffle(seed = seed).select(range(num_samples))\n",
    "test_dataset = copy.deepcopy(datasets.Dataset.from_dict(sample_dataset[:2]))\n",
    "\n",
    "# These are required to make the model end the sequence correctly - https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm\n",
    "terminators = [\n",
    "    128001,\n",
    "    128009,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['13681220', '13716809'],\n",
       " 'dialogue': ['Lucy: omg did you see JK this morning?\\r\\nSue: I try to avoid it lol\\r\\nLucy: you should have seen it it was disgusting\\r\\nSue: I cant do it anymore i try to listen to the radio in the mornings.. jk makes you think the whole world is full of idiots lol\\r\\nLucy: you may be right I dont know how some of them can go on there in public for the world to see\\r\\nSue: I would die if I got a call to go on there lol\\r\\nSue: could you imagine ha ha \\r\\nLucy: I would piss myself If I saw you and Andy up there\\r\\nSue: over my dead body !',\n",
       "  \"Wendy: What's up?\\r\\nSimon: Nothing much. I'm painting my cupboards. \\r\\nAngela: Cool what colour?\\r\\nSimon: Green.\\r\\nBen: I'm just chilling in the garden. \\r\\nAngela: Nice weekend! I'm about to meet Chris.\\r\\nWendy: Say hello from me!\\r\\nAngela: Will do! And how is your weekend, Wendy?\\r\\nWendy: Very lazy... The week was hard at work, I really needed some rest. \\r\\nBen: We should all come and visit Simon in his new apartment!\\r\\nSimon: You are welcome, guys! Whenever you wish.\\r\\nBen: I should be in Bournemouth next week. \\r\\nSimon: I'm not going anywhere :-)\\r\\nBen: Cool, I'll call you next week. \"],\n",
       " 'summary': [\"Sue doesn't watch JK any more as it's disgusting.\",\n",
       "  'This weekend Wendy is very lazy because she worked hard at work, and Angela is meeting Chris. Simon is chilling in the garden and painting his cupboards green. Next week, Ben, Angela, Chris and Wendy will visit him in his new apartament.']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.Dataset.from_dict(sample_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that can be used for evaluation, should take in y_true (list[dict]), y_pred (list) and return a single value\n",
    "rouge = evaluate.load('rouge')\n",
    "def get_rouge_score(y_true : list, y_pred : list):\n",
    "    return np.mean(rouge.compute(predictions=y_pred, references=[item['summary'] for item in y_true], use_stemmer=True, use_aggregator=False)['rouge2'])\n",
    "\n",
    "# Define a dataset preprocessor that is called for every example in the dataset separately - Should take in tokenizer & kwargs and return a string that can be input directly to the model, here we apply chat template which most decoder models use\n",
    "def sample_to_chat_format(tokenizer, **kwargs):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : \"system\",\n",
    "            'content' : \"You are a helpful AI assistant.\"\n",
    "        },\n",
    "        {\n",
    "            'role' : \"user\",\n",
    "            'content' : f\"Summarize the following text in less than 50 words: {kwargs['dialogue']}\"\n",
    "        }\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuner object, this preprocesses the dataset and creates an LLMEstimator that can be run with GridSearchCV / RandomizedSearchCV of scikit-learn\n",
    "tuner_ob = Tuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=sample_dataset,\n",
    "    device=\"cuda:0\",\n",
    "    # the tuner module automatically reduces the batch size while running inference if it goes OOM\n",
    "    batch_size=batch_size,\n",
    "    tokenizer_encode_args={\"padding\": \"longest\",'truncation' : True, \"add_special_tokens\": False, 'max_length' : 1024},\n",
    "    tokenizer_decode_args={\"spaces_between_special_tokens\": False, 'skip_special_tokens' : True},\n",
    "    # pass in the scorer that we will be used to evaluate (input to this function is a batch)\n",
    "    scorer=get_rouge_score,\n",
    "    # pass in `dataset` preprocessor, this is run on the passed in dataset before feeding into the model, input of this function is a single example\n",
    "    sample_preprocessor=sample_to_chat_format,\n",
    "    seed=seed,\n",
    "    # column mapping used to identify input and evaluation columns (these columns are passed in to the evaluation function (scorer) & the dataset preprocessor(sample_preprocessor))\n",
    "    column_mapping={\"input_cols\": [\"dialogue\"], \"eval_cols\": [\"summary\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Lucy: omg did you see JK this morning?\\r\\nSue: I try to avoid it lol\\r\\nLucy: you should have seen it it was disgusting\\r\\nSue: I cant do it anymore i try to listen to the radio in the mornings.. jk makes you think the whole world is full of idiots lol\\r\\nLucy: you may be right I dont know how some of them can go on there in public for the world to see\\r\\nSue: I would die if I got a call to go on there lol\\r\\nSue: could you imagine ha ha \\r\\nLucy: I would piss myself If I saw you and Andy up there\\r\\nSue: over my dead body !<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Wendy: What's up?\\r\\nSimon: Nothing much. I'm painting my cupboards. \\r\\nAngela: Cool what colour?\\r\\nSimon: Green.\\r\\nBen: I'm just chilling in the garden. \\r\\nAngela: Nice weekend! I'm about to meet Chris.\\r\\nWendy: Say hello from me!\\r\\nAngela: Will do! And how is your weekend, Wendy?\\r\\nWendy: Very lazy... The week was hard at work, I really needed some rest. \\r\\nBen: We should all come and visit Simon in his new apartment!\\r\\nSimon: You are welcome, guys! Whenever you wish.\\r\\nBen: I should be in Bournemouth next week. \\r\\nSimon: I'm not going anywhere :-)\\r\\nBen: Cool, I'll call you next week. <|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Petra: Hi Zack, I see you called. Sorry I can't pick up. In lectures all day.\\r\\nZack: Ok, when will the break be I can try then.\\r\\nPetra: I will call you back in the lunch break because I am not sure if this lecturer will stick closely to the break times in the programme.\\r\\nZack: OK. \\r\\nPetra: Or you can write to me what it's about. I can type  and read I just can't listen or talk.<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Amelia: Want to go shopping tomorrow? :)\\r\\nAnna: can't :(\\r\\nAnna: I'm meeting my study group in the morning\\r\\nAmelia: noon?\\r\\nAnna: I'm visiting my grandma, she turns 86\\r\\nAnna: Then I have to do some work cause I am sooo behind and have to make up for it by Monday\\r\\nAmelia: Omg that doesn't sound like a Sunday at all :(\\r\\nAnna: and then in the evening\\r\\nAnna: I have to help my mum cleaning windows\\r\\nAmelia: cherry on top\\r\\nAnna: I'm looking forward to Monday :(\\r\\nAmelia: shopping next weekend perhaps? :)\\r\\nAmelia: <file_photo><|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " '<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Niki: Guess what\\r\\nJeanna: Hmmm? ;p\\r\\nNiki: Angel has a boyfriend\\r\\nJeanna: No wayyyyy!!! She looks like an elephant!!!\\r\\nNiki: I know, she’s the most disgusting person I ever seen, the day she eats…\\r\\nJeanna: And WHEN she eats!\\r\\nNiki: I know, during the class, in front of the teacher\\r\\nJeanna: It’s not the worst thing… How does he cope with the smell??!!!\\r\\nNiki: I have no fucking idea :D\\r\\nJeanna: Maybe he’s as disgusting as her\\r\\nNiki: Nobody knows it, she told a few people and then they heard them talking at the phone\\r\\nJeanna: It’s ridiculous!\\r\\nNiki: YES it is xD\\r\\nJeanna: He must be as pathetic as hear, no other option\\r\\nNiki: We’ll see, she will surely bring him here for everyone to see ;D\\r\\nJeanna: Yea, that would be typical for her ;p<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Gabriel: I'm picking up my new car today!\\r\\nCharles: You bought a car?\\r\\nGabriel: Well, yes and no.\\r\\nCharles: Parents bought it for you?\\r\\nGabriel: The lent me some money. I had some of my own and took a loan on the rest. \\r\\nCharles: Did you buy a Lamborghini or what? How much did it cost?\\r\\nGabriel: 55k.\\r\\nCharles: So a new one?\\r\\nGabriel: Yup. \\r\\nCharles: Tell me about it. \\r\\nGabriel: Mercedes-Benz, sedan, 180hp, 0-100 kmh in 6.2 seconds\\r\\nCharles: Nice!\\r\\nGabriel: I know :) they just gave ma a call that after I provide them with insurance data, I can pick it up.\\r\\nCharles: So going to insure it now?\\r\\nGabriel: Yeah, have to. Wanna go for a ride later?\\r\\nCharles: Hell yeah! Will you let me drive?\\r\\nGabriel: Not in a million years!\\r\\nCharles: Still ;)<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Danny: geez, it's so boooring\\r\\nKate: tell me about it *snooze*\\r\\nDanny: have you heard about Frank and Sue?\\r\\nKate: ? no? what about them?\\r\\nDanny: I've heard they're dating\\r\\nKate: pfff, says who\\r\\nDanny: Michael\\r\\nKate: like he knows\\r\\nDanny: Hahaha, i know, right? :D\\r\\nKate: I'd rather believe Frank and Mike are dating :P<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Jenna: I'm cooking 2nite :)\\r\\nEaton: yeah we know\\r\\nLilly: so whatever are we going to eat?\\r\\nJenna: chicken? indian way?\\r\\nLilly: with rice? yummy\\r\\nEaton: fine w me as long as you let me add my spices this time\\r\\nJenna: if you don't add them to my food i don't care\\r\\nLilly: kids, cut it, we'll enjoy it anyway<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " \"<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Toby: Maria, are you moving away?\\nMaria: yes, my parents decided to move next month\\nEllen: wow, already? Have you found a new place?\\nMaria: seems so\\nJeff: in Louisiana?\\nMaria: LoL, I wish it was Louisiana\\nJeff: what?\\nMaria: Toby didn't tell you?\\nToby: I didn't know if you want me to tell anybody\\nMaria: oh, crap\\nMaria: My parents decided to move to Costa Rica as soon as my father retires\\nJeff: oh no, that's crazy\\nMaria: I think so too<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " '<|im_start|>system\\nYou are a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nSummarize the following text in less than 50 words: Monica: What time are you coming home?\\nLarry: 6 \\nFiona: 7:30<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_ob.dataset['_X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example Logs from the get score function - Calculate score on a different dataset\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gen_params = {\n",
    "    'max_new_tokens' : 70,\n",
    "    'generation_seed' : 42,\n",
    "    'eos_token_id' : terminators,\n",
    "}\n",
    "\n",
    "scores, outputs = tuner_ob.get_score(gen_params, dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "gc_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 289.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "input_cols = tuner_ob.input_cols\n",
    "eval_cols = tuner_ob.eval_cols\n",
    "\n",
    "\n",
    "processed_dataset = test_dataset.map(\n",
    "                lambda sample: {\n",
    "                    \"_X\": sample_to_chat_format(tokenizer, **{col: sample[col] for col in input_cols + eval_cols}),\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
