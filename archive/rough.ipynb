{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocompletion\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llmsearch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-08 19:39:39.807 - llmsearch.utils.mem_utils:28 - WARNING - pynvml package not found in a CUDA environment, Install it by running - `pip install nvidia-ml-py3` for betting caching of batch size while running hyp param search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkey Patching .generate function of `transformers` library\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import StoppingCriteriaList, AutoTokenizer\n",
    "\n",
    "from llmsearch.tuner import Tuner\n",
    "from llmsearch.utils.mem_utils import gc_cuda\n",
    "from llmsearch.utils.model_downloader import download_model_from_hf\n",
    "from llmsearch.scripts.stopping_criteria import MultiTokenStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt = True):\n",
    "\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = [] if system_prompt is None else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        formatted_pt = pt.format(**{pt_col : sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt=add_generation_prompt)\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample : {\n",
    "            \"X\" : wrapper(sample),\n",
    "            'actual input' : actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "\n",
    "    def extract_answer_from_out(s):\n",
    "        pattern = re.compile(r\"The answer is (\\d+(?:\\.\\d+)?)\")\n",
    "        match = pattern.search(s)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_t_answer = y_t['answer'].split(\"####\")[-1].strip()\n",
    "        y_p_answer = extract_answer_from_out(y_p)\n",
    "\n",
    "\n",
    "        if y_t_answer == y_p_answer:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to temp_dir/TheBloke_CapybaraHermes-2.5-Mistral-7B-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.9k /17.9k  35.9MiB/s\n",
      "100%|██████████| 51.0  /51.0   152kiB/s\n",
      "100%|██████████| 911   /911    2.85MiB/s\n",
      "100%|██████████| 115   /115    279kiB/s\n",
      "100%|██████████| 126   /126    432kiB/s\n",
      "100%|██████████| 420   /420    1.73MiB/s\n",
      "  0%|          | 0.00  /4.15G  ?iB/s \n",
      "100%|██████████| 1.60k /1.60k  5.12MiB/s\n",
      "  1%|          | 21.0M /4.15G  206MiB/s\n",
      "100%|██████████| 493k  /493k   10.4MiB/s\n",
      "100%|██████████| 1.80M /1.80M  4.22MiB/s\n",
      "100%|██████████| 4.15G /4.15G  367MiB/s\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.30it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:02<00:00, 11.17it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load dataset, model, tokenizer\n",
    "seed = 42\n",
    "gsm8k_dataset = datasets.load_dataset(\"gsm8k\", 'main')\n",
    "model_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ\"\n",
    "\n",
    "temp_model_dir = Path(f\"./temp_dir/\")\n",
    "temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "output_folder = download_model_from_hf(model_id, save_dir = temp_model_dir, branch = 'main')\n",
    "\n",
    "gc_cuda()\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "        quant_path=output_folder,\n",
    "        fuse_layers = True,\n",
    "        device_map = {'' : 0}\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_folder, local_files_only=True, legacy = False, use_fast = False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10711.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(\n",
    "    dataset, tokenizer, pt, pt_cols, system_prompt, add_generation_prompt=True\n",
    "):\n",
    "    def wrapper(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        messages = (\n",
    "            []\n",
    "            if system_prompt is None\n",
    "            else [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        )\n",
    "        formatted_pt = pt.format(**{pt_col: sample[pt_col] for pt_col in pt_cols})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_pt,\n",
    "            }\n",
    "        )\n",
    "        formatted_pt_with_ct = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=add_generation_prompt\n",
    "        )\n",
    "        return formatted_pt_with_ct\n",
    "\n",
    "    def actual_input(sample):\n",
    "        \"\"\"Takes in a sample, formats it using prompt template, applies chat template and returns the formatted string\"\"\"\n",
    "        return sample[pt_cols[0]]\n",
    "\n",
    "    pt_dataset = dataset.map(\n",
    "        lambda sample: {\n",
    "            \"X\": wrapper(sample),\n",
    "            \"actual input\": actual_input(sample),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pt_dataset\n",
    "\n",
    "pt = textwrap.dedent(\"\"\"\\\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "    A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "    Q: {question}\"\"\")\n",
    "pt_cols = ['question']\n",
    "system_prompt = \"Solve the following math problems, end with The answer is\"\n",
    "\n",
    "# Add prompt template\n",
    "processed_dataset = preprocess_dataset(gsm8k_dataset['train'], tokenizer,pt = pt, pt_cols = pt_cols, system_prompt = system_prompt, add_generation_prompt = True)\n",
    "\n",
    "bm_sample_size = 10\n",
    "bm_samples = processed_dataset.shuffle(seed = seed).select(range(bm_sample_size))\n",
    "\n",
    "# setup\n",
    "multi_token_stop_criteria_ob = MultiTokenStoppingCriteria(sequence_ids = [32000])\n",
    "stopping_criteria = StoppingCriteriaList([multi_token_stop_criteria_ob])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1487.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tuner_ob = Tuner(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset = bm_samples,\n",
    "    device = 'cuda:0',\n",
    "    batch_size = 1,\n",
    "    tokenizer_encode_args={'padding': 'longest', 'add_special_tokens' : False},\n",
    "    tokenizer_decode_args={'spaces_between_special_tokens' : False},\n",
    "    scorer = get_score,\n",
    "    prompt_template = \"{X}\",\n",
    "    is_encoder_decoder = False,\n",
    "    seed = seed,\n",
    "    column_mapping = {'input_cols' : [\"X\"],'eval_cols' : ['answer']},\n",
    "    callbacks_after_inference = [multi_token_stop_criteria_ob.reset],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:34<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 \n",
      "\n",
      "\n",
      "A: Mimi picked up 2 dozen seashells, which is 2 * 12 = 24 seashells. Kyle found twice as many shells as Mimi, so he found 24 * 2 = 48 seashells. Leigh grabbed one-third of the shells that Kyle found, so Leigh had 48 / 3 = 16 seashells. The answer is 16.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: Let's use variables to represent the number of each type of pet:\n",
      "\n",
      "Let C = number of cats\n",
      "Let S = number of snakes\n",
      "Let P = number of parrots\n",
      "\n",
      "We are given the following information:\n",
      "\n",
      "1. S = C + 6 (Frankie has six more snakes than cats)\n",
      "2. P = C - 1 (Frankie has one less parrot than cats)\n",
      "3. There are 6 pets with 4 legs, and Frankie has 2 dogs, so there are S + P + 2 dogs with 4 legs.\n",
      "4. The total number of pets is C + S + P + 2 dogs.\n",
      "\n",
      "Now, we can use the given information to solve for the total number of pets:\n",
      "\n",
      "From (1), we have S = C + 6.\n",
      "From (2), we have P = C - 1.\n",
      "\n",
      "Substitute these expressions into the total number of pets equation:\n",
      "\n",
      "C + (C + 6) + (C - 1) + 2 = total number of pets\n",
      "\n",
      "Combine the terms:\n",
      "\n",
      "3C + 5 = total number of pets\n",
      "\n",
      "Now, we need to find the value of C. Since Frankie has 2 dogs, we can use this information to find the value of C:\n",
      "\n",
      "C + 2 = 6 (because 6 pets have 4 legs)\n",
      "\n",
      "Subtract 2 from both sides:\n",
      "\n",
      "C = 4\n",
      "\n",
      "Now that we know C = 4, we can find the total number of pets:\n",
      "\n",
      "Total number of pets = C + S + P + 2\n",
      "Total number of pets = 4 + (4 + 6) + (4 - 1) + 2\n",
      "Total number of pets = 4 + 10 + 3 + 2\n",
      "Total number of pets = 19\n",
      "\n",
      "The answer is 19.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: Let's break down the information given:\n",
      "\n",
      "1. Olaf initially had 150 toy cars.\n",
      "2. Grandpa gave Olaf twice as many as Uncle, so Grandpa gave 2 * Uncle's gift.\n",
      "3. Dad gave Olaf 10 toy cars, 5 less than Mum, so Mum gave Olaf 10 + 5 = 15 toy cars.\n",
      "4. Auntie gave Olaf 6 toy cars, 1 more than Uncle, so Uncle gave Olaf 6 - 1 = 5 toy cars.\n",
      "\n",
      "Now let's calculate how many toy cars each family member gave:\n",
      "\n",
      "1. Uncle's gift: 5 toy cars\n",
      "2. Grandpa's gift: 2 * Uncle's gift = 2 * 5 = 10 toy cars\n",
      "3. Dad's gift: 10 toy cars\n",
      "4. Mum's gift: 15 toy cars\n",
      "5. Auntie's gift: 6 toy cars\n",
      "\n",
      "Now let's add all the gifts together:\n",
      "\n",
      "Total gifts = Uncle's gift + Grandpa's gift + Dad's gift + Mum's gift + Auntie's gift\n",
      "Total gifts = 5 + 10 + 10 + 15 + 6 = 56 toy cars\n",
      "\n",
      "So, Olaf received a total of 56 toy cars from his family. Adding this to his initial collection, we get:\n",
      "\n",
      "Total toy cars = Initial collection + Total gifts\n",
      "Total toy cars = 150 + 56 = 206 toy cars\n",
      "\n",
      "The answer is 206.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: Emma starts with $100 in her account. She spends $8 each day for a week, which is 7 days. So she spends 7 * $8 = $56 during the week. She has $100 - $56 = $44 remaining in her account. She asks for as many $5 bills as her account can give her, which is $44 / $5 = 8 $5 bills. Each $5 bill is worth $5, so she takes out 8 * $5 = $40. She leaves the remaining $44 - $40 = $4 in her account. The answer is $4.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: Ezekiel hiked 10 kilometers on the first day and half the full hike distance on the second day, which is 50 kilometers * 0.5 = 25 kilometers. So, on the third day, he had to hike the remaining distance, which is 50 kilometers - 10 kilometers - 25 kilometers = 15 kilometers. The answer is 15.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: On the first day, James collects 50 tins. On the second day, he collects 3 times that number, which is 3 * 50 = 150 tins. On the third day, he collects 50 tins fewer than the second day, which is 150 - 50 = 100 tins.\n",
      "\n",
      "So far, he has collected 50 + 150 + 100 = 300 tins in the first three days. He wants to collect 500 tins in total, so he needs to collect 500 - 300 = 200 tins in the remaining days of the week.\n",
      "\n",
      "Since he collects an equal number of tins each day for the rest of the week, he needs to collect 200 tins / 4 days = 50 tins per day. The answer is 50 tins per day for the rest of the week.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: The bullseye is worth 50 points. The third dart is worth half the points of the bullseye, so it's worth 50 / 2 = 25 points. The third dart missed the target, so it received 0 points. The final score is the sum of all the points: 50 (bullseye) + 25 (third dart) + 0 (second dart) = 75. The answer is 75.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: First, we need to find the total width of the splashes made by each type of rock.\n",
      "\n",
      "For pebbles, TreQuan throws 6 of them, and each pebble makes a splash 1/4 meter wide. So the total width of the splashes made by pebbles is 6 * 1/4 = 6/4 = 1.5 meters.\n",
      "\n",
      "For rocks, TreQuan throws 3 of them, and each rock makes a splash 1/2 meter wide. So the total width of the splashes made by rocks is 3 * 1/2 = 3/2 = 1.5 meters.\n",
      "\n",
      "For boulders, TreQuan throws 2 of them, and each boulder makes a splash 2 meters wide. So the total width of the splashes made by boulders is 2 * 2 = 4 meters.\n",
      "\n",
      "Now, we add up the total width of the splashes made by all types of rocks: 1.5 (pebbles) + 1.5 (rocks) + 4 (boulders) = 7 meters.\n",
      "\n",
      "The answer is 7.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: Pauly has three dozen eggs, which is 3 * 12 = 36 eggs. Since there are 3 people in the family, Pauly can make 36 / 4 = 9 omelets in total. Each person gets 9 / 3 = 3 omelets. The answer is 3 omelets per person.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n",
      "A: First, let's find the height of the second stack: 7 (first stack) + 3 = 10 blocks.\n",
      "The third stack is 10 - 6 = 4 blocks tall.\n",
      "The fourth stack is 10 + 10 = 20 blocks tall.\n",
      "The fifth stack has 10 (second stack) x 2 = 20 blocks.\n",
      "\n",
      "Now, let's add up the number of blocks in all stacks: 7 (first) + 10 (second) + 4 (third) + 20 (fourth) + 20 (fifth) = 61 blocks. The answer is 61.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen_params_normal = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'generation_seed' : 42,\n",
    "\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "}\n",
    "\n",
    "gen_params_tfs = {\n",
    "    'max_new_tokens' : 500,\n",
    "    # max_new_tokens take precendece over stopping criteria\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "\n",
    "    'tfs' : 0.99,\n",
    "    'do_sample' : True,\n",
    "}\n",
    "\n",
    "gen_params_top_a = {\n",
    "    'max_new_tokens' : 500,\n",
    "    # max_new_tokens take precendece over stopping criteria\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "\n",
    "    'top_a' : 0.1,\n",
    "    'do_sample' : True,\n",
    "}\n",
    "\n",
    "gen_params_mirostat = {\n",
    "    'max_new_tokens' : 500,\n",
    "\n",
    "    'stopping_criteria' : stopping_criteria,\n",
    "    'generation_seed' : 42,\n",
    "\n",
    "    'mirostat_mode' : 2,\n",
    "    'do_sample' : True\n",
    "}\n",
    "\n",
    "scores, outputs = tuner_ob.get_score(gen_params_normal)\n",
    "\n",
    "print(scores,'\\n\\n')\n",
    "for output in outputs:\n",
    "    print(output)\n",
    "    print('\\n\\n')\n",
    "    print('---' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
